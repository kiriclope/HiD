#+STARTUP: fold
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session skorch :kernel dual_data

* Notebook Settings

#+begin_src ipython
  %load_ext autoreload
  %autoreload 2
  %reload_ext autoreload

  %run /home/leon/dual_task/dual_data/notebooks/setup.py
  %matplotlib inline
  %config InlineBackend.figure_format = 'png'
#+end_src

#+RESULTS:
:RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/dual_data/bin/python
: <Figure size 700x432.624 with 0 Axes>
:END:

* Imports

#+begin_src ipython
  import sys
  sys.path.insert(0, '/home/leon/dual_task/dual_data/')

  import torch
  import pickle as pkl
  import numpy as np
  import matplotlib.pyplot as plt
  from scipy.stats import circmean
  from time import perf_counter

  from sklearn.model_selection import StratifiedKFold
  from src.common.plot_utils import add_vlines, add_vdashed
  from src.preprocess.helpers import avg_epochs

  import torch.optim as optim
  from torch.utils.data import Dataset, TensorDataset, DataLoader
  DEVICE = 'cuda'
#+end_src

#+RESULTS:

* Helpers
** Optimization
#+begin_src ipython
  from sklearn.model_selection import train_test_split, StratifiedShuffleSplit

  def split_data(X, Y, train_perc=0.8, batch_size=32):

      X_train, X_test, Y_train, Y_test = train_test_split(X, Y,
                                                          train_size=train_perc,
                                                          stratify=Y,
                                                          shuffle=True)

      print('X', X_train.shape, X_test.shape)
      print('Y', Y_train.shape, Y_test.shape)

      train_dataset = TensorDataset(X_train, Y_train)
      val_dataset = TensorDataset(X_test, Y_test)

      # Create data loaders
      train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
      val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)

      return train_loader, val_loader
#+end_src

#+RESULTS:


#+begin_src ipython
  def train(dataloader, model, loss_fn, optimizer, penalty=None, lbd=1, clip_grad=0):
      device = torch.device(DEVICE if torch.cuda.is_available() else "cpu")

      model.train()
      for batch, (X, y) in enumerate(dataloader):
          X, y = X.to(device), y.to(device)
          # Compute prediction error
          y_pred = model(X)

          # if y.ndim==y_pred.ndim:
          loss = loss_fn(y_pred, X)

          if penalty is not None:
              reg_loss = 0
              for param in model.parameters():
                  if penalty=='l1':
                      reg_loss += torch.sum(torch.abs(param))
                  else:
                      reg_loss += torch.sum(torch.square(param))

                  loss = loss + lbd * reg_loss

          # Backpropagation
          loss.backward()

          # Clip gradients
          if clip_grad:
              torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=10.0)
              #torch.nn.utils.clip_grad_value_(model.parameters(), clip_value=1.0)

          optimizer.step()
          optimizer.zero_grad()

      return loss
#+end_src

#+RESULTS:

#+begin_src ipython
  def test(dataloader, model, loss_fn):
      size = len(dataloader.dataset)
      num_batches = len(dataloader)

      device = torch.device(DEVICE if torch.cuda.is_available() else "cpu")

      # Validation loop.
      model.eval()
      val_loss = 0.0
      with torch.no_grad():
          for data, targets in dataloader:
              data, targets = data.to(device), targets.to(device)

              outputs = model(data)
              loss = loss_fn(outputs, data)
              val_loss += loss.item() * data.size(0)

          val_loss /= size

      return val_loss
#+end_src

#+RESULTS:

#+begin_src ipython
  def optimize(model, train_loader, val_loader, loss_fn, optimizer, num_epochs=100, penalty=None, lbd=1, thresh=.005):
      scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)
      # scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=10, factor=0.1, verbose=True)
      # scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

      device = torch.device(DEVICE if torch.cuda.is_available() else 'cpu')
      model.to(device)

      loss_list = []
      val_loss_list = []

      # Training loop.
      for epoch in range(num_epochs):
          loss = train(train_loader, model, loss_fn, optimizer, penalty, lbd)
          val_loss = test(val_loader, model, loss_fn)
          scheduler.step(val_loss)

          loss_list.append(loss.item())
          val_loss_list.append(val_loss)

          if epoch % int(num_epochs  / 10) == 0:
              print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')

          if val_loss < thresh:
              print(f'Stopping training as loss has fallen below the threshold: {val_loss}')
              break

          if val_loss > 300:
              print(f'Stopping training as loss is too high: {val_loss}')
              break

          if torch.isnan(loss):
              print(f'Stopping training as loss is NaN.')
              break

      return loss_list, val_loss_list
#+end_src

#+RESULTS:

** Other

#+begin_src ipython
  def convert_seconds(seconds):
      h = seconds // 3600
      m = (seconds % 3600) // 60
      s = seconds % 60
      return h, m, s
#+end_src

#+RESULTS:

#+begin_src ipython
  def angle_AB(A, B):
      A_norm = A / (np.linalg.norm(A) + 1e-5)
      B_norm = B / (np.linalg.norm(B) + 1e-5)

      return int(np.arccos(A_norm @ B_norm) * 180 / np.pi)
#+end_src

#+RESULTS:

#+begin_src ipython
  def get_theta(a, b, GM=0, IF_NORM=0):

      u, v = a, b

      if GM:
          v = b - np.dot(b, a) / np.dot(a, a) * a

      if IF_NORM:
          u = a / np.linalg.norm(a)
          v = b / np.linalg.norm(b)

      return np.arctan2(v, u) % (2.0 * np.pi)
#+end_src

#+RESULTS:

* Autoencoder

#+begin_src ipython
  import torch.nn as nn

  class Autoencoder(nn.Module):
      def __init__(self, num_features, encoding_dim=32, dropout=0.5):
          super(Autoencoder, self).__init__()
          # Encoder
          self.encoder = nn.Sequential(
              nn.Linear(num_features, 128),
              nn.Sigmoid(),
              # nn.ReLU(True),
              nn.Dropout(p=dropout),
              nn.Linear(128, 64),
              nn.Sigmoid(),
              # nn.ReLU(True),
              nn.Dropout(p=dropout),
              nn.Linear(64, encoding_dim),
              nn.Sigmoid(),
              # nn.ReLU(True),
              nn.Dropout(p=dropout),
          )
          # Decoder
          self.decoder = nn.Sequential(
              nn.Linear(encoding_dim, 64),
              nn.Sigmoid(),
              # nn.ReLU(True),
              nn.Dropout(p=dropout),
              nn.Linear(64, 128),
              nn.Sigmoid(),
              nn.Dropout(p=dropout),
              nn.Linear(128, num_features),
              nn.Sigmoid(),
              # nn.ReLU(True)
          )

      def forward(self, x):
          x = self.encoder(x)
          x = self.decoder(x)
          return x
#+end_src

#+RESULTS:

* Data
** Imports

#+begin_src ipython
  import sys
  sys.path.insert(0, '../')

  from src.common.get_data import get_X_y_days, get_X_y_S1_S2
  from src.common.options import set_options
#+end_src

#+RESULTS:

** Parameters

#+begin_src ipython
  mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
  tasks = ['DPA', 'DualGo', 'DualNoGo']
  days = ['first', 'last']

  kwargs = dict()
  kwargs = {'prescreen': None, 'pval': 0.05, 'trials': '', 'balance': 'under',
            'method': 'bootstrap', 'bolasso_pval':0.05, 'bolasso_penalty': 'l2',
            'bootstrap': True, 'n_boots': 1000,
            'preprocess': False, 'scaler_BL': 'robust', 'avg_noise':True, 'unit_var_BL':False,
            'clf':'log_loss', 'scaler': None, 'tol':0.001, 'penalty':'l2',
            'out_fold': 'stratified', 'n_out': 5,
            'in_fold': 'stratified', 'n_in': 5,
            'random_state': None, 'n_repeats': 10,
            'n_lambda': 20, 'T_WINDOW': 0.5,
            'features': 'sample',
            'day': 'last'
            }
#+end_src

#+RESULTS:

** Load X, y
*** Sample

#+begin_src ipython
  options = set_options(**kwargs)
  options['reload'] = 0
  options['data_type'] = 'raw'

  options['mouse'] = 'JawsM15'
  # options['features'] = 'sample'
  options['features'] = 'distractor'
  tasks = ["Dual"]
  options['trials'] = ''

  X_list = []
  y_list = []
  for task in tasks:
      options['task'] = task
      X_dum = []
      y_dum = []
      for day in days:
          options['day'] = day
          X_days, y_days = get_X_y_days(**options)
          X_data, y_data = get_X_y_S1_S2(X_days, y_days, **options)
          y_data[y_data==-1] = 0

          X_dum.append(X_data)
          y_dum.append(y_data)

      X_list.append(X_dum)
      y_list.append(y_dum)

  print('X', X_data.shape, 'y', y_data.shape)
#+end_src

#+RESULTS:
#+begin_example
  loading files from /home/leon/dual_task/dual_data/data/JawsM15
  X_days (1152, 693, 84) y_days (1152, 6)
  ##########################################
  DATA: FEATURES distractor TASK Dual TRIALS  DAYS first LASER 0
  ##########################################
  multiple days 0 3 0
  X_S1 (96, 693, 84) X_S2 (96, 693, 84)
  loading files from /home/leon/dual_task/dual_data/data/JawsM15
  X_days (1152, 693, 84) y_days (1152, 6)
  ##########################################
  DATA: FEATURES distractor TASK Dual TRIALS  DAYS last LASER 0
  ##########################################
  multiple days 0 3 0
  X_S1 (96, 693, 84) X_S2 (96, 693, 84)
  X (192, 693, 84) y (192,)
#+end_example

#+begin_src ipython
  X_list = np.array(X_list)
  y_list = np.array(y_list)
  print(X_list.shape, y_list.shape)
#+end_src

#+RESULTS:
: (1, 2, 192, 693, 84) (1, 2, 192)


* Model Fit

#+begin_src ipython
  task = 0
  day = 0
#+end_src

#+RESULTS:

#+begin_src ipython
  options['epochs'] = ['MD']

  X = avg_epochs(X_list[task][day], **options)
  X = X.astype(np.float32)

  y = np.float32(y_list[task][day][:, np.newaxis])
  print('X', X.shape, 'y', y.shape)
#+end_src

#+RESULTS:
: X (192, 693) y (192, 1)

#+begin_src ipython
  input_dim = X.shape[1]
  hidden_dim = 32  # Adjust this based on your needs

  autoencoder = Autoencoder(input_dim, hidden_dim)
  criterion = nn.MSELoss()
  optimizer = torch.optim.Adam(autoencoder.parameters(), lr=0.01)
   #+end_src

#+RESULTS:

#+begin_src ipython
  batch_size = 16
  train_loader, val_loader = split_data(torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32), train_perc=0.8, batch_size=batch_size)
#+end_src

#+RESULTS:
: X torch.Size([153, 693]) torch.Size([39, 693])
: Y torch.Size([153, 1]) torch.Size([39, 1])

#+begin_src ipython
  start = perf_counter()
  loss, val_loss = optimize(autoencoder, train_loader, val_loader, criterion, optimizer, num_epochs=1000)
  end = perf_counter()
  print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))
#+end_src

#+RESULTS:
#+begin_example
  /home/leon/mambaforge/envs/dual_data/lib/python3.11/site-packages/torch/optim/lr_scheduler.py:149: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.
    warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)
  Epoch 1/1000, Training Loss: 0.1110, Validation Loss: 0.1404
  Epoch 101/1000, Training Loss: 0.0989, Validation Loss: 0.1308
  Epoch 201/1000, Training Loss: 0.1029, Validation Loss: 0.1291
  Epoch 301/1000, Training Loss: 0.1159, Validation Loss: 0.1291
  Epoch 401/1000, Training Loss: 0.1047, Validation Loss: 0.1297
  Epoch 501/1000, Training Loss: 0.1030, Validation Loss: 0.1294
  Epoch 601/1000, Training Loss: 0.1112, Validation Loss: 0.1297
  Epoch 701/1000, Training Loss: 0.1130, Validation Loss: 0.1296
  Epoch 801/1000, Training Loss: 0.1129, Validation Loss: 0.1302
  Epoch 901/1000, Training Loss: 0.1090, Validation Loss: 0.1293
  Elapsed (with compilation) = 0h 0m 43s
#+end_example

#+begin_src ipython
  print(X.shape, encoded_data.shape)
#+end_src

#+RESULTS:
: (192, 693) torch.Size([96, 84, 64])

#+begin_src ipython
  print(X_list.shape)
#+end_src

#+RESULTS:
: (1, 2, 192, 693, 84)

#+begin_src ipython
  options = set_options(**kwargs)
  options['reload'] = 0
  options['data_type'] = 'raw'

  options['mouse'] = 'JawsM15'
  options['features'] = 'sample'
  # options['features'] = 'distractor'
  tasks = ["DPA", "DualGo", "DualNoGo"]
  options['trials'] = ''

  X_list = []
  y_list = []
  for task in tasks:
      options['task'] = task
      X_dum = []
      y_dum = []
      for day in days:
          options['day'] = day
          X_days, y_days = get_X_y_days(**options)
          X_data, y_data = get_X_y_S1_S2(X_days, y_days, **options)
          y_data[y_data==-1] = 0

          X_dum.append(X_data)
          y_dum.append(y_data)

      X_list.append(X_dum)
      y_list.append(y_dum)

  print('X', X_data.shape, 'y', y_data.shape)
#+end_src

#+RESULTS:
#+begin_example
  loading files from /home/leon/dual_task/dual_data/data/JawsM15
  X_days (1152, 693, 84) y_days (1152, 6)
  ##########################################
  DATA: FEATURES sample TASK DPA TRIALS  DAYS first LASER 0
  ##########################################
  multiple days 0 3 0
  X_S1 (48, 693, 84) X_S2 (48, 693, 84)
  loading files from /home/leon/dual_task/dual_data/data/JawsM15
  X_days (1152, 693, 84) y_days (1152, 6)
  ##########################################
  DATA: FEATURES sample TASK DPA TRIALS  DAYS last LASER 0
  ##########################################
  multiple days 0 3 0
  X_S1 (48, 693, 84) X_S2 (48, 693, 84)
  loading files from /home/leon/dual_task/dual_data/data/JawsM15
  X_days (1152, 693, 84) y_days (1152, 6)
  ##########################################
  DATA: FEATURES sample TASK DualGo TRIALS  DAYS first LASER 0
  ##########################################
  multiple days 0 3 0
  X_S1 (48, 693, 84) X_S2 (48, 693, 84)
  loading files from /home/leon/dual_task/dual_data/data/JawsM15
  X_days (1152, 693, 84) y_days (1152, 6)
  ##########################################
  DATA: FEATURES sample TASK DualGo TRIALS  DAYS last LASER 0
  ##########################################
  multiple days 0 3 0
  X_S1 (48, 693, 84) X_S2 (48, 693, 84)
  loading files from /home/leon/dual_task/dual_data/data/JawsM15
  X_days (1152, 693, 84) y_days (1152, 6)
  ##########################################
  DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS first LASER 0
  ##########################################
  multiple days 0 3 0
  X_S1 (48, 693, 84) X_S2 (48, 693, 84)
  loading files from /home/leon/dual_task/dual_data/data/JawsM15
  X_days (1152, 693, 84) y_days (1152, 6)
  ##########################################
  DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS last LASER 0
  ##########################################
  multiple days 0 3 0
  X_S1 (48, 693, 84) X_S2 (48, 693, 84)
  X (96, 693, 84) y (96,)
#+end_example

#+begin_src ipython
  X_list = np.array(X_list)
  y_list = np.array(y_list)
  print(X_list.shape, y_list.shape)
#+end_src

#+RESULTS:
: (3, 2, 96, 693, 84) (3, 2, 96)


#+begin_src ipython
  day = 0
  X_DPA = torch.tensor(X_list[0][day], device='cuda', dtype=torch.float)
  X_DPA = torch.transpose(X_DPA, 1, 2)

  X_Go = torch.tensor(X_list[1][day], device='cuda', dtype=torch.float)
  X_Go = torch.transpose(X_Go, 1, 2)

  X_NoGo = torch.tensor(X_list[2][day], device='cuda', dtype=torch.float)
  X_NoGo = torch.transpose(X_NoGo, 1, 2)
  print(X.shape)
    #+end_src

#+RESULTS:
: (192, 693)

#+begin_src ipython
  with torch.no_grad():
      encoded_DPA = autoencoder.encoder(X_DPA)
      encoded_Go = autoencoder.encoder(X_Go)
      encoded_NoGo = autoencoder.encoder(X_NoGo)
#+end_src

#+RESULTS:

#+begin_src ipython
  plt.plot(encoded_DPA.mean((0,-1)).cpu());
  plt.plot(encoded_Go.mean((0,-1)).cpu());
  plt.plot(encoded_NoGo.mean((0,-1)).cpu());
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/aaad4372333a348edd0f0abf5500041cd787a2e0.png]]

#+begin_src ipython
  for i in range(3):
      plt.plot(encoded_data[i].mean(-1).cpu());
      plt.plot(encoded_data[-i].mean(-1).cpu());
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/156f8e7249a20d3a11c4ff02aa2ece177242cc66.png]]
