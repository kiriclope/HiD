#+STARTUP: fold
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session decoder :kernel dual_data :exports results :output-dir ./figures/landscape :file (lc/org-babel-tangle-figure-filename)

* Notebook Settings

#+begin_src ipython
%load_ext autoreload
%autoreload 2
%reload_ext autoreload

%run /home/leon/dual_task/dual_data/notebooks/setup.py
%matplotlib inline
%config InlineBackend.figure_format = 'png'
#+end_src

#+RESULTS:
:RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/dual_data/bin/python
: <Figure size 1200x741.641 with 0 Axes>
:END:

* Imports

#+begin_src ipython
from sklearn.exceptions import ConvergenceWarning
warnings.filterwarnings("ignore")

import sys
sys.path.insert(0, '/home/leon/dual_task/dual_data/')

import os
if not sys.warnoptions:
warnings.simplefilter("ignore")
os.environ["PYTHONWARNINGS"] = "ignore"

import pickle as pkl
import numpy as np
import matplotlib.pyplot as plt
from time import perf_counter

import torch
import torch.nn as nn
import torch.optim as optim
from skorch import NeuralNetClassifier

from sklearn.base import clone
from sklearn.metrics import make_scorer, roc_auc_score
from sklearn.ensemble import BaggingClassifier
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, LeaveOneOut
from sklearn.decomposition import PCA

from mne.decoding import SlidingEstimator, cross_val_multiscore, GeneralizingEstimator, get_coef

from src.common.plot_utils import add_vlines, add_vdashed
from src.common.options import set_options
from src.stats.bootstrap import my_boots_ci
from src.decode.bump import decode_bump, circcvl
from src.common.get_data import get_X_y_days, get_X_y_S1_S2
from src.decode.classifiers import safeSelector
from src.preprocess.helpers import avg_epochs
#+end_src

#+RESULTS:

* Helpers
** Perceptron

#+begin_src ipython :tangle ../src/torch/perceptron.py
import torch
import torch.nn as nn

class CustomBCEWithLogitsLoss(nn.BCEWithLogitsLoss):
    def __init__(self, pos_weight=None, weight=None, reduction='mean'):
        super(CustomBCEWithLogitsLoss, self).__init__(weight=weight, reduction=reduction, pos_weight=pos_weight)

    def forward(self, input, target):
        target = target.view(-1, 1)  # Make sure target shape is (n_samples, 1)
        return super().forward(input.to(torch.float32), target.to(torch.float32))
#+end_src

#+RESULTS:

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/perceptron.py
class Perceptron(nn.Module):
    def __init__(self, num_features, dropout_rate=0.0):
        super(Perceptron, self).__init__()
        self.linear = nn.Linear(num_features, 1)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        x = self.dropout(x)
        hidden = self.linear(x)
        return hidden
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/perceptron.py
class MLP(nn.Module):
    def __init__(self, num_features, hidden_units=64, dropout_rate=0.5):
        super(MLP, self).__init__()
        self.linear = nn.Linear(num_features, hidden_units)
        self.dropout = nn.Dropout(dropout_rate)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(hidden_units, 1)

    def forward(self, x):
        x = self.dropout(x)
        x = self.relu(self.linear(x))
        x = self.dropout(x)
        hidden = self.linear2(x)
        return hidden
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/skorch.py
import torch
from skorch import NeuralNetClassifier
from skorch.callbacks import Callback
from skorch.callbacks import EarlyStopping

early_stopping = EarlyStopping(
    monitor='train_loss',    # Metric to monitor
    patience=10,              # Number of epochs to wait for improvement
    threshold=0.001,       # Minimum change to qualify as an improvement
    threshold_mode='rel',    # 'rel' for relative change, 'abs' for absolute change
    lower_is_better=True     # Set to True if lower metric values are better
)

class RegularizedNet(NeuralNetClassifier):
    def __init__(self, module, alpha=0.001, l1_ratio=0.95, **kwargs):
        self.alpha = alpha  # Regularization strength
        self.l1_ratio = l1_ratio # Balance between L1 and L2 regularization

        super().__init__(module, **kwargs)

    def get_loss(self, y_pred, y_true, X=None, training=False):
        # Call super method to compute primary loss
        if y_pred.shape != y_true.shape:
            y_true = y_true.unsqueeze(-1)

        loss = super().get_loss(y_pred, y_true, X=X, training=training)

        if self.alpha>0:
            elastic_net_reg = 0
            for param in self.module_.parameters():
                elastic_net_reg += self.alpha * self.l1_ratio * torch.sum(torch.abs(param))
                elastic_net_reg += self.alpha * (1 - self.l1_ratio) * torch.sum(param ** 2)

        # Add the elastic net regularization term to the primary loss
        return loss + elastic_net_reg
#+end_src

#+RESULTS:

** Model
#+begin_src ipython
def get_bagged_coefs(clf, n_estimators):
    coefs_list = []
    bias_list = []
    for i in range(n_estimators):
        model = clf.estimators_[i]
        try:
            coefs = model.named_steps['net'].module_.linear.weight.data.cpu().detach().numpy()[0]
            bias = model.named_steps['net'].module_.linear.bias.data.cpu().detach().numpy()[0]
        except:
            coefs = model.named_steps['net'].coef_.T
            bias = model.named_steps['net'].intercept_.T

        # coefs, bias = rescale_coefs(model, coefs, bias)

        coefs_list.append(coefs)
        bias_list.append(bias)

    return np.array(coefs_list).mean(0), np.array(bias_list).mean(0)
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/classificationCV.py
from time import perf_counter
from sklearn.ensemble import BaggingClassifier
from sklearn.preprocessing import StandardScaler
  from sklearn.pipeline import Pipeline
  from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, LeaveOneOut
  from sklearn.decomposition import PCA

  from mne.decoding import SlidingEstimator, cross_val_multiscore

  class ClassificationCV():
      def __init__(self, net, params, **kwargs):

          pipe = []
          self.scaler = kwargs['scaler']
          if self.scaler is not None and self.scaler !=0 :
              pipe.append(("scaler", StandardScaler()))

          self.n_comp = kwargs['n_comp']
          if kwargs['n_comp'] is not None:
              self.n_comp = kwargs['n_comp']
              pipe.append(("pca", PCA(n_components=self.n_comp)))

          self.prescreen = kwargs['prescreen']
          self.alpha = kwargs['pval']
          if kwargs["prescreen"] is not None:
              pipe.append(("filter", safeSelector(method=kwargs['prescreen'] , alpha=kwargs["pval"])))

          pipe.append(("net", net))
          self.model = Pipeline(pipe)

          self.num_features = kwargs['num_features']
          self.scoring =  kwargs['scoring']

          if  kwargs['n_splits']==-1:
              self.cv = LeaveOneOut()
          else:
              self.cv = RepeatedStratifiedKFold(n_splits=kwargs['n_splits'], n_repeats=kwargs['n_repeats'])

          self.params = params
          self.verbose =  kwargs['verbose']
          self.n_jobs =  kwargs['n_jobs']

      def fit(self, X, y):
          start = perf_counter()
          if self.verbose:
              print('Fitting hyperparameters ...')

          try:
              self.model['net'].module__num_features = self.num_features
          except:
              pass

          grid = GridSearchCV(self.model, self.params, refit=True, cv=self.cv, scoring=self.scoring, n_jobs=self.n_jobs)
          grid.fit(X.astype('float32'), y.astype('float32'))
          end = perf_counter()
          if self.verbose:
              print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))

          self.best_model = grid.best_estimator_
          self.best_params = grid.best_params_

          if self.verbose:
              print(self.best_params)

          try:
              self.coefs = self.best_model.named_steps['net'].module_.linear.weight.data.cpu().detach().numpy()[0]
              self.bias = self.best_model.named_steps['net'].module_.linear.bias.data.cpu().detach().numpy()[0]
          except:
              self.coefs = self.best_model.named_steps['net'].coef_.T
              self.bias = self.best_model.named_steps['net'].intercept_.T

      def get_bootstrap_coefs(self, X, y, n_boots=10):
          start = perf_counter()
          if self.verbose:
              print('Bootstrapping coefficients ...')

          self.bagging_clf = BaggingClassifier(base_estimator=self.best_model, n_estimators=n_boots)
          self.bagging_clf.fit(X.astype('float32'), y.astype('float32'))
          end = perf_counter()

          if self.verbose:
              print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))

          self.coefs, self.bias = get_bagged_coefs(self.bagging_clf, n_estimators=n_boots)

          return self.coefs, self.bias


      def get_overlap(self, model, X):
          try:
              coefs = model.named_steps['net'].module_.linear.weight.data.cpu().detach().numpy()[0]
              bias = model.named_steps['net'].module_.linear.bias.data.cpu().detach().numpy()[0]
          except:
              coefs = model.named_steps['net'].coef_.T
              bias = model.named_steps['net'].intercept_.T

          if self.scaler is not None and self.scaler!=0:
              scaler = model.named_steps['scaler']
              for i in range(X.shape[-1]):
                  X[..., i] = scaler.transform(X[..., i])

          if (self.prescreen is not None) and (self.prescreen != 0):
              filter = model.named_steps['filter']
              idx = filter.selector.get_support(indices=True)
              self.overlaps = (np.swapaxes(X[:, idx], 1, -1) @ coefs) / np.linalg.norm(coefs, axis=0)

          elif (self.n_comp is not None) and (self.n_comp != 0):
              pca = model.named_steps['pca']
              X_pca = np.zeros((X.shape[0], self.n_comp, X.shape[-1]))

              for i in range(X.shape[-1]):
                  X_pca[..., i] = pca.transform(X[..., i])

              self.overlaps = (np.swapaxes(X_pca, 1, -1) @ coefs + bias) # / np.linalg.norm(coefs, axis=0)
          else:
              self.overlaps = -(np.swapaxes(X, 1, -1) @ coefs) / np.linalg.norm(coefs, axis=0)
              # self.overlaps = -(np.swapaxes(X, 1, -1) @ coefs + bias) / np.linalg.norm(coefs, axis=0)

          return self.overlaps

      def get_bootstrap_overlaps(self, X):
          start = perf_counter()
          if self.verbose:
              print('Getting bootstrapped overlaps ...')

          X_copy = np.copy(X)
          overlaps_list = []
          n_boots = len(self.bagging_clf.estimators_)

          for i in range(n_boots):
              model = self.bagging_clf.estimators_[i]
              overlaps = self.get_overlap(model, X_copy)
              overlaps_list.append(overlaps)

          end = perf_counter()
          if self.verbose:
              print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))

          return np.array(overlaps_list).mean(0)

      def get_cv_scores(self, X, y, scoring):
          start = perf_counter()
          if self.verbose:
              print('Computing cv scores ...')

          estimator = SlidingEstimator(clone(self.best_model), n_jobs=1,
                                       scoring=scoring, verbose=False)

          self.scores = cross_val_multiscore(estimator, X.astype('float32'), y.astype('float32'),
                                             cv=self.cv, n_jobs=-1, verbose=False)
          end = perf_counter()
          if self.verbose:
              print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))

          return self.scores
#+end_src

#+RESULTS:

  #+begin_src ipython :tangle ../src/torch/main.py
from src.common.get_data import get_X_y_days, get_X_y_S1_S2
from src.preprocess.helpers import avg_epochs

def get_classification(model, RETURN='overlaps', **options):
        start = perf_counter()

        dum = 0
        if options['features'] == 'distractor':
                if options['task'] != 'Dual':
                        task = options['task']
                        options['task'] = 'Dual'
                        dum = 1

        X_days, y_days = get_X_y_days(**options)
        X, y = get_X_y_S1_S2(X_days, y_days, **options)

        y_labels = y.copy()

        if options['features'] == 'sample':
            y = y.sample_odor.dropna().to_numpy()
        elif options['features'] == 'distractor':
            y = y.dist_odor.dropna().to_numpy()
        elif options['features'] == 'choice':
            y = y.choice.to_numpy()

        y[y==-1] = 0

        if options['verbose']:
            print('X', X.shape, 'y', y.shape)

        X_avg = avg_epochs(X, **options).astype('float32')
        y_avg = y

        if options['trials'] == 'correct':
            options['trials'] = ''
            X, _ = get_X_y_S1_S2(X_days, y_days, **options)

        if dum:
                options['features'] = 'sample'
                options['task'] = task
                X, _ = get_X_y_S1_S2(X_days, y_days, **options)

        # if options['class_weight']:
        #         pos_weight = torch.tensor(np.sum(y==0) / np.sum(y==1), device=DEVICE).to(torch.float32)
        #         print('imbalance', pos_weight)
        #         model.criterion__pos_weight = pos_weight

        if RETURN is None:
            return None
        else:
            model.fit(X_avg, y_avg)

        if 'scores' in RETURN:
            scores = model.get_cv_scores(X, y, options['scoring'])
            end = perf_counter()
            print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))
            return scores
        elif 'overlaps' in RETURN:
            coefs, bias = model.get_bootstrap_coefs(X_avg, y_avg, n_boots=options['n_boots'])
            overlaps = model.get_bootstrap_overlaps(X)
            end = perf_counter()
            print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))
            return overlaps
        elif 'coefs' in RETURN:
            coefs, bias = model.get_bootstrap_coefs(X_avg, y_avg, n_boots=options['n_boots'])
            end = perf_counter()
            print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))
            return coefs, bias
        else:
            return None
#+end_src

#+RESULTS:

** Other

#+begin_src ipython :tangle ../src/torch/utils.py
  import numpy as np

  def safe_roc_auc_score(y_true, y_score):
      y_true = np.asarray(y_true)
      if len(np.unique(y_true)) == 1:
          return np.nan  # return np.nan where the score cannot be calculated
      return roc_auc_score(y_true, y_score)
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  def rescale_coefs(model, coefs, bias):

          try:
                  means = model.named_steps["scaler"].mean_
                  scales = model.named_steps["scaler"].scale_

                  # Rescale the coefficients
                  rescaled_coefs = np.true_divide(coefs, scales)

                  # Adjust the intercept
                  rescaled_bias = bias - np.sum(rescaled_coefs * means)

                  return rescaled_coefs, rescaled_bias
          except:
                  return coefs, bias

#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  from scipy.stats import bootstrap

  def get_bootstrap_ci(data, statistic=np.mean, confidence_level=0.95, n_resamples=1000, random_state=None):
      result = bootstrap((data,), statistic)
      ci_lower, ci_upper = result.confidence_interval
      return np.array([ci_lower, ci_upper])
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  def convert_seconds(seconds):
      h = seconds // 3600
      m = (seconds % 3600) // 60
      s = seconds % 60
      return h, m, s
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  import pickle as pkl

  def pkl_save(obj, name, path="."):
      pkl.dump(obj, open(path + "/" + name + ".pkl", "wb"))


  def pkl_load(name, path="."):
      return pkl.load(open(path + "/" + name, "rb"))

#+end_src

#+RESULTS:

* Parameters

#+begin_src ipython
  DEVICE = 'cuda:0'
  mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
  N_NEURONS = [668, 693, 444, 361, 113]

  tasks = ['DPA', 'DualGo', 'DualNoGo']
  params = { 'net__alpha': np.logspace(-4, 4, 10),
             # 'net__l1_ratio': np.linspace(0, 1, 10),
             # 'net__module__dropout_rate': np.linspace(0, 1, 10),
            }

  # ['AP02', 'AP12', 'PP09', 'PP17', 'RP17']

  kwargs = {
      'mouse': 'ACCM04', 'laser': 0,
      'trials': '', 'reload': 1, 'data_type': 'dF',
      'prescreen': None, 'pval': 0.05,
      'preprocess': False, 'scaler_BL': 'robust',
      'avg_noise':True, 'unit_var_BL': True,
      'random_state': None, 'T_WINDOW': 0.0,
      'l1_ratio': 0.95,
      'n_comp': None, 'scaler': None,
      'bootstrap': 1, 'n_boots': 128,
      'n_splits': 3, 'n_repeats': 32,
      'class_weight': 0,
      'multilabel':0,
  }

  # kwargs['days'] = ['first', 'middle', 'last']
  options = set_options(**kwargs)
  days = np.arange(1, options['n_days']+1)
  # days = ['first', 'middle', 'last']

  safe_roc_auc = make_scorer(safe_roc_auc_score, needs_proba=True)
  options['scoring'] = safe_roc_auc
  options['n_jobs'] = 30
#+end_src

#+RESULTS:

* DATA

#+begin_src ipython
import pandas as pd
options['n_days'] = 3
y = []
for mouse in options['mice']:
    print(mouse)
    try:
        y_mouse = pkl_load('y_%s.pkl' % mouse)
        y_mouse['mouse'] = mouse
        y.append(y_mouse)
    except:
        pass
y = pd.concat(y)
print(y.day.unique())
#+end_src

#+RESULTS:
: ChRM04
: JawsM15
: JawsM18
: ACCM03
: ACCM04
: [1. 2. 3. 4. 5. 6.]

#+begin_src ipython
print(y.keys())
#+end_src

#+RESULTS:
: Index(['sample_odor', 'test_odor', 'response', 'tasks', 'laser', 'day',
:        'dist_odor', 'choice', 'behavior', 'pair', 'sample', 'sample_STIM',
:        'sample_ED', 'sample_MD', 'sample_LD', 'dist', 'dist_STIM', 'dist_ED',
:        'dist_MD', 'dist_LD', 'mouse'],
:       dtype='object')

* Sample Late Delay
** behavior ~ day * tasks * overlaps + (1|mouse)

#+begin_src ipython
import rpy2.robjects as robjects
from rpy2.robjects.packages import importr

# Set the .libPaths in R
custom_r_libpath = '~/R/x86_64-pc-linux-gnu-library/4.3/'
robjects.r('.libPaths("{0}")'.format(custom_r_libpath))

from pymer4.models import Lmer
#+end_src

#+RESULTS:
:RESULTS:
: R[write to console]: Error in dyn.load(file, DLLpath = DLLpath, ...) :
:   unable to load shared object '/usr/lib/R/library/methods/libs/methods.so':
:   /usr/lib/R/library/methods/libs/methods.so: undefined symbol: R_typeToChar
:
# [goto error]
#+begin_example
---------------------------------------------------------------------------
RRuntimeError                             Traceback (most recent call last)
Cell In[2], line 1
----> 1 import rpy2.robjects as robjects
      2 from rpy2.robjects.packages import importr
      4 # Set the .libPaths in R

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/rpy2/robjects/__init__.py:18
     15 import rpy2.rinterface as rinterface
     16 import rpy2.rlike.container as rlc
---> 18 from rpy2.robjects.robject import RObjectMixin, RObject
     19 import rpy2.robjects.functions
     20 from rpy2.robjects.environments import (Environment,
     21                                         local_context)

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/rpy2/robjects/robject.py:81
     75             yield v
     78 _get_exported_value = rpy2.rinterface.baseenv['::']
---> 81 class RObjectMixin(abc.ABC):
     82     """ Abstract class to provide methods common to all RObject instances. """
     84     __rname__: typing.Optional[str] = None

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/rpy2/robjects/robject.py:93, in RObjectMixin()
     91 __readlines = rpy2.rinterface.baseenv.find("readLines")
     92 __unlink = rpy2.rinterface.baseenv.find("unlink")
---> 93 __show = _get_exported_value('methods', 'show')
     94 __print = _get_exported_value('base', 'print')
     96 __slots = None

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/rpy2/rinterface_lib/conversion.py:45, in _cdata_res_to_rinterface.<locals>._(*args, **kwargs)
     44 def _(*args, **kwargs):
---> 45     cdata = function(*args, **kwargs)
     46     # TODO: test cdata is of the expected CType
     47     return _cdata_to_rinterface(cdata)

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/rpy2/rinterface.py:817, in SexpClosure.__call__(self, *args, **kwargs)
    810     res = rmemory.protect(
    811         openrlib.rlib.R_tryEval(
    812             call_r,
    813             call_context.__sexp__._cdata,
    814             error_occured)
    815     )
    816     if error_occured[0]:
--> 817         raise embedded.RRuntimeError(_rinterface._geterrmessage())
    818 return res

RRuntimeError: Error in dyn.load(file, DLLpath = DLLpath, ...) :
  unable to load shared object '/usr/lib/R/library/methods/libs/methods.so':
  /usr/lib/R/library/methods/libs/methods.so: undefined symbol: R_typeToChar
#+end_example
:END:

#+begin_src ipython
  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('int')

  print(y.behavior.unique())

  formula = 'behavior ~ day * tasks * sample_LD + (1+ tasks + day + sample_LD | mouse)'

  results = []
  data = y.copy()

  glm = Lmer(formula=formula, data=data, family='binomial')
  result = glm.fit()
#+end_src

#+RESULTS:
#+begin_example
[1 0]
boundary (singular) fit: see help('isSingular')

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~day*tasks*sample_LD+(1+tasks+day+sample_LD|mouse)

Family: binomial	 Inference: parametric

Number of observations: 3648	 Groups: {'mouse': 5.0}

Log-likelihood: -1748.159 	 AIC: 3550.318

Random effects:

                Name    Var    Std
mouse    (Intercept)  0.151  0.389
mouse    tasksDualGo  0.063  0.251
mouse  tasksDualNoGo  0.006  0.077
mouse            day  0.028  0.166
mouse      sample_LD  0.003  0.055

                 IV1            IV2   Corr
mouse    (Intercept)    tasksDualGo  0.117
mouse    (Intercept)  tasksDualNoGo -0.918
mouse    (Intercept)            day -0.020
mouse    (Intercept)      sample_LD  0.959
mouse    tasksDualGo  tasksDualNoGo  0.285
mouse    tasksDualGo            day -0.422
mouse    tasksDualGo      sample_LD  0.382
mouse  tasksDualNoGo            day -0.149
mouse  tasksDualNoGo      sample_LD -0.773
mouse            day      sample_LD -0.205

Fixed effects:
#+end_example

#+begin_src ipython
random_effects = glm.ranef
print(random_effects.keys())
#+end_src

#+RESULTS:
: Index(['X.Intercept.', 'tasksDualGo', 'tasksDualNoGo', 'day', 'sample_LD'], dtype='object')

#+begin_src ipython
# plt.figure(figsize=(15, 5))
colors = ['blue', 'green', 'red', 'purple', 'orange']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .5

keys = ['(Intercept)', 'tasksDualGo', 'tasksDualNoGo']
# keys = result.Estimate.keys()

for i, key in enumerate(keys):
     if key == '(Intercept)':
          df = result.Estimate['(Intercept)']+ random_effects['X.Intercept.']
     else:
          df = result.Estimate['(Intercept)']+ result.Estimate[key] + random_effects[key]

     mean_value = df.mean()
     std_dev = df.std()

     if result['P-val'][key]<0.001:
          plt.text(i,   1.51, '***', ha='center', va='bottom')
     elif result['P-val'][key]<0.01:
          plt.text(i,   1.51, '**', ha='center', va='bottom')
     elif result['P-val'][key]<0.05:
          plt.text(i,   1.51, '*', ha='center', va='bottom')

     # Plot individual points
     plt.scatter(i * np.ones(df.shape[0]) + space, df, color=colors)
     # Plot mean and stddev as error bars
     plt.plot(i, mean_value, '_k', ms=20)
     plt.errorbar(i * np.ones(df.shape[0]), [mean_value]*len(df), yerr=[std_dev]*len(df), fmt='-', linestyle='None', color='k', capsize=15)

plt.axhline(y=0, color='black', linestyle='--')
plt.xticks(np.arange(len(keys)), keys)
plt.ylim([-1.5,1.5])
plt.ylabel('$\\beta$')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_20.png]]
#+RESULTS:

#+begin_src ipython
# plt.figure(figsize=(15, 5))
colors = ['blue', 'green', 'red', 'purple', 'orange']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1

keys = ['day', 'day:tasksDualGo', 'day:tasksDualNoGo']
# keys = result.Estimate.keys()
for i, key in enumerate(keys):
     if i == 0 :
          df = result.Estimate[key] + random_effects['day']
     else:
          df = result.Estimate['day']+ result.Estimate[key] + random_effects['day']

     mean_value = df.mean()
     std_dev = df.std()

     if result['P-val'][key]<0.001:
          plt.text(i,   1.01, '***', ha='center', va='bottom')
     elif result['P-val'][key]<0.01:
          plt.text(i,   1.01, '**', ha='center', va='bottom')
     elif result['P-val'][key]<0.05:
          plt.text(i,   1.01, '*', ha='center', va='bottom')

     # Plot individual points
     plt.scatter(i * np.ones(df.shape[0]) + space, df, color=colors)
     # Plot mean and stddev as error bars
     plt.plot(i, mean_value, '_k', ms=20)
     plt.errorbar(i * np.ones(df.shape[0]), [mean_value]*len(df), yerr=[std_dev]*len(df), fmt='-', linestyle='None', color='k', capsize=15)

plt.axhline(y=0, color='black', linestyle='--')
plt.xticks(np.arange(len(keys)), keys)
plt.ylim([-1,1])
plt.ylabel('$\\beta$')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_21.png]]

#+begin_src ipython
# plt.figure(figsize=(15, 5))
colors = ['blue', 'green', 'red', 'purple', 'orange']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1

keys = ['sample_LD', 'tasksDualGo:sample_LD', 'tasksDualNoGo:sample_LD']
for i, key in enumerate(keys):
     if i==0:
          df = result.Estimate[key] + random_effects['sample_LD']
     else:
          df = result.Estimate['sample_LD']+ result.Estimate[key] + random_effects['sample_LD']

     mean_value = df.mean()
     std_dev = df.std()

     if result['P-val'][key]<0.001:
          plt.text(i,   1.51, '***', ha='center', va='bottom')
     elif result['P-val'][key]<0.01:
          plt.text(i,   1.51, '**', ha='center', va='bottom')
     elif result['P-val'][key]<0.05:
          plt.text(i,   1.51, '*', ha='center', va='bottom')

     # Plot individual points
     plt.scatter(i * np.ones(df.shape[0]) + space, df, color=colors)
     # Plot mean and stddev as error bars
     plt.plot(i, mean_value, '_k', ms=20)
     plt.errorbar(i * np.ones(df.shape[0]), [mean_value]*len(df), yerr=[std_dev]*len(df), fmt='-', linestyle='None', color='k', capsize=15)

plt.axhline(y=0, color='black', linestyle='--')
plt.xticks(np.arange(len(keys)), keys)
plt.ylim([-1.5,1.5])
plt.ylabel('$\\beta$')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_22.png]]

#+begin_src ipython
plt.figure(figsize=(15, 5))
colors = ['blue', 'green', 'red', 'purple', 'orange']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1

keys = ['day:sample_LD', 'day:tasksDualGo:sample_LD', 'day:tasksDualNoGo:sample_LD']
# keys = result.Estimate.keys()
for i, key in enumerate(keys):
     if i == 0 :
          df = result.Estimate[key] + random_effects['day']
     else:
          df = result.Estimate['day:sample_LD']+ result.Estimate[key] + random_effects['day']

     mean_value = df.mean()
     std_dev = df.std()

     if result['P-val'][key]<0.001:
          plt.text(i,   1.01, '***', ha='center', va='bottom')
     elif result['P-val'][key]<0.01:
          plt.text(i,   1.01, '**', ha='center', va='bottom')
     elif result['P-val'][key]<0.05:
          plt.text(i,   1.01, '*', ha='center', va='bottom')

     # Plot individual points
     plt.scatter(i * np.ones(df.shape[0]) + space, df, color=colors)
     # Plot mean and stddev as error bars
     plt.plot(i, mean_value, '_k', ms=20)
     plt.errorbar(i * np.ones(df.shape[0]), [mean_value]*len(df), yerr=[std_dev]*len(df), fmt='-', linestyle='None', color='k', capsize=15)

plt.axhline(y=0, color='black', linestyle='--')
plt.xticks(np.arange(len(keys)), keys)
plt.ylim([-1,1])
plt.ylabel('$\\beta$')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_23.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

** behavior ~ tasks * overlaps, per day

#+begin_src ipython
  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('int')

  results = []
  data = y.copy()

  formula = 'behavior ~ tasks * sample_LD + (1 | mouse)'
  # formula = 'behavior ~ tasks * sample_LD + (1 | mouse)'
  # data = data[data.mouse!='ACCM04']

  glms = []
  results = []
  for day in y.day.unique():
      glm = Lmer(formula=formula, data=data[data.day==day], family='binomial')
      glm.fit()
      glms.append(glm)
#+end_src

#+RESULTS:
#+begin_example
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~tasks*sample_LD+(1|mouse)

Family: binomial	 Inference: parametric

Number of observations: 672	 Groups: {'mouse': 5.0}

Log-likelihood: -459.770 	 AIC: 933.541

Random effects:

              Name    Var    Std
mouse  (Intercept)  0.029  0.171

No random effect correlations specified

Fixed effects:
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~tasks*sample_LD+(1|mouse)

Family: binomial	 Inference: parametric

Number of observations: 672	 Groups: {'mouse': 5.0}

Log-likelihood: -381.684 	 AIC: 777.367

Random effects:

              Name    Var    Std
mouse  (Intercept)  0.996  0.998

No random effect correlations specified

Fixed effects:
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~tasks*sample_LD+(1|mouse)

Family: binomial	 Inference: parametric

Number of observations: 672	 Groups: {'mouse': 5.0}

Log-likelihood: -339.996 	 AIC: 693.991

Random effects:

              Name    Var    Std
mouse  (Intercept)  0.305  0.552

No random effect correlations specified

Fixed effects:
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~tasks*sample_LD+(1|mouse)

Family: binomial	 Inference: parametric

Number of observations: 672	 Groups: {'mouse': 5.0}

Log-likelihood: -243.949 	 AIC: 501.898

Random effects:

              Name    Var    Std
mouse  (Intercept)  1.432  1.197

No random effect correlations specified

Fixed effects:
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~tasks*sample_LD+(1|mouse)

Family: binomial	 Inference: parametric

Number of observations: 672	 Groups: {'mouse': 5.0}

Log-likelihood: -238.565 	 AIC: 491.131

Random effects:

              Name   Var    Std
mouse  (Intercept)  1.08  1.039

No random effect correlations specified

Fixed effects:
Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~tasks*sample_LD+(1|mouse)

Family: binomial	 Inference: parametric

Number of observations: 288	 Groups: {'mouse': 3.0}

Log-likelihood: -63.413 	 AIC: 140.825

Random effects:

              Name    Var    Std
mouse  (Intercept)  0.071  0.267

No random effect correlations specified

Fixed effects:
#+end_example

#+begin_src ipython
coefs = []
rands = []

for day in y.day.unique():
    glm = glms[day-1]
    df = glm.coefs
    df['day'] = day
    coefs.append(df)

    df2 = glm.ranef
    df2['day'] = day
    rands.append(df2)

coefs = pd.concat(coefs)
rands = pd.concat(rands)
# coefs = pd.concat(coefs, ignore_index=True)
#+end_src

#+RESULTS:

#+begin_src ipython
print(rands.keys())
#+end_src

#+RESULTS:
: Index(['X.Intercept.', 'day'], dtype='object')

#+begin_src ipython
plt.figure(figsize=(15, 5))
colors = ['red', 'blue', 'green', 'k']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1

keys = ['sample_LD', 'tasksDualGo:sample_LD', 'tasksDualNoGo:sample_LD']

for i, key in enumerate(keys):
    df = []
    for day in y.day.unique():
        result = coefs[coefs.day==day]

        if key == '(Intercept)':
            df.append(result.Estimate['(Intercept)'])
        else:
            df.append(result.Estimate['sample_LD'] + result.Estimate[key])

        if result['P-val'][key]<0.001:
            plt.text(day,   5.1 + i * 0.5, '***', ha='center', va='bottom', color=colors[i])
        elif result['P-val'][key]<0.01:
            plt.text(day,   5.1 + i * 0.5, '**', ha='center', va='bottom', color=colors[i])
        elif result['P-val'][key]<0.05:
            plt.text(day,   5.1 + i * 0.5, '*', ha='center', va='bottom', color=colors[i])


    plt.plot(np.arange(1, 7), df, '-o', color=colors[i])
plt.axhline(y=0, color='black', linestyle='--')
plt.ylabel('$\\beta_{sample_{LD}}$')
plt.xlabel('Day')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_28.png]]

#+begin_src ipython
plt.figure(figsize=(15, 5))
colors = ['red', 'blue', 'green', 'k']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1

keys = ['(Intercept)', 'tasksDualGo', 'tasksDualNoGo']

for i, key in enumerate(keys):
    df = []
    for day in y.day.unique():
        result = coefs[coefs.day==day]

        if key == '(Intercept)':
            df.append(result.Estimate['(Intercept)'])
        else:
            df.append(result.Estimate['(Intercept)'] + result.Estimate[key])

        if result['P-val'][key]<0.001:
            plt.text(day,   3.1 + i * 0.5, '***', ha='center', va='bottom', color=colors[i])
        elif result['P-val'][key]<0.01:
            plt.text(day,   3.1 + i * 0.5, '**', ha='center', va='bottom', color=colors[i])
        elif result['P-val'][key]<0.05:
            plt.text(day,   3.1 + i * 0.5, '*', ha='center', va='bottom', color=colors[i])


    plt.plot(np.arange(1, 7), df, '-o', color=colors[i])
plt.axhline(y=0, color='black', linestyle='--')
plt.ylabel('$\\beta_{tasks}$')
plt.xlabel('Day')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_29.png]]

** overlaps ~ tasks

#+begin_src ipython
  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('int')

  results = []
  data = y.copy()
  # data = data[data.mouse != 'JawsM18']

  formula = 'sample_LD ~ tasks + (1 + tasks | mouse)'

  glms = []
  results = []
  for day in y.day.unique():
      glm = Lmer(formula=formula, data=data[data.day==day], family='gaussian')
      glm.fit()
      glms.append(glm)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example
---------------------------------------------------------------------------
RRuntimeError                             Traceback (most recent call last)
Cell In[74], line 14
     12 for day in y.day.unique():
     13     glm = Lmer(formula=formula, data=data[data.day==day], family='gaussian')
---> 14     glm.fit()
     15     glms.append(glm)

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/pymer4/models/Lmer.py:424, in Lmer.fit(self, conf_int, n_boot, factors, permute, ordered, verbose, REML, rank, rank_group, rank_exclude_cols, no_warnings, control, old_optimizer, **kwargs)
    422     lmer = importr("lmerTest")
    423     lmc = robjects.r(f"lmerControl({control})")
--> 424     self.model_obj = lmer.lmer(
    425         self.formula, data=data, REML=REML, control=lmc, contrasts=contrasts
    426     )
    427 else:
    428     if verbose:

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/rpy2/robjects/functions.py:208, in SignatureTranslatedFunction.__call__(self, *args, **kwargs)
    206         v = kwargs.pop(k)
    207         kwargs[r_k] = v
--> 208 return (super(SignatureTranslatedFunction, self)
    209         .__call__(*args, **kwargs))

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/rpy2/robjects/functions.py:131, in Function.__call__(self, *args, **kwargs)
    129     else:
    130         new_kwargs[k] = cv.py2rpy(v)
--> 131 res = super(Function, self).__call__(*new_args, **new_kwargs)
    132 res = cv.rpy2py(res)
    133 return res

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/rpy2/rinterface_lib/conversion.py:45, in _cdata_res_to_rinterface.<locals>._(*args, **kwargs)
     44 def _(*args, **kwargs):
---> 45     cdata = function(*args, **kwargs)
     46     # TODO: test cdata is of the expected CType
     47     return _cdata_to_rinterface(cdata)

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/rpy2/rinterface.py:817, in SexpClosure.__call__(self, *args, **kwargs)
    810     res = rmemory.protect(
    811         openrlib.rlib.R_tryEval(
    812             call_r,
    813             call_context.__sexp__._cdata,
    814             error_occured)
    815     )
    816     if error_occured[0]:
--> 817         raise embedded.RRuntimeError(_rinterface._geterrmessage())
    818 return res

RRuntimeError: Error in eval(predvars, data, env) : object 'mouse' not found
#+end_example
:END:

#+begin_src ipython
coefs = []
rands = []

for day in y.day.unique():
    glm = glms[day-1]
    df = glm.coefs
    df['day'] = day
    coefs.append(df)

    df2 = glm.ranef
    df2['day'] = day
    rands.append(df2)

coefs = pd.concat(coefs)
rands = pd.concat(rands)
# coefs = pd.concat(coefs, ignore_index=True)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: IndexError                                Traceback (most recent call last)
: Cell In[75], line 5
:       2 rands = []
:       4 for day in y.day.unique():
: ----> 5     glm = glms[day-1]
:       6     df = glm.coefs
:       7     df['day'] = day
:
: IndexError: list index out of range
:END:

#+begin_src ipython
plt.figure(figsize=(15, 5))
colors = ['red', 'blue', 'green', 'k']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1

keys = ['(Intercept)', 'tasksDualGo', 'tasksDualNoGo']

for i, key in enumerate(keys):
    df = []
    for day in y.day.unique():
        result = coefs[coefs.day==day]

        if i==0:
            df.append(result.Estimate[key])
        else:
            df.append(result.Estimate['(Intercept)'] + result.Estimate[key])

        if result['P-val'][key]<0.001:
            plt.text(day,  0.1 + i * 0.05, '***', ha='center', va='bottom', color=colors[i])
        elif result['P-val'][key]<0.01:
            plt.text(day,   0.1 + i * 0.05, '**', ha='center', va='bottom', color=colors[i])
        elif result['P-val'][key]<0.05:
            plt.text(day,   0.1 + i * 0.05, '*', ha='center', va='bottom', color=colors[i])


    plt.plot(np.arange(1, 7), df, '-o', color=colors[i])
plt.axhline(y=0, color='black', linestyle='--')
plt.ylabel('$\\beta_{Tasks}$')
plt.xlabel('Day')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: AttributeError                            Traceback (most recent call last)
: Cell In[76], line 10
:       8 df = []
:       9 for day in y.day.unique():
: ---> 10     result = coefs[coefs.day==day]
:      12     if i==0:
:      13         df.append(result.Estimate[key])
:
: AttributeError: 'list' object has no attribute 'day'
: <Figure size 1500x500 with 0 Axes>
:END:

#+begin_src ipython

#+end_src

#+RESULTS:

** overlaps ~ day

#+begin_src ipython
  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')

  results = []
  data = y.copy()
  # data = data[data.mouse != 'ACCM04']

  formula = 'sample_LD ~ day + (1 | mouse)'

  glms = []
  results = []
  glm = Lmer(formula=formula, data=data, family='gaussian')
  glm.fit();
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example
---------------------------------------------------------------------------
RRuntimeError                             Traceback (most recent call last)
Cell In[77], line 13
     11 results = []
     12 glm = Lmer(formula=formula, data=data, family='gaussian')
---> 13 glm.fit();

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/pymer4/models/Lmer.py:424, in Lmer.fit(self, conf_int, n_boot, factors, permute, ordered, verbose, REML, rank, rank_group, rank_exclude_cols, no_warnings, control, old_optimizer, **kwargs)
    422     lmer = importr("lmerTest")
    423     lmc = robjects.r(f"lmerControl({control})")
--> 424     self.model_obj = lmer.lmer(
    425         self.formula, data=data, REML=REML, control=lmc, contrasts=contrasts
    426     )
    427 else:
    428     if verbose:

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/rpy2/robjects/functions.py:208, in SignatureTranslatedFunction.__call__(self, *args, **kwargs)
    206         v = kwargs.pop(k)
    207         kwargs[r_k] = v
--> 208 return (super(SignatureTranslatedFunction, self)
    209         .__call__(*args, **kwargs))

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/rpy2/robjects/functions.py:131, in Function.__call__(self, *args, **kwargs)
    129     else:
    130         new_kwargs[k] = cv.py2rpy(v)
--> 131 res = super(Function, self).__call__(*new_args, **new_kwargs)
    132 res = cv.rpy2py(res)
    133 return res

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/rpy2/rinterface_lib/conversion.py:45, in _cdata_res_to_rinterface.<locals>._(*args, **kwargs)
     44 def _(*args, **kwargs):
---> 45     cdata = function(*args, **kwargs)
     46     # TODO: test cdata is of the expected CType
     47     return _cdata_to_rinterface(cdata)

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/rpy2/rinterface.py:817, in SexpClosure.__call__(self, *args, **kwargs)
    810     res = rmemory.protect(
    811         openrlib.rlib.R_tryEval(
    812             call_r,
    813             call_context.__sexp__._cdata,
    814             error_occured)
    815     )
    816     if error_occured[0]:
--> 817         raise embedded.RRuntimeError(_rinterface._geterrmessage())
    818 return res

RRuntimeError: Error in eval(predvars, data, env) : object 'mouse' not found
#+end_example
:END:


#+begin_src ipython
print(glm.coefs.Estimate.keys())
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: AttributeError                            Traceback (most recent call last)
: Cell In[78], line 1
: ----> 1 print(glm.coefs.Estimate.keys())
:
: AttributeError: 'NoneType' object has no attribute 'Estimate'
:END:

#+begin_src ipython
plt.figure(figsize=(15, 5))
colors = ['red', 'blue', 'green', 'k']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1

keys = glm.coefs.Estimate.keys()
result = glm.coefs

df = []
for i, key in enumerate(keys):

    if i==0:
        df.append(result.Estimate[key])
    else:
        df.append(result.Estimate['(Intercept)'] + result.Estimate[key])

    if result['P-val'][key]<0.001:
        plt.text(i+1,  0.1 , '***', ha='center', va='bottom')
    elif result['P-val'][key]<0.01:
        plt.text(i+1,   0.1 , '**', ha='center', va='bottom')
    elif result['P-val'][key]<0.05:
        plt.text(i+1,   0.1 , '*', ha='center', va='bottom')


plt.plot(np.arange(1, 7), df, '-o')
plt.axhline(y=0, color='black', linestyle='--')
plt.ylabel('$\\beta_{Day}$')
plt.xlabel('Day')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
: ---------------------------------------------------------------------------
: AttributeError                            Traceback (most recent call last)
: Cell In[79], line 5
:       2 colors = ['red', 'blue', 'green', 'k']
:       3 space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1
: ----> 5 keys = glm.coefs.Estimate.keys()
:       6 result = glm.coefs
:       8 df = []
:
: AttributeError: 'NoneType' object has no attribute 'Estimate'
: <Figure size 1500x500 with 0 Axes>
:END:

#+begin_src ipython

#+end_src

#+RESULTS:

* Distractor Early Delay
** behavior ~ day * tasks * overlaps

#+begin_src ipython
import rpy2.robjects as robjects
from rpy2.robjects.packages import importr

# Set the .libPaths in R
custom_r_libpath = '~/R/x86_64-pc-linux-gnu-library/4.3/'
robjects.r('.libPaths("{0}")'.format(custom_r_libpath))

from pymer4.models import Lmer
#+end_src

#+RESULTS:

#+begin_src ipython
  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('int')

  print(y.behavior.unique())

  formula = 'behavior ~ day * tasks * dist_ED + (1+ tasks + day + dist_ED | mouse)'

  results = []
  data = y.copy()
  # data = data[data.mouse != 'JawsM18']

  glm = Lmer(formula=formula, data=data, family='binomial')
  result = glm.fit()
#+end_src

#+RESULTS:
#+begin_example
[1 0]
[1] "failure to converge in 10000 evaluations"
[2] " \n"

boundary (singular) fit: see help('isSingular')

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~day*tasks*dist_ED+(1+tasks+day+dist_ED|mouse)

Family: binomial	 Inference: parametric

Number of observations: 3648	 Groups: {'mouse': 5.0}

Log-likelihood: -1751.956 	 AIC: 3557.911

Random effects:

                Name    Var    Std
mouse    (Intercept)  0.185  0.430
mouse    tasksDualGo  0.086  0.293
mouse  tasksDualNoGo  0.013  0.114
mouse            day  0.033  0.181
mouse        dist_ED  0.004  0.064

                 IV1            IV2   Corr
mouse    (Intercept)    tasksDualGo -0.001
mouse    (Intercept)  tasksDualNoGo -0.535
mouse    (Intercept)            day -0.089
mouse    (Intercept)        dist_ED  0.947
mouse    tasksDualGo  tasksDualNoGo  0.846
mouse    tasksDualGo            day -0.540
mouse    tasksDualGo        dist_ED  0.314
mouse  tasksDualNoGo            day -0.409
mouse  tasksDualNoGo        dist_ED -0.240
mouse            day        dist_ED -0.303

Fixed effects:
#+end_example

#+begin_src ipython
random_effects = glm.ranef
print(random_effects)
#+end_src

#+RESULTS:
:          X.Intercept.  tasksDualGo  tasksDualNoGo       day   dist_ED
: ACCM03      -0.419763    -0.321440      -0.046173  0.141837 -0.081215
: ACCM04      -0.457873     0.344073       0.177631 -0.184507 -0.038531
: ChRM04       0.535574     0.177842      -0.017322 -0.084956  0.087309
: JawsM15      0.151374    -0.111512      -0.057990 -0.136626  0.017701
: JawsM18      0.191389    -0.078830      -0.052919  0.249132  0.015813

#+begin_src ipython
print(result['P-val'])
#+end_src

#+RESULTS:
#+begin_example
(Intercept)                  0.976
day                          0.000
tasksDualGo                  0.757
tasksDualNoGo                0.644
dist_ED                      0.819
day:tasksDualGo              0.187
day:tasksDualNoGo            0.249
day:dist_ED                  0.533
tasksDualGo:dist_ED          0.858
tasksDualNoGo:dist_ED        0.347
day:tasksDualGo:dist_ED      0.567
day:tasksDualNoGo:dist_ED    0.784
Name: P-val, dtype: float64
#+end_example

#+begin_src ipython
# plt.figure(figsize=(15, 5))
colors = ['blue', 'green', 'red', 'purple', 'orange']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .5

keys = ['(Intercept)', 'tasksDualGo', 'tasksDualNoGo']
# keys = result.Estimate.keys()

for i, key in enumerate(keys):
     if key == '(Intercept)':
          df = result.Estimate['(Intercept)']+ random_effects['X.Intercept.']
     else:
          df = result.Estimate['(Intercept)']+ result.Estimate[key] + random_effects[key]

     mean_value = df.mean()
     std_dev = df.std()

     if result['P-val'][key]<0.001:
          plt.text(i,   1.51, '***', ha='center', va='bottom')
     elif result['P-val'][key]<0.01:
          plt.text(i,   1.51, '**', ha='center', va='bottom')
     elif result['P-val'][key]<0.05:
          plt.text(i,   1.51, '*', ha='center', va='bottom')

     # Plot individual points
     plt.scatter(i * np.ones(df.shape[0]) + space, df, color=colors)
     # Plot mean and stddev as error bars
     plt.plot(i, mean_value, '_k', ms=20)
     plt.errorbar(i * np.ones(df.shape[0]), [mean_value]*len(df), yerr=[std_dev]*len(df), fmt='-', linestyle='None', color='k', capsize=15)

plt.axhline(y=0, color='black', linestyle='--')
plt.xticks(np.arange(len(keys)), keys)
plt.ylim([-1.5,1.5])
plt.ylabel('$\\beta$')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_42.png]]
#+RESULTS:

#+begin_src ipython
# plt.figure(figsize=(15, 5))
colors = ['blue', 'green', 'red', 'purple', 'orange']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1

keys = ['day', 'day:tasksDualGo', 'day:tasksDualNoGo']
# keys = result.Estimate.keys()
for i, key in enumerate(keys):
     if i == 0 :
          df = result.Estimate[key] + random_effects['day']
     else:
          df = result.Estimate['day']+ result.Estimate[key] + random_effects['day']

     mean_value = df.mean()
     std_dev = df.std()

     if result['P-val'][key]<0.001:
          plt.text(i,   1.01, '***', ha='center', va='bottom')
     elif result['P-val'][key]<0.01:
          plt.text(i,   1.01, '**', ha='center', va='bottom')
     elif result['P-val'][key]<0.05:
          plt.text(i,   1.01, '*', ha='center', va='bottom')

     # Plot individual points
     plt.scatter(i * np.ones(df.shape[0]) + space, df, color=colors)
     # Plot mean and stddev as error bars
     plt.plot(i, mean_value, '_k', ms=20)
     plt.errorbar(i * np.ones(df.shape[0]), [mean_value]*len(df), yerr=[std_dev]*len(df), fmt='-', linestyle='None', color='k', capsize=15)

plt.axhline(y=0, color='black', linestyle='--')
plt.xticks(np.arange(len(keys)), keys)
plt.ylim([-1,1])
plt.ylabel('$\\beta$')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_43.png]]

#+begin_src ipython
# plt.figure(figsize=(15, 5))
colors = ['blue', 'green', 'red', 'purple', 'orange']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1

keys = ['dist_ED', 'tasksDualGo:dist_ED', 'tasksDualNoGo:dist_ED']
for i, key in enumerate(keys):
     if i==0:
          df = result.Estimate[key] + random_effects['dist_ED']
     else:
          df = result.Estimate['dist_ED']+ result.Estimate[key] + random_effects['dist_ED']

     mean_value = df.mean()
     std_dev = df.std()

     if result['P-val'][key]<0.001:
          plt.text(i,   1.51, '***', ha='center', va='bottom')
     elif result['P-val'][key]<0.01:
          plt.text(i,   1.51, '**', ha='center', va='bottom')
     elif result['P-val'][key]<0.05:
          plt.text(i,   1.51, '*', ha='center', va='bottom')

     # Plot individual points
     plt.scatter(i * np.ones(df.shape[0]) + space, df, color=colors)
     # Plot mean and stddev as error bars
     plt.plot(i, mean_value, '_k', ms=20)
     plt.errorbar(i * np.ones(df.shape[0]), [mean_value]*len(df), yerr=[std_dev]*len(df), fmt='-', linestyle='None', color='k', capsize=15)

plt.axhline(y=0, color='black', linestyle='--')
plt.xticks(np.arange(len(keys)), keys)
plt.ylim([-1.5,1.5])
plt.ylabel('$\\beta$')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_44.png]]

#+begin_src ipython
# plt.figure(figsize=(15, 5))
colors = ['blue', 'green', 'red', 'purple', 'orange']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1

keys = ['day:dist_ED', 'day:tasksDualGo:dist_ED', 'day:tasksDualNoGo:dist_ED']
# keys = result.Estimate.keys()
for i, key in enumerate(keys):
     if i == 0 :
          df = result.Estimate[key] + random_effects['day']
     else:
          df = result.Estimate['day:dist_ED']+ result.Estimate[key] + random_effects['day']

     mean_value = df.mean()
     std_dev = df.std()

     if result['P-val'][key]<0.001:
          plt.text(i,   1.01, '***', ha='center', va='bottom')
     elif result['P-val'][key]<0.01:
          plt.text(i,   1.01, '**', ha='center', va='bottom')
     elif result['P-val'][key]<0.05:
          plt.text(i,   1.01, '*', ha='center', va='bottom')

     # Plot individual points
     plt.scatter(i * np.ones(df.shape[0]) + space, df, color=colors)
     # Plot mean and stddev as error bars
     plt.plot(i, mean_value, '_k', ms=20)
     plt.errorbar(i * np.ones(df.shape[0]), [mean_value]*len(df), yerr=[std_dev]*len(df), fmt='-', linestyle='None', color='k', capsize=15)

plt.axhline(y=0, color='black', linestyle='--')
plt.xticks(np.arange(len(keys)), keys)
plt.ylim([-1,1])
plt.ylabel('$\\beta$')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_45.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

** behavior ~ tasks * overlaps, per day

#+begin_src ipython
  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('int')

  results = []
  data = y.copy()
  data = data[data.mouse != 'ACCM04']

  formula = 'behavior ~ tasks * dist_ED + (1 + tasks + dist_ED | mouse)'

  glms = []
  results = []
  for day in y.day.unique():
      glm = Lmer(formula=formula, data=data[data.day==day], family='binomial')
      glm.fit()
      glms.append(glm)
#+end_src

#+RESULTS:
#+begin_example
boundary (singular) fit: see help('isSingular')

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~tasks*dist_ED+(1+tasks+dist_ED|mouse)

Family: binomial	 Inference: parametric

Number of observations: 480	 Groups: {'mouse': 4.0}

Log-likelihood: -324.092 	 AIC: 680.185

Random effects:

                Name    Var    Std
mouse    (Intercept)  0.004  0.061
mouse    tasksDualGo  0.016  0.126
mouse  tasksDualNoGo  0.002  0.048
mouse        dist_ED  0.015  0.122

                 IV1            IV2  Corr
mouse    (Intercept)    tasksDualGo   1.0
mouse    (Intercept)  tasksDualNoGo   1.0
mouse    (Intercept)        dist_ED   1.0
mouse    tasksDualGo  tasksDualNoGo   1.0
mouse    tasksDualGo        dist_ED   1.0
mouse  tasksDualNoGo        dist_ED   1.0

Fixed effects:
Model failed to converge with max|grad| = 0.0171665 (tol = 0.002, component 1)

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~tasks*dist_ED+(1+tasks+dist_ED|mouse)

Family: binomial	 Inference: parametric

Number of observations: 480	 Groups: {'mouse': 4.0}

Log-likelihood: -245.261 	 AIC: 522.523

Random effects:

                Name    Var    Std
mouse    (Intercept)  1.373  1.172
mouse    tasksDualGo  0.055  0.235
mouse  tasksDualNoGo  0.014  0.120
mouse        dist_ED  0.161  0.401

                 IV1            IV2   Corr
mouse    (Intercept)    tasksDualGo -1.000
mouse    (Intercept)  tasksDualNoGo -1.000
mouse    (Intercept)        dist_ED  1.000
mouse    tasksDualGo  tasksDualNoGo  1.000
mouse    tasksDualGo        dist_ED -1.000
mouse  tasksDualNoGo        dist_ED -0.999

Fixed effects:
boundary (singular) fit: see help('isSingular')

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~tasks*dist_ED+(1+tasks+dist_ED|mouse)

Family: binomial	 Inference: parametric

Number of observations: 480	 Groups: {'mouse': 4.0}

Log-likelihood: -204.863 	 AIC: 441.726

Random effects:

                Name    Var    Std
mouse    (Intercept)  0.000  0.000
mouse    tasksDualGo  0.120  0.346
mouse  tasksDualNoGo  0.009  0.092
mouse        dist_ED  0.000  0.016

                 IV1            IV2      Corr
mouse    (Intercept)    tasksDualGo
mouse    (Intercept)  tasksDualNoGo
mouse    (Intercept)        dist_ED
mouse    tasksDualGo  tasksDualNoGo -0.998774
mouse    tasksDualGo        dist_ED -0.998913
mouse  tasksDualNoGo        dist_ED  0.997749

Fixed effects:
Model failed to converge with max|grad| = 0.00976406 (tol = 0.002, component 1)

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~tasks*dist_ED+(1+tasks+dist_ED|mouse)

Family: binomial	 Inference: parametric

Number of observations: 480	 Groups: {'mouse': 4.0}

Log-likelihood: -116.704 	 AIC: 265.409

Random effects:

                Name    Var    Std
mouse    (Intercept)  0.455  0.674
mouse    tasksDualGo  0.221  0.470
mouse  tasksDualNoGo  0.000  0.014
mouse        dist_ED  0.394  0.628

                 IV1            IV2   Corr
mouse    (Intercept)    tasksDualGo  1.000
mouse    (Intercept)  tasksDualNoGo -0.884
mouse    (Intercept)        dist_ED  1.000
mouse    tasksDualGo  tasksDualNoGo -0.873
mouse    tasksDualGo        dist_ED  1.000
mouse  tasksDualNoGo        dist_ED -0.881

Fixed effects:
Model failed to converge with max|grad| = 0.0114473 (tol = 0.002, component 1)

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~tasks*dist_ED+(1+tasks+dist_ED|mouse)

Family: binomial	 Inference: parametric

Number of observations: 480	 Groups: {'mouse': 4.0}

Log-likelihood: -140.217 	 AIC: 312.434

Random effects:

                Name    Var    Std
mouse    (Intercept)  2.223  1.491
mouse    tasksDualGo  0.426  0.653
mouse  tasksDualNoGo  0.072  0.268
mouse        dist_ED  0.000  0.006

                 IV1            IV2   Corr
mouse    (Intercept)    tasksDualGo -1.000
mouse    (Intercept)  tasksDualNoGo -1.000
mouse    (Intercept)        dist_ED  0.671
mouse    tasksDualGo  tasksDualNoGo  1.000
mouse    tasksDualGo        dist_ED -0.670
mouse  tasksDualNoGo        dist_ED -0.655

Fixed effects:
Model failed to converge with max|grad| = 0.0806336 (tol = 0.002, component 1)

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~tasks*dist_ED+(1+tasks+dist_ED|mouse)

Family: binomial	 Inference: parametric

Number of observations: 288	 Groups: {'mouse': 3.0}

Log-likelihood: -62.542 	 AIC: 157.085

Random effects:

                Name    Var    Std
mouse    (Intercept)  5.175  2.275
mouse    tasksDualGo  3.697  1.923
mouse  tasksDualNoGo  1.985  1.409
mouse        dist_ED  0.155  0.394

                 IV1            IV2   Corr
mouse    (Intercept)    tasksDualGo -1.000
mouse    (Intercept)  tasksDualNoGo -1.000
mouse    (Intercept)        dist_ED  0.998
mouse    tasksDualGo  tasksDualNoGo  1.000
mouse    tasksDualGo        dist_ED -0.998
mouse  tasksDualNoGo        dist_ED -0.998

Fixed effects:
#+end_example

#+begin_src ipython
coefs = []
rands = []

for day in y.day.unique():
    glm = glms[day-1]
    df = glm.coefs
    df['day'] = day
    coefs.append(df)

    df2 = glm.ranef
    df2['day'] = day
    rands.append(df2)

coefs = pd.concat(coefs)
rands = pd.concat(rands)
# coefs = pd.concat(coefs, ignore_index=True)
#+end_src

#+RESULTS:

#+begin_src ipython
plt.figure(figsize=(15, 5))
colors = ['red', 'blue', 'green', 'k']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1

keys = ['dist_ED', 'tasksDualGo:dist_ED', 'tasksDualNoGo:dist_ED']

for i, key in enumerate(keys):
    df = []
    for day in y.day.unique():
        result = coefs[coefs.day==day]

        if i==0:
            df.append(result.Estimate[key])
        else:
            df.append(result.Estimate['dist_ED'] + result.Estimate[key])

        if result['P-val'][key]<0.001:
            plt.text(day,   1.1 + i * 0.5, '***', ha='center', va='bottom', color=colors[i])
        elif result['P-val'][key]<0.01:
            plt.text(day,   1.1 + i * 0.5, '**', ha='center', va='bottom', color=colors[i])
        elif result['P-val'][key]<0.05:
            plt.text(day,   1.1 + i * 0.5, '*', ha='center', va='bottom', color=colors[i])


    plt.plot(np.arange(1, 7), df, '-o', color=colors[i])
plt.axhline(y=0, color='black', linestyle='--')
plt.ylabel('$\\beta_{Dist_{ED}}$')
plt.xlabel('Day')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_49.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

** behavior ~ overlaps, per day

#+begin_src ipython
  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('int')

  results = []
  data = y.copy()
  # data = data[data.mouse != 'JawsM18']

  formula = 'behavior ~ dist_ED + (1 + dist_ED | mouse)'

  glms = []
  results = []
  for day in y.day.unique():
      glm = Lmer(formula=formula, data=data[data.day==day], family='binomial')
      glm.fit()
      glms.append(glm)
#+end_src

#+RESULTS:
#+begin_example
boundary (singular) fit: see help('isSingular')

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~dist_ED+(1+dist_ED|mouse)

Family: binomial	 Inference: parametric

Number of observations: 672	 Groups: {'mouse': 5.0}

Log-likelihood: -459.334 	 AIC: 928.668

Random effects:

              Name    Var    Std
mouse  (Intercept)  0.026  0.160
mouse      dist_ED  0.072  0.268

               IV1      IV2  Corr
mouse  (Intercept)  dist_ED   1.0

Fixed effects:
boundary (singular) fit: see help('isSingular')

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~dist_ED+(1+dist_ED|mouse)

Family: binomial	 Inference: parametric

Number of observations: 672	 Groups: {'mouse': 5.0}

Log-likelihood: -382.697 	 AIC: 775.394

Random effects:

              Name    Var    Std
mouse  (Intercept)  1.125  1.061
mouse      dist_ED  0.420  0.648

               IV1      IV2  Corr
mouse  (Intercept)  dist_ED   1.0

Fixed effects:
boundary (singular) fit: see help('isSingular')

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~dist_ED+(1+dist_ED|mouse)

Family: binomial	 Inference: parametric

Number of observations: 672	 Groups: {'mouse': 5.0}

Log-likelihood: -344.557 	 AIC: 699.115

Random effects:

              Name    Var    Std
mouse  (Intercept)  0.248  0.498
mouse      dist_ED  0.003  0.058

               IV1      IV2  Corr
mouse  (Intercept)  dist_ED   1.0

Fixed effects:
boundary (singular) fit: see help('isSingular')

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~dist_ED+(1+dist_ED|mouse)

Family: binomial	 Inference: parametric

Number of observations: 672	 Groups: {'mouse': 5.0}

Log-likelihood: -246.041 	 AIC: 502.082

Random effects:

              Name    Var    Std
mouse  (Intercept)  1.291  1.136
mouse      dist_ED  0.028  0.169

               IV1      IV2  Corr
mouse  (Intercept)  dist_ED   1.0

Fixed effects:
boundary (singular) fit: see help('isSingular')

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~dist_ED+(1+dist_ED|mouse)

Family: binomial	 Inference: parametric

Number of observations: 672	 Groups: {'mouse': 5.0}

Log-likelihood: -241.380 	 AIC: 492.760

Random effects:

              Name    Var    Std
mouse  (Intercept)  1.025  1.012
mouse      dist_ED  0.044  0.210

               IV1      IV2  Corr
mouse  (Intercept)  dist_ED  -1.0

Fixed effects:
boundary (singular) fit: see help('isSingular')

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~dist_ED+(1+dist_ED|mouse)

Family: binomial	 Inference: parametric

Number of observations: 288	 Groups: {'mouse': 3.0}

Log-likelihood: -70.467 	 AIC: 150.934

Random effects:

              Name    Var    Std
mouse  (Intercept)  0.541  0.735
mouse      dist_ED  0.343  0.586

               IV1      IV2  Corr
mouse  (Intercept)  dist_ED   1.0

Fixed effects:
#+end_example

#+begin_src ipython
coefs = []
rands = []

for day in y.day.unique():
    glm = glms[day-1]
    df = glm.coefs
    df['day'] = day
    coefs.append(df)

    df2 = glm.ranef
    df2['day'] = day
    rands.append(df2)

coefs = pd.concat(coefs)
rands = pd.concat(rands)
# coefs = pd.concat(coefs, ignore_index=True)
#+end_src

#+RESULTS:

#+begin_src ipython
plt.figure(figsize=(15, 5))
colors = ['k', 'red', 'blue', 'green', 'k']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1

keys = ['dist_ED']

for i, key in enumerate(keys):
    df = []
    for day in y.day.unique():
        result = coefs[coefs.day==day]

        if i==0:
            df.append(result.Estimate[key])
        else:
            df.append(result.Estimate['dist_ED'] + result.Estimate[key])

        if result['P-val'][key]<0.001:
            plt.text(day,  0.1 + i * 0.5, '***', ha='center', va='bottom', color=colors[i])
        elif result['P-val'][key]<0.01:
            plt.text(day,   0.1 + i * 0.5, '**', ha='center', va='bottom', color=colors[i])
        elif result['P-val'][key]<0.05:
            plt.text(day,   0.1 + i * 0.5, '*', ha='center', va='bottom', color=colors[i])


    plt.plot(np.arange(1, 7), df, '-o', color=colors[i])
plt.axhline(y=0, color='black', linestyle='--')
plt.ylabel('$\\beta_{Dist_{ED}}$')
plt.xlabel('Day')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_53.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

** overlaps ~ tasks

#+begin_src ipython
  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('int')

  results = []
  data = y.copy()
  # data = data[data.mouse != 'JawsM18']

  formula = 'dist_ED ~ tasks + (1 + tasks | mouse)'

  glms = []
  results = []
  for day in y.day.unique():
      glm = Lmer(formula=formula, data=data[data.day==day], family='gaussian')
      glm.fit()
      glms.append(glm)
#+end_src

#+RESULTS:
#+begin_example
Linear mixed model fit by REML [’lmerMod’]
Formula: dist_ED~tasks+(1+tasks|mouse)

Family: gaussian	 Inference: parametric

Number of observations: 672	 Groups: {'mouse': 5.0}

Log-likelihood: -250.256 	 AIC: 520.512

Random effects:

                   Name    Var    Std
mouse       (Intercept)  0.042  0.205
mouse       tasksDualGo  0.065  0.256
mouse     tasksDualNoGo  0.009  0.095
Residual                 0.116  0.341

               IV1            IV2   Corr
mouse  (Intercept)    tasksDualGo  0.689
mouse  (Intercept)  tasksDualNoGo  0.057
mouse  tasksDualGo  tasksDualNoGo  0.558

Fixed effects:
boundary (singular) fit: see help('isSingular')

Linear mixed model fit by REML [’lmerMod’]
Formula: dist_ED~tasks+(1+tasks|mouse)

Family: gaussian	 Inference: parametric

Number of observations: 672	 Groups: {'mouse': 5.0}

Log-likelihood: -195.331 	 AIC: 410.661

Random effects:

                   Name    Var    Std
mouse       (Intercept)  0.099  0.314
mouse       tasksDualGo  0.013  0.116
mouse     tasksDualNoGo  0.024  0.156
Residual                 0.098  0.313

               IV1            IV2   Corr
mouse  (Intercept)    tasksDualGo  0.337
mouse  (Intercept)  tasksDualNoGo -0.270
mouse  tasksDualGo  tasksDualNoGo -0.998

Fixed effects:
boundary (singular) fit: see help('isSingular')

Linear mixed model fit by REML [’lmerMod’]
Formula: dist_ED~tasks+(1+tasks|mouse)

Family: gaussian	 Inference: parametric

Number of observations: 672	 Groups: {'mouse': 5.0}

Log-likelihood: -154.862 	 AIC: 329.724

Random effects:

                   Name    Var    Std
mouse       (Intercept)  0.112  0.335
mouse       tasksDualGo  0.035  0.187
mouse     tasksDualNoGo  0.001  0.036
Residual                 0.087  0.295

               IV1            IV2   Corr
mouse  (Intercept)    tasksDualGo  0.594
mouse  (Intercept)  tasksDualNoGo  0.932
mouse  tasksDualGo  tasksDualNoGo  0.844

Fixed effects:
Linear mixed model fit by REML [’lmerMod’]
Formula: dist_ED~tasks+(1+tasks|mouse)

Family: gaussian	 Inference: parametric

Number of observations: 672	 Groups: {'mouse': 5.0}

Log-likelihood: -308.999 	 AIC: 637.997

Random effects:

                   Name    Var    Std
mouse       (Intercept)  0.117  0.342
mouse       tasksDualGo  0.089  0.299
mouse     tasksDualNoGo  0.025  0.157
Residual                 0.138  0.371

               IV1            IV2   Corr
mouse  (Intercept)    tasksDualGo -0.904
mouse  (Intercept)  tasksDualNoGo  0.104
mouse  tasksDualGo  tasksDualNoGo  0.133

Fixed effects:
boundary (singular) fit: see help('isSingular')

Linear mixed model fit by REML [’lmerMod’]
Formula: dist_ED~tasks+(1+tasks|mouse)

Family: gaussian	 Inference: parametric

Number of observations: 672	 Groups: {'mouse': 5.0}

Log-likelihood: -443.367 	 AIC: 906.733

Random effects:

                   Name    Var    Std
mouse       (Intercept)  0.133  0.364
mouse       tasksDualGo  0.043  0.206
mouse     tasksDualNoGo  0.076  0.275
Residual                 0.206  0.454

               IV1            IV2   Corr
mouse  (Intercept)    tasksDualGo  0.143
mouse  (Intercept)  tasksDualNoGo -0.565
mouse  tasksDualGo  tasksDualNoGo -0.897

Fixed effects:
boundary (singular) fit: see help('isSingular')

Linear mixed model fit by REML [’lmerMod’]
Formula: dist_ED~tasks+(1+tasks|mouse)

Family: gaussian	 Inference: parametric

Number of observations: 288	 Groups: {'mouse': 3.0}

Log-likelihood: -151.457 	 AIC: 322.915

Random effects:

                   Name    Var    Std
mouse       (Intercept)  0.403  0.635
mouse       tasksDualGo  0.021  0.143
mouse     tasksDualNoGo  0.009  0.097
Residual                 0.155  0.394

               IV1            IV2  Corr
mouse  (Intercept)    tasksDualGo  -1.0
mouse  (Intercept)  tasksDualNoGo   1.0
mouse  tasksDualGo  tasksDualNoGo  -1.0

Fixed effects:
#+end_example

#+begin_src ipython
coefs = []
rands = []

for day in y.day.unique():
    glm = glms[day-1]
    df = glm.coefs
    df['day'] = day
    coefs.append(df)

    df2 = glm.ranef
    df2['day'] = day
    rands.append(df2)

coefs = pd.concat(coefs)
rands = pd.concat(rands)
# coefs = pd.concat(coefs, ignore_index=True)
#+end_src

#+RESULTS:

#+begin_src ipython
plt.figure(figsize=(15, 5))
colors = ['red', 'blue', 'green', 'k']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1

keys = ['(Intercept)', 'tasksDualGo', 'tasksDualNoGo']

for i, key in enumerate(keys):
    df = []
    for day in y.day.unique():
        result = coefs[coefs.day==day]

        if i==0:
            df.append(result.Estimate[key])
        else:
            df.append(result.Estimate['(Intercept)'] + result.Estimate[key])

        if result['P-val'][key]<0.001:
            plt.text(day,  0.1 + i * 0.05, '***', ha='center', va='bottom', color=colors[i])
        elif result['P-val'][key]<0.01:
            plt.text(day,   0.1 + i * 0.05, '**', ha='center', va='bottom', color=colors[i])
        elif result['P-val'][key]<0.05:
            plt.text(day,   0.1 + i * 0.05, '*', ha='center', va='bottom', color=colors[i])


    plt.plot(np.arange(1, 7), df, '-o', color=colors[i])
plt.axhline(y=0, color='black', linestyle='--')
plt.ylabel('$\\beta_{Tasks}$')
plt.xlabel('Day')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_57.png]]

#+begin_src ipython

#+end_src

#+RESULTS:

** overlaps ~ day

#+begin_src ipython
  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')

  results = []
  data = y.copy()
  # data = data[data.mouse != 'ACCM04']

  formula = 'dist_ED ~ day + (1 | mouse)'

  glms = []
  results = []
  glm = Lmer(formula=formula, data=data, family='gaussian')
  glm.fit();
#+end_src

#+RESULTS:
#+begin_example
Linear mixed model fit by REML [’lmerMod’]
Formula: dist_ED~day+(1|mouse)

Family: gaussian	 Inference: parametric

Number of observations: 3648	 Groups: {'mouse': 5.0}

Log-likelihood: -2450.747 	 AIC: 4917.494

Random effects:

                 Name    Var    Std
mouse     (Intercept)  0.018  0.134
Residual               0.222  0.471

No random effect correlations specified

Fixed effects:
#+end_example


#+begin_src ipython
print(glm.coefs.Estimate.keys())
#+end_src

#+RESULTS:
: Index(['(Intercept)', 'day2', 'day3', 'day4', 'day5', 'day6'], dtype='object')

#+begin_src ipython
plt.figure(figsize=(15, 5))
colors = ['red', 'blue', 'green', 'k']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1

keys = glm.coefs.Estimate.keys()
result = glm.coefs

df = []
for i, key in enumerate(keys):

    if i==0:
        df.append(result.Estimate[key])
    else:
        df.append(result.Estimate['(Intercept)'] + result.Estimate[key])

    if result['P-val'][key]<0.001:
        plt.text(i+1,  0.1 , '***', ha='center', va='bottom')
    elif result['P-val'][key]<0.01:
        plt.text(i+1,   0.1 , '**', ha='center', va='bottom')
    elif result['P-val'][key]<0.05:
        plt.text(i+1,   0.1 , '*', ha='center', va='bottom')


plt.plot(np.arange(1, 7), df, '-o')
plt.axhline(y=0, color='black', linestyle='--')
plt.ylabel('$\\beta_{Day}$')
plt.xlabel('Day')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_61.png]]

#+begin_src ipython

#+end_src

#+RESULTS:
