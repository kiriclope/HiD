#+STARTUP: fold
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session decoder :kernel dual_data :exports results :output-dir ./figures/landscape :file (lc/org-babel-tangle-figure-filename)

* Notebook Settings

#+begin_src ipython
%load_ext autoreload
%autoreload 2
%reload_ext autoreload

%run /home/leon/dual_task/dual_data/notebooks/setup.py
%matplotlib inline
%config InlineBackend.figure_format = 'png'
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/dual_data/bin/python

* Imports
#+begin_src ipython
  from sklearn.exceptions import ConvergenceWarning
  warnings.filterwarnings("ignore")

  import sys
  sys.path.insert(0, '/home/leon/dual_task/dual_data/')

  import os
  if not sys.warnoptions:
    warnings.simplefilter("ignore")
    os.environ["PYTHONWARNINGS"] = "ignore"

  import pickle as pkl
  import numpy as np
  import matplotlib.pyplot as plt
  import pandas as pd

  from time import perf_counter

  from sklearn.base import clone
  from sklearn.metrics import make_scorer, roc_auc_score
  from sklearn.preprocessing import StandardScaler, RobustScaler
  from sklearn.model_selection import RepeatedStratifiedKFold, LeaveOneOut, StratifiedKFold

  from src.common.plot_utils import add_vlines, add_vdashed
  from src.common.options import set_options
  from src.stats.bootstrap import my_boots_ci
  from src.common.get_data import get_X_y_days, get_X_y_S1_S2
  from src.preprocess.helpers import avg_epochs

  from src.torch.classificationCV import ClassificationCV
  from src.torch.main import get_classification
#+end_src

#+RESULTS:

* Helpers

#+begin_src ipython
def pad_with_nans(array, target_shape):
    result = np.full(target_shape, np.nan)  # Create an array filled with NaNs
    print(result.shape)
    slices = tuple(slice(0, min(dim, target)) for dim, target in zip(array.shape, target_shape))
    result[slices] = array[slices]
    return result
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  import numpy as np

  def safe_roc_auc_score(y_true, y_score):
      y_true = np.asarray(y_true)
      if len(np.unique(y_true)) == 1:
          return np.nan  # return np.nan where the score cannot be calculated
      return roc_auc_score(y_true, y_score)
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  def rescale_coefs(model, coefs, bias):

          try:
                  means = model.named_steps["scaler"].mean_
                  scales = model.named_steps["scaler"].scale_

                  # Rescale the coefficients
                  rescaled_coefs = np.true_divide(coefs, scales)

                  # Adjust the intercept
                  rescaled_bias = bias - np.sum(rescaled_coefs * means)

                  return rescaled_coefs, rescaled_bias
          except:
                  return coefs, bias

#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  from scipy.stats import bootstrap

  def get_bootstrap_ci(data, statistic=np.mean, confidence_level=0.95, n_resamples=1000, random_state=None):
      result = bootstrap((data,), statistic)
      ci_lower, ci_upper = result.confidence_interval
      return np.array([ci_lower, ci_upper])
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  def convert_seconds(seconds):
      h = seconds // 3600
      m = (seconds % 3600) // 60
      s = seconds % 60
      return h, m, s
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  import pickle as pkl

  def pkl_save(obj, name, path="."):
      os.makedirs(path, exist_ok=True)
      destination = path + "/" + name + ".pkl"
      print("saving to", destination)
      pkl.dump(obj, open(destination, "wb"))


  def pkl_load(name, path="."):
      source = path + "/" + name + '.pkl'
      print('loading from', source)
      return pkl.load(open( source, "rb"))

#+end_src

#+RESULTS:

* Parameters

#+begin_src ipython
  DEVICE = 'cuda:0'
  mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
  N_NEURONS = [668, 693, 444, 361, 113]

  tasks = ['DPA', 'DualGo', 'DualNoGo']
  params = { 'net__alpha': np.logspace(-4, 4, 10),
             # 'net__l1_ratio': np.linspace(0, 1, 10),
             # 'net__module__dropout_rate': np.linspace(0, 1, 10),
            }

  # ['AP02', 'AP12', 'PP09', 'PP17', 'RP17']

  kwargs = {
      'mouse': 'ACCM04', 'laser': 0,
      'trials': '', 'reload': 0, 'data_type': 'dF',
      'prescreen': None, 'pval': 0.05,
      'preprocess': False, 'scaler_BL': 'robust',
      'avg_noise':True, 'unit_var_BL': True,
      'random_state': None, 'T_WINDOW': 0.0,
      'l1_ratio': 0.95,
      'n_comp': None, 'scaler': None,
      'bootstrap': 1, 'n_boots': 128,
      'n_splits': 3, 'n_repeats': 32,
      'class_weight': 0,
      'multilabel':0,
  }

  kwargs['days'] = ['first', 'middle', 'last']
  options = set_options(**kwargs)
  # days = np.arange(1, options['n_days']+1)
  days = ['first', 'middle', 'last']

  safe_roc_auc = make_scorer(safe_roc_auc_score, needs_proba=True)
  options['hp_scoring'] = safe_roc_auc
  options['n_jobs'] = 30
#+end_src

#+RESULTS:

#+begin_src ipython
def overlaps_scorer(estimator, X_test, y_test, IF_SIGN=0):
    coef = estimator.named_steps["net"].coef_.flatten()
    if IF_SIGN:
        dot_product = (2*y_test -1) * np.dot(X_test, coef) / np.linalg.norm(coef)
    else:
        dot_product = -np.dot(X_test, coef) / np.linalg.norm(coef)

    return dot_product.mean()


options['scoring'] = overlaps_scorer
# options['hp_scoring'] = 'overlaps_scorer'
#+end_src

#+RESULTS:

#+begin_src ipython
def signed_overlaps_scorer(estimator, X_test, y_test, IF_SIGN=1):
    coef = estimator.named_steps["net"].coef_.flatten()
    if IF_SIGN:
        dot_product = (2*y_test -1) * np.dot(X_test, coef) / np.linalg.norm(coef)
    else:
        dot_product = -np.dot(X_test, coef) / np.linalg.norm(coef)

    return dot_product.mean()


options['scoring'] = overlaps_scorer
# options['hp_scoring'] = 'overlaps_scorer'
#+end_src

#+RESULTS:

* Decoding vs days
** Model

#+begin_src ipython
from sklearn.linear_model import LogisticRegression
# net = LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced', n_jobs=None)
net = LogisticRegression(penalty='elasticnet', solver='saga', class_weight='balanced', n_jobs=None, l1_ratio=0.95, max_iter=100, tol=.001)
# net = LogisticRegression(penalty='elasticnet', solver='saga', class_weight='balanced', n_jobs=None, l1_ratio=0.95, max_iter=100, tol=.001, multi_class='multinomial')

params = {'net__C': np.logspace(-4, 4, 10)}

options['n_jobs'] = -1
options['verbose'] = 0
model = ClassificationCV(net, params, **options)
options['verbose'] = 1
options['cv'] = LeaveOneOut()
#+end_src

#+RESULTS:

** Sample Overlap

#+begin_src ipython
options['features'] = 'sample'
options['epochs'] = ['ED']
options['scoring'] = signed_overlaps_scorer

tasks = ['DPA', 'DualGo', 'DualNoGo']

dfs = []
for mouse in mice:
    df_mouse = []
    options['mouse'] = mouse

    for task in tasks:
        options['task'] = task
        for day in days:
            options['day'] = day
            overlaps = get_classification(model, RETURN='df_scores', **options)
            options['reload'] = 0
            df_mouse.append(overlaps)

    df_mouse = pd.concat(df_mouse)
    df_mouse['mouse'] = mouse
    dfs.append(df_mouse)

df_mice = pd.concat(dfs)
print(df_mice.shape)
    #+end_src

#+RESULTS:
#+begin_example
Loading files from /home/leon/dual_task/dual_data/data/ChRM04
DATA: FEATURES sample TASK DPA TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (32, 668, 84) X_S2 (32, 668, 84)
X (64, 668, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ChRM04
DATA: FEATURES sample TASK DPA TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (32, 668, 84) X_S2 (32, 668, 84)
X (64, 668, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ChRM04
DATA: FEATURES sample TASK DPA TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 668, 84) X_S2 (32, 668, 84)
X (64, 668, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ChRM04
DATA: FEATURES sample TASK DualGo TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (32, 668, 84) X_S2 (32, 668, 84)
X (64, 668, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ChRM04
DATA: FEATURES sample TASK DualGo TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (32, 668, 84) X_S2 (32, 668, 84)
X (64, 668, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ChRM04
DATA: FEATURES sample TASK DualGo TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 668, 84) X_S2 (32, 668, 84)
X (64, 668, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ChRM04
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (32, 668, 84) X_S2 (32, 668, 84)
X (64, 668, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ChRM04
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (32, 668, 84) X_S2 (32, 668, 84)
X (64, 668, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ChRM04
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 668, 84) X_S2 (32, 668, 84)
X (64, 668, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM15
DATA: FEATURES sample TASK DPA TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
X (64, 693, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM15
DATA: FEATURES sample TASK DPA TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
X (64, 693, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM15
DATA: FEATURES sample TASK DPA TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
X (64, 693, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM15
DATA: FEATURES sample TASK DualGo TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
X (64, 693, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM15
DATA: FEATURES sample TASK DualGo TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
X (64, 693, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM15
DATA: FEATURES sample TASK DualGo TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
X (64, 693, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM15
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
X (64, 693, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM15
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
X (64, 693, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM15
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
X (64, 693, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM18
DATA: FEATURES sample TASK DPA TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (32, 444, 84) X_S2 (32, 444, 84)
X (64, 444, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM18
DATA: FEATURES sample TASK DPA TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (32, 444, 84) X_S2 (32, 444, 84)
X (64, 444, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM18
DATA: FEATURES sample TASK DPA TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 444, 84) X_S2 (32, 444, 84)
X (64, 444, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM18
DATA: FEATURES sample TASK DualGo TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (32, 444, 84) X_S2 (32, 444, 84)
X (64, 444, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM18
DATA: FEATURES sample TASK DualGo TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (32, 444, 84) X_S2 (32, 444, 84)
X (64, 444, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM18
DATA: FEATURES sample TASK DualGo TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 444, 84) X_S2 (32, 444, 84)
X (64, 444, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM18
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (32, 444, 84) X_S2 (32, 444, 84)
X (64, 444, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM18
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (32, 444, 84) X_S2 (32, 444, 84)
X (64, 444, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM18
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 444, 84) X_S2 (32, 444, 84)
X (64, 444, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM03
DATA: FEATURES sample TASK DPA TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (64, 361, 84) X_S2 (64, 361, 84)
X (128, 361, 84) y (128,) [0. 1.]
X_test==X_train
scores (128, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM03
DATA: FEATURES sample TASK DPA TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (64, 361, 84) X_S2 (64, 361, 84)
X (128, 361, 84) y (128,) [0. 1.]
X_test==X_train
scores (128, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM03
DATA: FEATURES sample TASK DPA TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 361, 84) X_S2 (32, 361, 84)
X (64, 361, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM03
DATA: FEATURES sample TASK DualGo TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (64, 361, 84) X_S2 (64, 361, 84)
X (128, 361, 84) y (128,) [0. 1.]
X_test==X_train
scores (128, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM03
DATA: FEATURES sample TASK DualGo TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (64, 361, 84) X_S2 (64, 361, 84)
X (128, 361, 84) y (128,) [0. 1.]
X_test==X_train
scores (128, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM03
DATA: FEATURES sample TASK DualGo TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 361, 84) X_S2 (32, 361, 84)
X (64, 361, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM03
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (64, 361, 84) X_S2 (64, 361, 84)
X (128, 361, 84) y (128,) [0. 1.]
X_test==X_train
scores (128, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM03
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (64, 361, 84) X_S2 (64, 361, 84)
X (128, 361, 84) y (128,) [0. 1.]
X_test==X_train
scores (128, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM03
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 361, 84) X_S2 (32, 361, 84)
X (64, 361, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM04
DATA: FEATURES sample TASK DPA TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (64, 113, 84) X_S2 (64, 113, 84)
X (128, 113, 84) y (128,) [0. 1.]
X_test==X_train
scores (128, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM04
DATA: FEATURES sample TASK DPA TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (64, 113, 84) X_S2 (64, 113, 84)
X (128, 113, 84) y (128,) [0. 1.]
X_test==X_train
scores (128, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM04
DATA: FEATURES sample TASK DPA TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 113, 84) X_S2 (32, 113, 84)
X (64, 113, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM04
DATA: FEATURES sample TASK DualGo TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (64, 113, 84) X_S2 (64, 113, 84)
X (128, 113, 84) y (128,) [0. 1.]
X_test==X_train
scores (128, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM04
DATA: FEATURES sample TASK DualGo TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (64, 113, 84) X_S2 (64, 113, 84)
X (128, 113, 84) y (128,) [0. 1.]
X_test==X_train
scores (128, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM04
DATA: FEATURES sample TASK DualGo TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 113, 84) X_S2 (32, 113, 84)
X (64, 113, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM04
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (64, 113, 84) X_S2 (64, 113, 84)
X (128, 113, 84) y (128,) [0. 1.]
X_test==X_train
scores (128, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM04
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (64, 113, 84) X_S2 (64, 113, 84)
X (128, 113, 84) y (128,) [0. 1.]
X_test==X_train
scores (128, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM04
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 113, 84) X_S2 (32, 113, 84)
X (64, 113, 84) y (64,) [0. 1.]
X_test==X_train
scores (64, 84, 84)
(64, 1) (64, 8)
df (64, 9)
(3648, 10)
#+end_example

#+begin_src ipython
df_mice['performance'] = df_mice['response'].apply(lambda x: 0 if 'incorrect' in x else 1)
df_mice['pair'] = df_mice['response'].apply(lambda x: 0 if (('rej' in x) or ('fa' in x)) else 1)
#+end_src

#+RESULTS:

#+begin_src ipython
name = 'df_sample_overlaps'
pkl_save(df_mice, '%s' % name, path="../data/mice/overlaps")
#+end_src

#+RESULTS:
: saving to ../data/mice/overlaps/df_sample_overlaps.pkl

#+begin_src ipython
options['epochs'] = ['ED']
df_mice['overlaps_LD'] = df_mice['overlaps'].apply(lambda x: avg_epochs(np.array(x).reshape(84, 84).T, **options))
#+end_src
#+RESULTS:

#+begin_src ipython
options['epochs'] = ['LD']
df_mice['overlaps_ED_LD'] = df_mice['overlaps'].apply(lambda x: avg_epochs(np.array(x), **options))
print(df_mice.tail())
#+end_src

#+RESULTS:
#+begin_example
    sample_odor  test_odor     response     tasks  laser   day  dist_odor  \
59          1.0        0.0  correct_rej  DualNoGo    0.0  last        1.0
60          1.0        0.0  correct_rej  DualNoGo    0.0  last        1.0
61          1.0        1.0  correct_hit  DualNoGo    0.0  last        1.0
62          1.0        0.0  correct_rej  DualNoGo    0.0  last        1.0
63          1.0        1.0  correct_hit  DualNoGo    0.0  last        1.0

    choice                                           overlaps   mouse  \
59     0.0  [0.021860120818018913, -0.1765112578868866, -0...  ACCM04
60     0.0  [-0.007423197850584984, -0.022668523713946342,...  ACCM04
61     1.0  [0.00963343121111393, 0.028276054188609123, 0....  ACCM04
62     0.0  [0.03200778737664223, -0.019482169300317764, 0...  ACCM04
63     1.0  [0.0785323828458786, 0.06033065915107727, 0.15...  ACCM04

    performance  pair                                        overlaps_LD  \
59            1     0  [-0.0011915224604308605, -0.08283980563282967,...
60            1     0  [-0.010635368500111831, -0.08864985571967231, ...
61            1     1  [-0.12521141457060972, 0.038258075869331755, -...
62            1     0  [0.06355245248414576, 0.09502256516781118, 0.0...
63            1     1  [-0.18541216622624132, -0.10632237512618303, -...

    overlaps_ED_LD
59       -0.156789
60        0.341610
61       -0.199051
62       -0.406881
63       -0.167328
#+end_example

#+begin_src ipython

#+end_src

** Distractor overlap
*** overlaps

#+begin_src ipython
options['features'] = 'distractor'
options['epochs'] = ['MD']
options['scoring'] = overlaps_scorer

tasks = ['DPA', 'Dual']
dfs = []
for mouse in mice:
    df_mouse = []
    options['mouse'] = mouse

    for task in tasks:
        options['task'] = task
        for day in days:
            options['day'] = day
            overlaps = get_classification(model, RETURN='df_scores', **options)
            options['reload'] = 0
            df_mouse.append(overlaps)

    df_mouse = pd.concat(df_mouse)
    df_mouse['mouse'] = mouse
    dfs.append(df_mouse)

df_mice = pd.concat(dfs)
print(df_mice.shape)
    #+end_src

#+RESULTS:
#+begin_example
Loading files from /home/leon/dual_task/dual_data/data/ChRM04
DATA: FEATURES sample TASK DPA TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (32, 668, 84) X_S2 (32, 668, 84)
X_test (64, 668, 84) y_test (64,)
DATA: FEATURES distractor TASK Dual TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (64, 668, 84) X_S2 (64, 668, 84)
X (128, 668, 84) y (128,) [0. 1. 2. 3.]
scores (64, 84, 84)
scores (64, 1, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ChRM04
DATA: FEATURES sample TASK DPA TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (32, 668, 84) X_S2 (32, 668, 84)
X_test (64, 668, 84) y_test (64,)
DATA: FEATURES distractor TASK Dual TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (64, 668, 84) X_S2 (64, 668, 84)
X (128, 668, 84) y (128,) [0. 1. 2. 3.]
scores (64, 84, 84)
scores (64, 1, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ChRM04
DATA: FEATURES sample TASK DPA TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 668, 84) X_S2 (32, 668, 84)
X_test (64, 668, 84) y_test (64,)
DATA: FEATURES distractor TASK Dual TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (64, 668, 84) X_S2 (64, 668, 84)
X (128, 668, 84) y (128,) [0. 1. 2. 3.]
scores (64, 84, 84)
scores (64, 1, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ChRM04
DATA: FEATURES distractor TASK Dual TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (64, 668, 84) X_S2 (64, 668, 84)
X (128, 668, 84) y (128,) [0. 1. 2. 3.]
X_test==X_train
scores (128, 84, 84)
scores (64, 2, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ChRM04
DATA: FEATURES distractor TASK Dual TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (64, 668, 84) X_S2 (64, 668, 84)
X (128, 668, 84) y (128,) [0. 1. 2. 3.]
X_test==X_train
scores (128, 84, 84)
scores (64, 2, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ChRM04
DATA: FEATURES distractor TASK Dual TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (64, 668, 84) X_S2 (64, 668, 84)
X (128, 668, 84) y (128,) [0. 1. 2. 3.]
X_test==X_train
scores (128, 84, 84)
scores (64, 2, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM15
DATA: FEATURES sample TASK DPA TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
X_test (64, 693, 84) y_test (64,)
DATA: FEATURES distractor TASK Dual TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (64, 693, 84) X_S2 (64, 693, 84)
X (128, 693, 84) y (128,) [0. 1. 2. 3.]
scores (64, 84, 84)
scores (64, 1, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM15
DATA: FEATURES sample TASK DPA TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
X_test (64, 693, 84) y_test (64,)
DATA: FEATURES distractor TASK Dual TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (64, 693, 84) X_S2 (64, 693, 84)
X (128, 693, 84) y (128,) [0. 1. 2. 3.]
scores (64, 84, 84)
scores (64, 1, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM15
DATA: FEATURES sample TASK DPA TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
X_test (64, 693, 84) y_test (64,)
DATA: FEATURES distractor TASK Dual TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (64, 693, 84) X_S2 (64, 693, 84)
X (128, 693, 84) y (128,) [0. 1. 2. 3.]
scores (64, 84, 84)
scores (64, 1, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM15
DATA: FEATURES distractor TASK Dual TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (64, 693, 84) X_S2 (64, 693, 84)
X (128, 693, 84) y (128,) [0. 1. 2. 3.]
X_test==X_train
scores (128, 84, 84)
scores (64, 2, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM15
DATA: FEATURES distractor TASK Dual TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (64, 693, 84) X_S2 (64, 693, 84)
X (128, 693, 84) y (128,) [0. 1. 2. 3.]
X_test==X_train
scores (128, 84, 84)
scores (64, 2, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM15
DATA: FEATURES distractor TASK Dual TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (64, 693, 84) X_S2 (64, 693, 84)
X (128, 693, 84) y (128,) [0. 1. 2. 3.]
X_test==X_train
scores (128, 84, 84)
scores (64, 2, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM18
DATA: FEATURES sample TASK DPA TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (32, 444, 84) X_S2 (32, 444, 84)
X_test (64, 444, 84) y_test (64,)
DATA: FEATURES distractor TASK Dual TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (64, 444, 84) X_S2 (64, 444, 84)
X (128, 444, 84) y (128,) [0. 1. 2. 3.]
scores (64, 84, 84)
scores (64, 1, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM18
DATA: FEATURES sample TASK DPA TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (32, 444, 84) X_S2 (32, 444, 84)
X_test (64, 444, 84) y_test (64,)
DATA: FEATURES distractor TASK Dual TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (64, 444, 84) X_S2 (64, 444, 84)
X (128, 444, 84) y (128,) [0. 1. 2. 3.]
scores (64, 84, 84)
scores (64, 1, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM18
DATA: FEATURES sample TASK DPA TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 444, 84) X_S2 (32, 444, 84)
X_test (64, 444, 84) y_test (64,)
DATA: FEATURES distractor TASK Dual TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (64, 444, 84) X_S2 (64, 444, 84)
X (128, 444, 84) y (128,) [0. 1. 2. 3.]
scores (64, 84, 84)
scores (64, 1, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM18
DATA: FEATURES distractor TASK Dual TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (64, 444, 84) X_S2 (64, 444, 84)
X (128, 444, 84) y (128,) [0. 1. 2. 3.]
X_test==X_train
scores (128, 84, 84)
scores (64, 2, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM18
DATA: FEATURES distractor TASK Dual TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (64, 444, 84) X_S2 (64, 444, 84)
X (128, 444, 84) y (128,) [0. 1. 2. 3.]
X_test==X_train
scores (128, 84, 84)
scores (64, 2, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/JawsM18
DATA: FEATURES distractor TASK Dual TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (64, 444, 84) X_S2 (64, 444, 84)
X (128, 444, 84) y (128,) [0. 1. 2. 3.]
X_test==X_train
scores (128, 84, 84)
scores (64, 2, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM03
DATA: FEATURES sample TASK DPA TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (64, 361, 84) X_S2 (64, 361, 84)
X_test (128, 361, 84) y_test (128,)
DATA: FEATURES distractor TASK Dual TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (128, 361, 84) X_S2 (128, 361, 84)
X (256, 361, 84) y (256,) [0. 1. 2. 3.]
scores (128, 84, 84)
scores (128, 1, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM03
DATA: FEATURES sample TASK DPA TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (64, 361, 84) X_S2 (64, 361, 84)
X_test (128, 361, 84) y_test (128,)
DATA: FEATURES distractor TASK Dual TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (128, 361, 84) X_S2 (128, 361, 84)
X (256, 361, 84) y (256,) [0. 1. 2. 3.]
scores (128, 84, 84)
scores (128, 1, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM03
DATA: FEATURES sample TASK DPA TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 361, 84) X_S2 (32, 361, 84)
X_test (64, 361, 84) y_test (64,)
DATA: FEATURES distractor TASK Dual TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (64, 361, 84) X_S2 (64, 361, 84)
X (128, 361, 84) y (128,) [0. 1. 2. 3.]
scores (64, 84, 84)
scores (64, 1, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM03
DATA: FEATURES distractor TASK Dual TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (128, 361, 84) X_S2 (128, 361, 84)
X (256, 361, 84) y (256,) [0. 1. 2. 3.]
X_test==X_train
scores (256, 84, 84)
scores (128, 2, 84, 84)
(256, 1) (256, 8)
df (256, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM03
DATA: FEATURES distractor TASK Dual TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (128, 361, 84) X_S2 (128, 361, 84)
X (256, 361, 84) y (256,) [0. 1. 2. 3.]
X_test==X_train
scores (256, 84, 84)
scores (128, 2, 84, 84)
(256, 1) (256, 8)
df (256, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM03
DATA: FEATURES distractor TASK Dual TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (64, 361, 84) X_S2 (64, 361, 84)
X (128, 361, 84) y (128,) [0. 1. 2. 3.]
X_test==X_train
scores (128, 84, 84)
scores (64, 2, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM04
DATA: FEATURES sample TASK DPA TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (64, 113, 84) X_S2 (64, 113, 84)
X_test (128, 113, 84) y_test (128,)
DATA: FEATURES distractor TASK Dual TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (128, 113, 84) X_S2 (128, 113, 84)
X (256, 113, 84) y (256,) [0. 1. 2. 3.]
scores (128, 84, 84)
scores (128, 1, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM04
DATA: FEATURES sample TASK DPA TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (64, 113, 84) X_S2 (64, 113, 84)
X_test (128, 113, 84) y_test (128,)
DATA: FEATURES distractor TASK Dual TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (128, 113, 84) X_S2 (128, 113, 84)
X (256, 113, 84) y (256,) [0. 1. 2. 3.]
scores (128, 84, 84)
scores (128, 1, 84, 84)
(128, 1) (128, 8)
df (128, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM04
DATA: FEATURES sample TASK DPA TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (32, 113, 84) X_S2 (32, 113, 84)
X_test (64, 113, 84) y_test (64,)
DATA: FEATURES distractor TASK Dual TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (64, 113, 84) X_S2 (64, 113, 84)
X (128, 113, 84) y (128,) [0. 1. 2. 3.]
scores (64, 84, 84)
scores (64, 1, 84, 84)
(64, 1) (64, 8)
df (64, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM04
DATA: FEATURES distractor TASK Dual TRIALS  DAYS first LASER 0
multiple days 0 2 2
X_S1 (128, 113, 84) X_S2 (128, 113, 84)
X (256, 113, 84) y (256,) [0. 1. 2. 3.]
X_test==X_train
scores (256, 84, 84)
scores (128, 2, 84, 84)
(256, 1) (256, 8)
df (256, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM04
DATA: FEATURES distractor TASK Dual TRIALS  DAYS middle LASER 0
multiple days 0 2 2
X_S1 (128, 113, 84) X_S2 (128, 113, 84)
X (256, 113, 84) y (256,) [0. 1. 2. 3.]
X_test==X_train
scores (256, 84, 84)
scores (128, 2, 84, 84)
(256, 1) (256, 8)
df (256, 9)
Loading files from /home/leon/dual_task/dual_data/data/ACCM04
DATA: FEATURES distractor TASK Dual TRIALS  DAYS last LASER 0
multiple days 0 2 2
X_S1 (64, 113, 84) X_S2 (64, 113, 84)
X (128, 113, 84) y (128,) [0. 1. 2. 3.]
X_test==X_train
scores (128, 84, 84)
scores (64, 2, 84, 84)
(128, 1) (128, 8)
df (128, 9)
(3648, 10)
#+end_example

#+begin_src ipython
print(options['day'])
#+end_src

#+RESULTS:
: last

#+begin_src ipython
print(df_mice.head())
#+end_src

#+RESULTS:
#+begin_example
   sample_odor  test_odor      response tasks  laser    day  dist_odor  \
0          0.0        0.0   correct_hit   DPA    0.0  first        NaN
1          0.0        1.0  incorrect_fa   DPA    0.0  first        NaN
2          0.0        1.0  incorrect_fa   DPA    0.0  first        NaN
3          0.0        0.0   correct_hit   DPA    0.0  first        NaN
4          0.0        0.0   correct_hit   DPA    0.0  first        NaN

   choice                                           overlaps   mouse
0     1.0  [0.33869385719299316, 0.43425390124320984, 0.3...  ChRM04
1     1.0  [0.2098347544670105, 0.12883895635604858, 0.23...  ChRM04
2     1.0  [0.9241777062416077, 0.8288626074790955, 0.756...  ChRM04
3     1.0  [0.029684897512197495, -0.114313505589962, -0....  ChRM04
4     1.0  [-0.09056789427995682, -0.1494910717010498, -0...  ChRM04
#+end_example

#+begin_src ipython
print(overlaps.shape)
print(overlaps.head())
#+end_src

#+RESULTS:
#+begin_example
(128, 9)
   sample_odor  test_odor      response   tasks  laser   day  dist_odor  \
0          0.0        0.0   correct_hit  DualGo    0.0  last        0.0
1          0.0        1.0  incorrect_fa  DualGo    0.0  last        0.0
2          1.0        0.0  incorrect_fa  DualGo    0.0  last        0.0
3          1.0        0.0  incorrect_fa  DualGo    0.0  last        0.0
4          0.0        0.0   correct_hit  DualGo    0.0  last        0.0

   choice                                           overlaps
0     1.0  [0.06730170547962189, 0.020876089110970497, -0...
1     1.0  [-0.10135798901319504, -0.07752344757318497, 0...
2     1.0  [0.27986982464790344, 0.020366549491882324, 0....
3     1.0  [-0.24835672974586487, 0.277708500623703, -0.2...
4     1.0  [0.00796190183609724, 0.0032995680812746286, 0...
#+end_example

#+begin_src ipython
df_mice['performance'] = df_mice['response'].apply(lambda x: 0 if 'incorrect' in x else 1)
df_mice['pair'] = df_mice['response'].apply(lambda x: 0 if (('rej' in x) or ('fa' in x)) else 1)
#+end_src

#+RESULTS:

#+begin_src ipython
name = 'df_distractor_overlaps'
pkl_save(df_mice, '%s' % name, path="../data/mice/overlaps")
#+end_src

#+RESULTS:
: saving to ../data/mice/overlaps/df_distractor_overlaps.pkl

#+begin_src ipython
options['epochs'] = ['MD']
df_mice['overlaps_MD'] = df_mice['overlaps'].apply(lambda x: avg_epochs(np.array(x).reshape(84, 84).T, **options))
#+end_src
#+RESULTS:

#+begin_src ipython
options['epochs'] = ['ED']
df_mice['overlaps_MD_ED'] = df_mice['overlaps'].apply(lambda x: avg_epochs(np.array(x), **options))
print(df_mice.head())
#+end_src

#+RESULTS:
#+begin_example
   sample_odor  test_odor      response tasks  laser    day  dist_odor  \
0          0.0        0.0   correct_hit   DPA    0.0  first        NaN
1          0.0        1.0  incorrect_fa   DPA    0.0  first        NaN
2          0.0        1.0  incorrect_fa   DPA    0.0  first        NaN
3          0.0        0.0   correct_hit   DPA    0.0  first        NaN
4          0.0        0.0   correct_hit   DPA    0.0  first        NaN

   choice                                           overlaps   mouse  \
0     1.0  [0.33869385719299316, 0.43425390124320984, 0.3...  ChRM04
1     1.0  [0.2098347544670105, 0.12883895635604858, 0.23...  ChRM04
2     1.0  [0.9241777062416077, 0.8288626074790955, 0.756...  ChRM04
3     1.0  [0.029684897512197495, -0.114313505589962, -0....  ChRM04
4     1.0  [-0.09056789427995682, -0.1494910717010498, -0...  ChRM04

   performance  pair                                        overlaps_MD  \
0            1     1  [-0.08201275172177702, 0.12144934261838596, 0....
1            0     0  [0.07180969386051099, 0.17966431379318237, 0.0...
2            0     0  [0.09106567005316417, 0.036718823636571564, -0...
3            1     1  [-0.07473786237339179, -0.11553772787253062, -...
4            1     1  [-0.03940283227711916, 0.07037405110895634, -0...

   overlaps_MD_ED
0       -0.243245
1       -0.157439
2       -0.609804
3       -0.448176
4        0.186318
#+end_example

    #+begin_src ipython
distractor_overlaps = np.concatenate((np.array(overlaps_dist[0]), np.array(overlaps_dist[1])), axis=2)
print(distractor_overlaps.shape)
#+end_src

#+RESULTS:
: (3, 128, 3, 84, 84)

#+begin_src ipython
name = 'distractor_overlaps'
pkl_save(distractor_overlaps, '%s_%s' % (options['mouse'], name), path="../data/%s/overlaps" % options['mouse'])
#+end_src

#+RESULTS:
: saving to ../data/ACCM04/overlaps/ACCM04_distractor_overlaps.pkl


* Pickle

** Sample dfs

#+begin_src ipython
dfs = []

for mouse in options['mice']:
    name = '%s_df_sample_overlaps' % options['mouse']
    df = pkl_load(name, path="../data/%s/overlaps" % options['mouse'])
    df['mouse'] = mouse

    dfs.append(df)

dfs = pd.concat(dfs)
dfs['behavior'] = dfs['response'].apply(lambda x: 0 if 'incorrect' in x else 1)
dfs['pair'] = dfs['response'].apply(lambda x: 0 if (('rej' in x) or ('fa' in x)) else 1)

print(dfs.head())
#+end_src

#+RESULTS:
#+begin_example
   index  sample_odor  test_odor      response tasks  laser  day  dist_odor  \
0      6          0.0        0.0   correct_hit   DPA    0.0  1.0        NaN
1      9          0.0        0.0   correct_hit   DPA    0.0  1.0        NaN
2     10          0.0        1.0  incorrect_fa   DPA    0.0  1.0        NaN
3     18          0.0        1.0  incorrect_fa   DPA    0.0  1.0        NaN
4     26          0.0        1.0  incorrect_fa   DPA    0.0  1.0        NaN

   choice                                      overlaps_diag  \
0     1.0  [-0.17198432981967926, 0.2414817363023758, 0.1...
1     1.0  [-0.21350054442882538, 0.12840549647808075, 0....
2     1.0  [0.07649800926446915, 0.00802299752831459, 0.0...
3     1.0  [0.06410852074623108, 0.17648139595985413, 0.2...
4     1.0  [-0.1557382196187973, -0.14372573792934418, 0....

                                         overlaps_ED   mouse
0  [0.11371712386608124, 0.05915814341278747, 0.0...  ChRM04
1  [0.10189793507258098, 0.07579134259786871, 0.0...  ChRM04
2  [0.05394936205508808, 0.11763202336927255, -0....  ChRM04
3  [-0.09181576139397091, 0.04750561569299963, 0....  ChRM04
4  [0.23679555786980522, -0.000536787323653698, 0...  ChRM04
#+end_example

#+begin_src ipython
name = 'df_sample_mice'
pkl_save(dfs, name, path="../data/overlaps")
#+end_src

#+RESULTS:
: saving to ../data/overlaps/df_sample_mice.pkl

#+begin_src ipython
import seaborn as sns
sns.lineplot(data=df_mice, x='day', y='performance', hue='tasks', marker='o', legend=0)

# Set plot labels and title
plt.xlabel('Day')
plt.ylabel('Behavior')
plt.title('Behavior vs Day per Task')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_30.png]]

#+begin_src ipython
sns.lineplot(data=df_mice, x='day', y='overlaps_ED', hue='mouse', marker='o', legend=0)
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
Cell In[26], line 1
----> 1 sns.lineplot(data=df_mice, x='day', y='overlaps_ED', hue='mouse', marker='o', legend=0)

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/seaborn/relational.py:618, in lineplot(data, x, y, hue, size, style, units, palette, hue_order, hue_norm, sizes, size_order, size_norm, dashes, markers, style_order, estimator, errorbar, n_boot, seed, orient, sort, err_style, err_kws, legend, ci, ax, **kwargs)
    615 errorbar = _deprecate_ci(errorbar, ci)
    617 variables = _LinePlotter.get_semantics(locals())
--> 618 p = _LinePlotter(
    619     data=data, variables=variables,
    620     estimator=estimator, n_boot=n_boot, seed=seed, errorbar=errorbar,
    621     sort=sort, orient=orient, err_style=err_style, err_kws=err_kws,
    622     legend=legend,
    623 )
    625 p.map_hue(palette=palette, order=hue_order, norm=hue_norm)
    626 p.map_size(sizes=sizes, order=size_order, norm=size_norm)

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/seaborn/relational.py:365, in _LinePlotter.__init__(self, data, variables, estimator, n_boot, seed, errorbar, sort, orient, err_style, err_kws, legend)
    351 def __init__(
    352     self, *,
    353     data=None, variables={},
   (...)
    359     # the kind of plot to draw, but for the time being we need to set
    360     # this information so the SizeMapping can use it
    361     self._default_size_range = (
    362         np.r_[.5, 2] * mpl.rcParams["lines.linewidth"]
    363     )
--> 365     super().__init__(data=data, variables=variables)
    367     self.estimator = estimator
    368     self.errorbar = errorbar

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/seaborn/_oldcore.py:640, in VectorPlotter.__init__(self, data, variables)
    635 # var_ordered is relevant only for categorical axis variables, and may
    636 # be better handled by an internal axis information object that tracks
    637 # such information and is set up by the scale_* methods. The analogous
    638 # information for numeric axes would be information about log scales.
    639 self._var_ordered = {"x": False, "y": False}  # alt., used DefaultDict
--> 640 self.assign_variables(data, variables)
    642 for var, cls in self._semantic_mappings.items():
    643
    644     # Create the mapping function
    645     map_func = partial(cls.map, plotter=self)

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/seaborn/_oldcore.py:701, in VectorPlotter.assign_variables(self, data, variables)
    699 else:
    700     self.input_format = "long"
--> 701     plot_data, variables = self._assign_variables_longform(
    702         data, **variables,
    703     )
    705 self.plot_data = plot_data
    706 self.variables = variables

File ~/mambaforge/envs/dual_data/lib/python3.11/site-packages/seaborn/_oldcore.py:938, in VectorPlotter._assign_variables_longform(self, data, **kwargs)
    933 elif isinstance(val, (str, bytes)):
    934
    935     # This looks like a column name but we don't know what it means!
    937     err = f"Could not interpret value `{val}` for parameter `{key}`"
--> 938     raise ValueError(err)
    940 else:
    941
    942     # Otherwise, assume the value is itself data
    943
    944     # Raise when data object is present and a vector can't matched
    945     if isinstance(data, pd.DataFrame) and not isinstance(val, pd.Series):

ValueError: Could not interpret value `overlaps_ED` for parameter `y`
#+end_example
:END:

** distractor dfs
*** data

#+begin_src ipython
dfs = []

for mouse in options['mice']:
    name = '%s_df_distractor_overlaps' % options['mouse']
    df = pkl_load(name, path="../data/%s/overlaps" % options['mouse'])
    df['mouse'] = mouse

    dfs.append(df)

dfs = pd.concat(dfs)

dfs['behavior'] = dfs['response'].apply(lambda x: 0 if 'incorrect' in x else 1)
dfs['pair'] = dfs['response'].apply(lambda x: 0 if (('rej' in x) or ('fa' in x)) else 1)

print(dfs.head())
#+end_src

#+RESULTS:
#+begin_example
   index  sample_odor  test_odor      response tasks  laser  day  dist_odor  \
0      6          0.0        0.0   correct_hit   DPA    0.0  1.0        NaN
1      9          0.0        0.0   correct_hit   DPA    0.0  1.0        NaN
2     10          0.0        1.0  incorrect_fa   DPA    0.0  1.0        NaN
3     18          0.0        1.0  incorrect_fa   DPA    0.0  1.0        NaN
4     26          0.0        1.0  incorrect_fa   DPA    0.0  1.0        NaN

   choice                                      overlaps_diag  \
0     1.0  [0.11676032841205597, -0.03953419625759125, -0...
1     1.0  [0.12259367108345032, -0.018929380923509598, 0...
2     1.0  [-0.18104955554008484, 0.09383393079042435, 0....
3     1.0  [-0.12184631824493408, 0.1557987630367279, -0....
4     1.0  [-0.07315205782651901, 0.04572875425219536, 0....

                                         overlaps_MD   mouse  behavior  pair
0  [0.03378348393986622, 0.010957124157963941, -0...  ChRM04         1     1
1  [0.012054286897182465, 0.04256816146274408, 0....  ChRM04         1     1
2  [-0.04626151639968157, 0.06041626073420048, -0...  ChRM04         0     0
3  [-0.05099838972091675, -0.07124164483199517, -...  ChRM04         0     0
4  [0.034752229073395334, -0.06487469980493188, -...  ChRM04         0     0
#+end_example

#+begin_src ipython
name = 'df_distractor_mice'
pkl_save(dfs, name, path="../data/overlaps")
#+end_src

#+RESULTS:
: saving to ../data/overlaps/df_distractor_mice.pkl


* GLM

#+begin_src ipython
name = 'df_sample_mice'
df_overlaps = pkl_load(name, path="../data/overlaps")
df_overlaps['behavior'] = df_overlaps['response'].apply(lambda x: 0 if 'incorrect' in x else 1)
df_overlaps['pair'] = df_overlaps['response'].apply(lambda x: 0 if (('rej' in x) or ('fa' in x)) else 1)

df_overlaps['day'] = df_overlaps['day'].apply(lambda x: 'first' if x < 3 else ('middle' if x < 5 else 'last'))

options['epochs'] = ['ED']
df_overlaps['ED'] = df_overlaps['overlaps_ED'].apply(lambda x: avg_epochs(np.array(x), **options))

print(df_overlaps.head())
#+end_src

#+RESULTS:
#+begin_example
loading from ../data/overlaps/df_sample_mice.pkl
   index  sample_odor  test_odor      response tasks  laser    day  dist_odor  \
0      6          0.0        0.0   correct_hit   DPA    0.0  first        NaN
1      9          0.0        0.0   correct_hit   DPA    0.0  first        NaN
2     10          0.0        1.0  incorrect_fa   DPA    0.0  first        NaN
3     18          0.0        1.0  incorrect_fa   DPA    0.0  first        NaN
4     26          0.0        1.0  incorrect_fa   DPA    0.0  first        NaN

   choice                                      overlaps_diag  \
0     1.0  [-0.17198432981967926, 0.2414817363023758, 0.1...
1     1.0  [-0.21350054442882538, 0.12840549647808075, 0....
2     1.0  [0.07649800926446915, 0.00802299752831459, 0.0...
3     1.0  [0.06410852074623108, 0.17648139595985413, 0.2...
4     1.0  [-0.1557382196187973, -0.14372573792934418, 0....

                                         overlaps_ED   mouse  behavior  pair  \
0  [0.11371712386608124, 0.05915814341278747, 0.0...  ChRM04         1     1
1  [0.10189793507258098, 0.07579134259786871, 0.0...  ChRM04         1     1
2  [0.05394936205508808, 0.11763202336927255, -0....  ChRM04         0     0
3  [-0.09181576139397091, 0.04750561569299963, 0....  ChRM04         0     0
4  [0.23679555786980522, -0.000536787323653698, 0...  ChRM04         0     0

         ED
0  0.343289
1  0.012575
2  0.427514
3  0.197421
4  0.259208
#+end_example

#+begin_src ipython
print(df_overlaps.laser.unique())
print(df_overlaps.tasks.unique())
print(df_overlaps.day.unique())

print((df_overlaps.sample_odor==0).mean())
print((df_overlaps.sample_odor==1).mean())

print((df_overlaps.dist_odor==0).mean())
print((df_overlaps.dist_odor==1).mean())

print(np.isnan(df_overlaps.dist_odor).mean())
#+end_src

#+RESULTS:
: [0.]
: ['DPA' 'DualGo' 'DualNoGo']
: ['first' 'middle' 'last']
: 0.5
: 0.5
: 0.3333333333333333
: 0.3333333333333333
: 0.3333333333333333

#+begin_src ipython
df_behavior = df_overlaps[['behavior', 'day', 'tasks']].groupby(['day', 'tasks']).mean().reset_index()
print(df_behavior.head())
#+end_src

#+RESULTS:
:      day     tasks  behavior
: 0  first       DPA  0.531250
: 1  first    DualGo  0.546875
: 2  first  DualNoGo  0.554688
: 3   last       DPA  0.796875
: 4   last    DualGo  0.781250

#+begin_src ipython
import seaborn as sns
sns.lineplot(data=df_behavior, x='day', y='behavior', hue='tasks', marker='o', legend=0)

# Set plot labels and title
plt.xlabel('Day')
plt.ylabel('Behavior')
plt.title('Behavior vs Day per Task')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_42.png]]

#+begin_src ipython

#+end_src

#+begin_src ipython
import rpy2.robjects as robjects
from rpy2.robjects.packages import importr

# Set the .libPaths in R
custom_r_libpath = '~/R/x86_64-pc-linux-gnu-library/4.3/'
robjects.r('.libPaths("{0}")'.format(custom_r_libpath))

from pymer4.models import Lmer
#+end_src

#+RESULTS:

#+begin_src ipython
  df_overlaps['tasks'] = df_overlaps['tasks'].astype('category')
  df_overlaps['day'] = df_overlaps['day'].astype('int')

  print(df_overlaps.behavior.unique() )

  formula = 'behavior ~ day * tasks  + (1+ tasks + day | mouse)'

  results = []
  data = df_overlaps.copy()

  glm = Lmer(formula=formula, data=data, family='binomial')
  result = glm.fit()
  print(result)
#+end_src

#+RESULTS:
#+begin_example
[1 0]
boundary (singular) fit: see help('isSingular')

Linear mixed model fit by maximum likelihood  ['lmerMod']
Formula: behavior~day*tasks+(1+tasks+day|mouse)

Family: binomial	 Inference: parametric

Number of observations: 5760	 Groups: {'mouse': 5.0}

Log-likelihood: -3520.606 	 AIC: 7073.213

Random effects:

                Name  Var  Std
mouse    (Intercept)  0.0  0.0
mouse    tasksDualGo  0.0  0.0
mouse  tasksDualNoGo  0.0  0.0
mouse            day  0.0  0.0

                 IV1            IV2   Corr
mouse    (Intercept)    tasksDualGo -1.000
mouse    (Intercept)  tasksDualNoGo -0.790
mouse    (Intercept)            day -0.835
mouse    tasksDualGo  tasksDualNoGo  0.790
mouse    tasksDualGo            day  0.835
mouse  tasksDualNoGo            day  0.323

Fixed effects:

                   Estimate  2.5_ci  97.5_ci     SE     OR  OR_2.5_ci  \
(Intercept)          -0.389  -0.614   -0.164  0.115  0.678      0.541
day                   0.321   0.256    0.386  0.033  1.378      1.291
tasksDualGo           0.141  -0.178    0.460  0.163  1.152      0.837
tasksDualNoGo        -0.005  -0.324    0.315  0.163  0.995      0.723
day:tasksDualGo      -0.026  -0.118    0.066  0.047  0.975      0.889
day:tasksDualNoGo     0.029  -0.063    0.122  0.047  1.030      0.939

                   OR_97.5_ci   Prob  Prob_2.5_ci  Prob_97.5_ci  Z-stat  \
(Intercept)             0.849  0.404        0.351         0.459  -3.385
day                     1.471  0.580        0.564         0.595   9.673
tasksDualGo             1.584  0.535        0.456         0.613   0.867
tasksDualNoGo           1.370  0.499        0.420         0.578  -0.030
day:tasksDualGo         1.069  0.494        0.471         0.517  -0.545
day:tasksDualNoGo       1.130  0.507        0.484         0.531   0.622

                   P-val  Sig
(Intercept)        0.001  ***
day                0.000  ***
tasksDualGo        0.386
tasksDualNoGo      0.976
day:tasksDualGo    0.586
day:tasksDualNoGo  0.534
#+end_example

#+begin_src ipython
print(result.Estimate)
#+end_src

#+RESULTS:
: (Intercept)         -0.389
: day                  0.321
: tasksDualGo          0.141
: tasksDualNoGo       -0.005
: day:tasksDualGo     -0.026
: day:tasksDualNoGo    0.029
: Name: Estimate, dtype: float64

#+begin_src ipython
print(result['P-val'])
#+end_src

#+RESULTS:
: (Intercept)          0.001
: day                  0.000
: tasksDualGo          0.386
: tasksDualNoGo        0.976
: day:tasksDualGo      0.586
: day:tasksDualNoGo    0.534
: Name: P-val, dtype: float64

#+begin_src ipython
random_effects = glm.ranef
print(random_effects.keys())
#+end_src

#+RESULTS:
: Index(['X.Intercept.', 'tasksDualGo', 'tasksDualNoGo', 'day'], dtype='object')

#+begin_src ipython
# plt.figure(figsize=(15, 5))
colors = ['blue', 'green', 'red', 'purple', 'orange']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .5

keys = ['(Intercept)', 'tasksDualGo', 'tasksDualNoGo']
# keys = result.Estimate.keys()

for i, key in enumerate(keys):
     if key == '(Intercept)':
          df = result.Estimate['(Intercept)']+ random_effects['X.Intercept.']
     else:
          df = result.Estimate['(Intercept)']+ result.Estimate[key] + random_effects[key]

     mean_value = df.mean()
     std_dev = df.std()

     if result['P-val'][key]<0.001:
          plt.text(i,   1.51, '***', ha='center', va='bottom')
     elif result['P-val'][key]<0.01:
          plt.text(i,   1.51, '**', ha='center', va='bottom')
     elif result['P-val'][key]<0.05:
          plt.text(i,   1.51, '*', ha='center', va='bottom')

     # Plot individual points
     plt.scatter(i * np.ones(df.shape[0]) + space, df, color=colors)
     # Plot mean and stddev as error bars
     plt.plot(i, mean_value, '_k', ms=20)
     plt.errorbar(i * np.ones(df.shape[0]), [mean_value]*len(df), yerr=[std_dev]*len(df), fmt='-', linestyle='None', color='k', capsize=15)

plt.axhline(y=0, color='black', linestyle='--')
plt.xticks(np.arange(len(keys)), keys)
plt.ylim([-1.5,1.5])
plt.ylabel('$\\beta$')
plt.xticks(rotation=45, ha='right', fontsize=14) # 'ha' stands for horizontal alignment
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_45.png]]

* afeafaefea
#+begin_src ipython
import pandas as pd
options['n_days'] = 6
y = []
for mouse in options['mice']:
    print(mouse)
    try:
        y_mouse = pkl_load('y_%s.pkl' % mouse)
        y_mouse['mouse'] = mouse
        y.append(y_mouse)
    except:
        pass
y = pd.concat(y)
#+end_src

#+RESULTS:
: ChRM04
: JawsM15
: JawsM18
: ACCM03
: ACCM04

#+begin_src ipython
print(y.keys())
#+end_src

#+RESULTS:
: Index(['sample_odor', 'test_odor', 'response', 'tasks', 'laser', 'day',
:        'dist_odor', 'choice', 'behavior', 'pair', 'sample', 'sample_STIM',
:        'sample_ED', 'sample_MD', 'sample_LD', 'dist', 'dist_STIM', 'dist_ED',
:        'dist_MD', 'dist_LD', 'OED_sign', 'OLD_sign', 'mouse'],
:       dtype='object')

#+begin_src ipython
sample_overlaps = pkl_load('sample_overlaps', path="../data/%s/overlaps" % options['mouse'])
#+end_src

* Data

#+begin_src ipython
if len(days) == 3:
    name = 'overlaps_tasks_days'
else:
    name = 'overlaps_tasks_'

filename = '%s_%s_%.2f_l1_ratio%s.pkl' % (options['mouse'], name, options['l1_ratio'], options['fname'])
print(filename)

try:
      overlaps = pkl_load(filename, path="../data/%s/" % options['mouse'])
      print('overlaps', overlaps.shape)
except:
      print('file not found')
#+end_src

#+RESULTS:
: JawsM15_overlaps_tasks_0.95_l1_ratio.pkl
: overlaps (2, 3, 3, 64, 84, 1)

#+begin_src ipython
overlaps_sample = overlaps[0]
overlaps_dist = overlaps[1]
# overlaps_choice = overlaps[2]
print(overlaps_sample.shape)
#+end_src

#+RESULTS:
: (3, 3, 64, 84, 1)

#+begin_src ipython
overlaps_sample = np.array(overlaps_sample)
print(overlaps_sample.shape)

overlaps_dist = np.array(overlaps_dist)
print(overlaps_dist.shape)
#+end_src

#+RESULTS:
: (3, 3, 64, 84, 1)
: (3, 3, 64, 84, 1)

#+begin_src ipython
  cmap = plt.get_cmap('Blues')
  colors = [cmap((i+1) / options['n_days'] ) for i in range(options['n_days'])]
  cmap = plt.get_cmap('Reds')
  colors2 = [cmap((i+1) / options['n_days'] ) for i in range(options['n_days'])]
  width = 6
  golden_ratio = (5**.5 - 1) / 2

  task = 1
  # mask = ~np.isnan(overlaps_dist).any(axis=2)
  # overlaps_dist = overlaps_dist[:, mask.any(axis=0)]
  options['features'] = 'choice'
  options['preprocess'] = False
  X_days, y_days = get_X_y_days(**options)

  time = np.linspace(0, 14, X_days.shape[-1])

  fig, ax = plt.subplots(3, 2, figsize= [2* width, 3*height])

  for task in range(3):
    for i in range(options['n_days']):
        overlap = overlaps_sample[task][i]
        size = overlap.shape[0] // 2

        sample = overlap[:size].mean(0)
        ax[task][0].plot(time, sample, label=i+1, color = colors[i]);

        sample = overlap[size:].mean(0)
        ax[task][0].plot(time, sample, label=i+1, color = colors[i]);

        # ax[task][0].plot(time, circcvl(overlaps_sample[task][i][:size].mean(0), windowSize=2), label=i+1, color = colors[i]);
        # ax[task][0].plot(time, circcvl(overlaps_sample[task][i][size:].mean(0), windowSize=2), label=i+1, color = colors2[i]);

        # size = overlaps_dist[task][i].shape[0] // 2
        overlap = overlaps_dist[task][i]
        size = overlap.shape[0] // 2
        dist = overlap[:size].mean(0)
        ax[task][1].plot(time, dist, label=i+1, color = colors[i]);

        dist = overlap[size:].mean(0)
        ax[task][1].plot(time, dist, label=i+1, color = colors2[i]);

        # ax[task][1].plot(overlaps_dist[task][i][:size].mean(0), label=i+1, color = colors[i]);
        # ax[task][1].plot(time, circcvl(overlaps_dist[task][i][:size].mean(0), windowSize=2), label=i+1, color = colors[i]);
        # ax[task][1].plot(time, circcvl(overlaps_dist[task][i][size:].mean(0), windowSize=2), label=i+1, color = colors2[i]);

        options['day'] = i+1
        # X, y = get_X_y_S1_S2(X_days, y_days, **options)
        # size = np.sum(y==-1)

        # ax[task][2].plot(time, circcvl(overlaps_choice[task][i][size:].mean(0), windowSize=2), label=i+1, color = colors2[i]);
        # ax[task][2].plot(time, circcvl(overlaps_choice[task][i][:size].mean(0), windowSize=2), label=i+1, color = colors[i]);

    # ax[task][1].legend(fontsize=10)
    ax[task][0].set_xlabel('Time (s)')
    ax[task][1].set_xlabel('Time (s)')
    ax[task][0].set_ylabel('Sample Overlap')
    ax[task][1].set_ylabel('Distractor Overlap')

    for i in range(2):
        ax[task][i].set_xticks(np.arange(0, 16, 2))
        ax[task][i].set_xlim([0, 14])
        add_vlines(ax[task][i])
        # ax[task][i].set_ylim([-20, 20])

  # plt.savefig('%s_overlaps.svg' % options['mouse'], dpi=300)
  # plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: Loading files from /home/leon/dual_task/dual_data/data/JawsM15
[[./figures/landscape/figure_28.png]]
:END:

#+begin_src ipython
import pandas as pd
options['trials'] = ''
options['verbose'] = 0
options['features'] = 'sample'
df = []

X_days, y_days = get_X_y_days(**options)

for task in tasks:
    options['task'] = task
    df2 = []
    for day in days:
        options['day'] = day
        X, y = get_X_y_S1_S2(X_days, y_days, **options)

        df2.append(y)
    df.append(pd.concat(df2))
y = pd.concat(df)
#+end_src

#+RESULTS:
#+begin_example
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
multiple days 0 2 2
X_S1 (32, 693, 84) X_S2 (32, 693, 84)
#+end_example

#+begin_src ipython
# y['choice'] = ~y['choice'].astype('int')
y['behavior'] = y['response'].apply(lambda x: 0 if 'incorrect' in x else 1)
y['pair'] = y['response'].apply(lambda x: 0 if (('rej' in x) or ('fa' in x)) else 1)
#+end_src

#+RESULTS:

#+begin_src ipython
# print(np.vstack(overlaps_dist).shape)
# print(np.vstack(np.vstack(np.swapaxes(overlaps_dist, 0, -3))).shape)
# overlaps  = np.vstack(np.hstack(overlaps_sample)[..., 0])
sample  = overlaps_sample[..., 0].reshape(-1, 84)
dist  = overlaps_dist[..., 0].reshape(-1, 84)
# overlaps = np.vstack(np.vstack(np.swapaxes(overlaps_dist, 0, -3)))[..., 0]
#+end_src

#+RESULTS:

#+begin_src ipython
y['sample'] = sample.tolist()
y['sample'] = y['sample'].apply(np.array)
# y['sample'] = (2*y.sample_odor-1) * y['sample']

options['epochs'] = ['STIM']
y['sample_STIM'] = y['sample'].apply(lambda x: avg_epochs(np.array(x), **options))

options['epochs'] = ['ED']
y['sample_ED'] = y['sample'].apply(lambda x: avg_epochs(np.array(x), **options))

options['epochs'] = ['MD']
y['sample_MD'] = y['sample'].apply(lambda x: avg_epochs(np.array(x), **options))

options['epochs'] = ['LD']
y['sample_LD'] = y['sample'].apply(lambda x: avg_epochs(np.array(x), **options))

print(sample.shape)
#+end_src

#+RESULTS:
: (960, 84)

#+begin_src ipython
y['dist'] = dist.tolist()
y['dist'] = y['dist'].apply(np.array)
# y['dist'] = (2*y.dist_odor-1) * y['dist']

options['epochs'] = ['STIM']
y['dist_STIM'] = y['dist'].apply(lambda x: avg_epochs(np.array(x), **options))

options['epochs'] = ['ED']
y['dist_ED'] = y['dist'].apply(lambda x: avg_epochs(np.array(x), **options))

options['epochs'] = ['MD']
y['dist_MD'] = y['dist'].apply(lambda x: avg_epochs(np.array(x), **options))

options['epochs'] = ['LD']
y['dist_LD'] = y['dist'].apply(lambda x: avg_epochs(np.array(x), **options))

print(dist.shape)
#+end_src

#+RESULTS:
: (960, 84)
:

#+begin_src ipython
y['OED_sign'] = y['dist_ED'].apply(lambda x: 0 if x<=0 else 1)
y['OLD_sign'] = (-(2 * y.sample_odor -1 ) * y['sample_LD']).apply(lambda x: 1 if x<=0 else 0)
#+end_src

#+RESULTS:

#+begin_src ipython
k=23
print(y.sample_odor.iloc[k], y.sample_ED.iloc[k], y.OLD_sign.iloc[k])
#+end_src

#+RESULTS:
: 1.0 -0.9962940578650235 0

#+begin_src ipython
df = y[y.tasks=='DualGo'].copy()
# df['overlaps'] = df['overlaps'].apply(np.array)

# Group by 'day' and compute the mean overlaps for each day
mean_overlaps_by_day = df.groupby('day')['sample'].apply(lambda x: np.mean(np.stack(x)**2, axis=0))

# Prepare data for plotting
mean_overlaps_df = pd.DataFrame(mean_overlaps_by_day.tolist(), index=mean_overlaps_by_day.index)

# Plotting
for idx, row in mean_overlaps_df.iterrows():
    plt.plot(np.linspace(0, 14, 84), row, label=f"Day {idx}")

plt.xlabel('Time (s)')
plt.ylabel('Overlap')
plt.legend(fontsize=10)
add_vlines()
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_33.png]]

* Overlaps
** Sample OLD
*** Tasks

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['mouse'] = y['mouse'].astype('category')
  print(y.sample_odor.unique())
  formula = 'sample_LD ~ tasks -1 '

  results = []
  beta = []
  pval = []

  for day in y.day.unique():
      # data = y[(y['day'] == day) & (y.pair==0)]
      data = y[(y['day'] == day) & (y.mouse == 'JawsM15')]
      data['sample_LD'] = -(2*data.sample_odor-1) * data['sample_LD']
      glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

      result = glm.fit()
      results.append(result)
      beta.append(result.params)
      pval.append(result.pvalues)

print(result.summary())
    #+end_src

#+RESULTS:
#+begin_example
[0. 1.]
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:              sample_LD   No. Observations:                   96
Model:                            GLM   Df Residuals:                       93
Model Family:                Gaussian   Df Model:                            2
Link Function:               Identity   Scale:                         0.38268
Method:                          IRLS   Log-Likelihood:                -88.587
Date:                Wed, 28 Aug 2024   Deviance:                       35.589
Time:                        17:22:39   Pearson chi2:                     35.6
No. Iterations:                     3   Pseudo R-squ. (CS):             0.2218
Covariance Type:            nonrobust
===================================================================================
                      coef    std err          z      P>|z|      [0.025      0.975]
-----------------------------------------------------------------------------------
tasks[DPA]          1.1661      0.109     10.664      0.000       0.952       1.380
tasks[DualGo]       0.4110      0.109      3.758      0.000       0.197       0.625
tasks[DualNoGo]     0.8452      0.109      7.729      0.000       0.631       1.060
===================================================================================
#+end_example

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
for i in range(3):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta).T[i], '-o', color=cols[i])
  print(np.round(np.array(pval).T[i], 3))

  for j, p in enumerate(np.array(pval).T[i]):
    if p < 0.05:
      plt.text(j+1,  np.max(beta) + .01 + i * .05, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('Sample OLD $\\beta_{Task}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.006 0.    0.    0.    0.    0.   ]
: [0.    0.    0.005 0.    0.    0.   ]
: [0.    0.003 0.    0.    0.    0.   ]
[[./figures/landscape/figure_36.png]]
:END:

*** Choice

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'sample_LD ~ choice'

  results = []
  beta = []
  pval = []

  for task in tasks:
    for day in y.day.unique():
        data = y[(y['day'] == day) & (y.tasks==task) & (y.mouse=='JawsM15')]
        data['sample_LD'] = -(2*data.sample_odor-1) * data['sample_LD']
        # data = y[(y['day'] == day)]
        glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

        result = glm.fit()
        results.append(result)
        beta.append(result.params)
        pval.append(result.pvalues)

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:              sample_LD   No. Observations:                   32
Model:                            GLM   Df Residuals:                       30
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.23209
Method:                          IRLS   Log-Likelihood:                -21.004
Date:                Wed, 28 Aug 2024   Deviance:                       6.9628
Time:                        17:33:35   Pearson chi2:                     6.96
No. Iterations:                     3   Pseudo R-squ. (CS):            0.02567
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.7669      0.124      6.165      0.000       0.523       1.011
choice        -0.1495      0.171     -0.876      0.381      -0.484       0.185
==============================================================================
#+end_example

#+begin_src ipython
beta = np.array(beta).reshape((3, options['n_days'], -1))
print(beta.shape)

pval = np.array(pval).reshape((3, options['n_days'], -1))
print(pval.shape)
#+end_src

#+RESULTS:
: (3, 6, 2)
: (3, 6, 2)

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
k=1
for i in range(3):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta)[i, :, k], '-o', color=cols[i])
  print(np.round(pval[i, :, k], 3))
  for j in range(pval.shape[1]):
    if pval[i, j, k] < 0.05:
      plt.text(j+1, np.max(beta[..., k]) + .01 + i * .05, '*', ha='center', va='bottom', color=cols[i])
      # plt.text(j+1, .01, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('SOLD $\\beta_{choice}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.086 0.973 0.381 0.692 0.458 0.186]
: [0.125 0.375 0.296 0.448 0.066 0.767]
: [0.699 0.634 0.679 0.338 0.348 0.419]
[[./figures/landscape/figure_39.png]]
:END:

*** Behavior

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')
  # y['behavior'] = 2*y.behavior -1
  formula = 'sample_LD ~ behavior'

  results = []
  beta = []
  pval = []

  for task in tasks:
    for day in y.day.unique():
        data = y[(y['day'] == day) & (y.tasks==task)]
        data['sample_LD'] = -(2*data.sample_odor-1) * data['sample_LD']
        # data = y[(y['day'] == day)]
        glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

        result = glm.fit()
        results.append(result)
        beta.append(result.params)
        pval.append(result.pvalues)

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:              sample_LD   No. Observations:                  224
Model:                            GLM   Df Residuals:                      222
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.40234
Method:                          IRLS   Log-Likelihood:                -214.87
Date:                Wed, 28 Aug 2024   Deviance:                       89.320
Time:                        17:44:33   Pearson chi2:                     89.3
No. Iterations:                     3   Pseudo R-squ. (CS):            0.01653
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.9551      0.188      5.078      0.000       0.586       1.324
behavior       0.1048      0.054      1.930      0.054      -0.002       0.211
==============================================================================
#+end_example

#+begin_src ipython
beta = np.array(beta).reshape((3, options['n_days'], -1))
print(beta.shape)

pval = np.array(pval).reshape((3, options['n_days'], -1))
print(pval.shape)
#+end_src

#+RESULTS:
: (3, 6, 2)
: (3, 6, 2)

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
k=1
for i in range(3):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta)[i, :, k], '-o', color=cols[i])
  print(pval[i, :, k])
  for j in range(pval.shape[1]):
    if pval[i, j, k] < 0.05:
      plt.text(j+1, .51+i*0.05, '*', ha='center', va='bottom', color=cols[i])
      # plt.text(j+1, .01, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('SOLD $\\beta_{behavior}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.86037155 0.21418035 0.05364159 0.02929007 0.06089178 0.00199354]
: [0.70087702 0.44260844 0.28563124 0.21726989 0.34405583 0.1625283 ]
: [0.43464444 0.3659488  0.05960316 0.32394952 0.58576318 0.64897676]
[[./figures/landscape/figure_42.png]]
:END:

*** Pair

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'sample_LD ~ pair'

  results = []
  beta = []
  pval = []

  for task in tasks:
    for day in y.day.unique():
        data = y[(y['day'] == day) & (y.tasks==task)]
        # data = y[(y['day'] == day)]
        glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

        result = glm.fit()
        results.append(result)
        beta.append(result.params)
        pval.append(result.pvalues)

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:              sample_LD   No. Observations:                  224
Model:                            GLM   Df Residuals:                      222
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.77313
Method:                          IRLS   Log-Likelihood:                -288.02
Date:                Wed, 28 Aug 2024   Deviance:                       171.64
Time:                        16:17:24   Pearson chi2:                     172.
No. Iterations:                     3   Pseudo R-squ. (CS):          0.0003932
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -0.0105      0.083     -0.127      0.899      -0.173       0.152
pair          -0.0330      0.117     -0.281      0.779      -0.263       0.197
==============================================================================
#+end_example

#+begin_src ipython
beta = np.array(beta).reshape((3, options['n_days'], -1))
print(beta.shape)

pval = np.array(pval).reshape((3, options['n_days'], -1))
print(pval.shape)
#+end_src

#+RESULTS:
: (3, 6, 2)
: (3, 6, 2)

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
k=1
for i in range(3):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta)[i, :, k], '-o', color=cols[i])
  print(np.round(pval[i, :, k], 3))
  for j in range(pval.shape[1]):
    if pval[i, j, k] < 0.05:
      plt.text(j+1, np.max(beta[..., k])+.01+.05*i, '*', ha='center', va='bottom', color=cols[i])
      # plt.text(j+1, .01, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('SOLD $\\beta_{pair}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.759 0.888 0.779 0.815 0.728 0.759]
: [0.316 0.64  0.824 0.43  0.944 0.694]
: [0.875 0.33  0.668 0.458 0.564 0.94 ]
[[./figures/landscape/figure_44.png]]
:END:

#+begin_src ipython

#+end_src

#+RESULTS:

** Sample sign OLD
*** Tasks

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  # y['choice'] = y['choice'].astype('category')

  formula = 'OLD_sign ~ tasks'

  results = []
  beta = []
  pval = []

  for day in y.day.unique():
      # data = y[(y['day'] == day) & (y.pair==0)]
      data = y[(y['day'] == day)]
      glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

      result = glm.fit()
      results.append(result)
      beta.append(result.params)
      pval.append(result.pvalues)

print(result.summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:               OLD_sign   No. Observations:                  288
Model:                            GLM   Df Residuals:                      285
Model Family:                Gaussian   Df Model:                            2
Link Function:               Identity   Scale:                         0.16177
Method:                          IRLS   Log-Likelihood:                -144.84
Date:                Wed, 28 Aug 2024   Deviance:                       46.104
Time:                        16:17:54   Pearson chi2:                     46.1
No. Iterations:                     3   Pseudo R-squ. (CS):           0.004664
Covariance Type:            nonrobust
=====================================================================================
                        coef    std err          z      P>|z|      [0.025      0.975]
-------------------------------------------------------------------------------------
Intercept             0.1771      0.041      4.314      0.000       0.097       0.258
tasks[T.DualGo]       0.0104      0.058      0.179      0.858      -0.103       0.124
tasks[T.DualNoGo]     0.0625      0.058      1.077      0.282      -0.051       0.176
=====================================================================================
#+end_example

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
for i in range(3):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta).T[i], '-o', color=cols[i])
  print(np.round(np.array(pval).T[i], 3))
  for j, p in enumerate(np.array(pval).T[i]):
    if p < 0.05:
      plt.text(j+1,  np.max(beta) + .01 + i* .05, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('sign(SOLD) $\\beta_{Task}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0. 0. 0. 0. 0. 0.]
: [0.393 0.472 0.    0.18  0.26  0.858]
: [0.625 0.023 0.738 0.003 0.055 0.282]
[[./figures/landscape/figure_47.png]]
:END:

*** Choice

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'OLD_sign ~ choice'

  results = []
  beta = []
  pval = []

  for task in tasks:
    for day in y.day.unique():
        data = y[(y['day'] == day) & (y.tasks==task)]
        # data = y[(y['day'] == day)]
        glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

        result = glm.fit()
        results.append(result)
        beta.append(result.params)
        pval.append(result.pvalues)

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:               OLD_sign   No. Observations:                  224
Model:                            GLM   Df Residuals:                      222
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.14396
Method:                          IRLS   Log-Likelihood:                -99.759
Date:                Wed, 28 Aug 2024   Deviance:                       31.960
Time:                        16:18:41   Pearson chi2:                     32.0
No. Iterations:                     3   Pseudo R-squ. (CS):           0.007766
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.1333      0.040      3.334      0.001       0.055       0.212
choice         0.0682      0.052      1.318      0.187      -0.033       0.170
==============================================================================
#+end_example

#+begin_src ipython
beta = np.array(beta).reshape((3, options['n_days'], -1))
print(beta.shape)

pval = np.array(pval).reshape((3, options['n_days'], -1))
print(pval.shape)
#+end_src

#+RESULTS:
: (3, 6, 2)
: (3, 6, 2)

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
k=1
for i in range(3):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta)[i, :, k], '-o', color=cols[i])
  print(np.round(pval[i, :, k], 3))
  for j in range(pval.shape[1]):
    if pval[i, j, k] < 0.05:
      plt.text(j+1, np.max(beta[..., k]) + .01 + i * .05, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('sign(SOLD) $\\beta_{choice}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.33  0.094 0.187 0.762 0.113 0.16 ]
: [0.374 0.763 0.913 0.235 0.112 0.475]
: [0.563 0.696 0.556 0.539 0.793 0.711]
[[./figures/landscape/figure_50.png]]
:END:

*** Behavior

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'OLD_sign ~ behavior'

  results = []
  beta = []
  pval = []

  for task in tasks:
    for day in y.day.unique():
        data = y[(y['day'] == day) & (y.tasks==task)]
        # data = y[(y['day'] == day)]
        glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

        try:
            result = glm.fit()
            results.append(result)
            beta.append(result.params)
            pval.append(result.pvalues)
        except:
            beta.append(np.zeros(2))
            pval.append(np.ones(2))
            pass

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:               OLD_sign   No. Observations:                  224
Model:                            GLM   Df Residuals:                      222
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.14329
Method:                          IRLS   Log-Likelihood:                -99.238
Date:                Wed, 28 Aug 2024   Deviance:                       31.811
Time:                        16:20:30   Pearson chi2:                     31.8
No. Iterations:                     3   Pseudo R-squ. (CS):            0.01238
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.1538      0.028      5.483      0.000       0.099       0.209
behavior       0.1081      0.065      1.668      0.095      -0.019       0.235
==============================================================================
#+end_example

#+begin_src ipython
beta = np.array(beta).reshape((3, options['n_days'], -1))
print(beta.shape)

pval = np.array(pval).reshape((3, options['n_days'], -1))
print(pval.shape)
#+end_src

#+RESULTS:
: (3, 6, 2)
: (3, 6, 2)

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
k=1
for i in range(3):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta)[i, :, k], '-o', color=cols[i])
  print(pval[i, :, k])
  for j in range(pval.shape[1]):
    if pval[i, j, k] < 0.05:
      plt.text(j+1, np.max(beta[..., k])+.01+i*.05, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('sign(SOLD) $\\beta_{behavior}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.7304999  0.37880538 0.09540339 0.45984996 0.78289986 0.00912811]
: [0.84462168 0.55503664 0.5356652  0.66135849 0.71381496 0.15582459]
: [0.1195424  0.2127877  0.03086698 0.81245162 0.49723088 0.53695203]
[[./figures/landscape/figure_53.png]]
:END:

*** Pair

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'OLD_sign ~ pair'

  results = []
  beta = []
  pval = []

  for task in tasks:
    for day in y.day.unique():
        data = y[(y['day'] == day) & (y.tasks==task)]
        # data = y[(y['day'] == day)]
        glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

        result = glm.fit()
        results.append(result)
        beta.append(result.params)
        pval.append(result.pvalues)

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:               OLD_sign   No. Observations:                  224
Model:                            GLM   Df Residuals:                      222
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.14459
Method:                          IRLS   Log-Likelihood:                -100.24
Date:                Wed, 28 Aug 2024   Deviance:                       32.098
Time:                        16:20:50   Pearson chi2:                     32.1
No. Iterations:                     3   Pseudo R-squ. (CS):           0.003480
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.1518      0.036      4.225      0.000       0.081       0.222
pair           0.0446      0.051      0.879      0.380      -0.055       0.144
==============================================================================
#+end_example

#+begin_src ipython
beta = np.array(beta).reshape((3, options['n_days'], -1))
print(beta.shape)

pval = np.array(pval).reshape((3, options['n_days'], -1))
print(pval.shape)
#+end_src

#+RESULTS:
: (3, 6, 2)
: (3, 6, 2)

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
k=1
for i in range(3):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta)[i, :, k], '-o', color=cols[i])
  print(np.round(pval[i, :, k], 3))
  for j in range(pval.shape[1]):
    if pval[i, j, k] < 0.05:
      plt.text(j+1, np.max(beta[..., k])+.01 + i*.05, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('sign(SOLD) $\\beta_{pair}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.305 0.855 0.38  0.434 0.171 0.791]
: [0.855 0.606 0.275 0.227 0.206 0.604]
: [0.322 0.35  0.305 0.35  0.45  0.813]
[[./figures/landscape/figure_56.png]]
:END:

#+begin_src ipython

#+end_src

#+RESULTS:

** Distractor OED
*** Tasks

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  # y['choice'] = y['choice'].astype('category')

  formula = 'dist_ED ~ tasks'

  results = []
  beta = []
  pval = []

  for day in y.day.unique():
      # data = y[(y['day'] == day) & (y.pair==0)]
      data = y[(y['day'] == day) & (y.mouse=='JawsM15')]
      glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

      result = glm.fit()
      results.append(result)
      beta.append(result.params)
      pval.append(result.pvalues)

print(result.summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:                dist_ED   No. Observations:                   96
Model:                            GLM   Df Residuals:                       93
Model Family:                Gaussian   Df Model:                            2
Link Function:               Identity   Scale:                         0.29278
Method:                          IRLS   Log-Likelihood:                -75.734
Date:                Wed, 28 Aug 2024   Deviance:                       27.229
Time:                        18:41:30   Pearson chi2:                     27.2
No. Iterations:                     3   Pseudo R-squ. (CS):            0.09962
Covariance Type:            nonrobust
=====================================================================================
                        coef    std err          z      P>|z|      [0.025      0.975]
-------------------------------------------------------------------------------------
Intercept            -0.8730      0.096     -9.127      0.000      -1.061      -0.686
tasks[T.DualGo]       0.2024      0.135      1.496      0.135      -0.063       0.467
tasks[T.DualNoGo]    -0.2258      0.135     -1.669      0.095      -0.491       0.039
=====================================================================================
#+end_example

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
for i in range(3):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta).T[i], '-o', color=cols[i])
  print(np.round(np.array(pval).T[i], 3))
  for j, p in enumerate(np.array(pval).T[i]):
    if p < 0.05:
      plt.text(j+1, np.max(beta)+.01 + i*.05, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('DOED $\\beta_{Task}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.    0.029 0.    0.    0.743 0.   ]
: [0.    0.014 0.    0.001 0.    0.135]
: [0.002 0.001 0.    0.004 0.    0.095]
[[./figures/landscape/figure_60.png]]
:END:

*** Choice

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'dist_ED ~ choice'

  results = []
  beta = []
  pval = []

  for task in tasks:
    for day in y.day.unique():
        data = y[(y['day'] == day) & (y.tasks==task) ]
        glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())
        try:
            result = glm.fit()
            results.append(result)
            beta.append(result.params)
            pval.append(result.pvalues)
        except:
            beta.append(np.zeros(2))
            pval.append(np.ones(2))
            pass

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:                dist_ED   No. Observations:                  224
Model:                            GLM   Df Residuals:                      222
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.16259
Method:                          IRLS   Log-Likelihood:                -113.38
Date:                Wed, 28 Aug 2024   Deviance:                       36.094
Time:                        16:21:56   Pearson chi2:                     36.1
No. Iterations:                     3   Pseudo R-squ. (CS):           0.005115
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.0463      0.043      1.089      0.276      -0.037       0.130
choice        -0.0587      0.055     -1.068      0.286      -0.166       0.049
==============================================================================
#+end_example


#+begin_src ipython
beta = np.array(beta).reshape((3, options['n_days'], -1))
print(beta.shape)

pval = np.array(pval).reshape((3, options['n_days'], -1))
print(pval.shape)
#+end_src

#+RESULTS:
: (3, 6, 2)
: (3, 6, 2)

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
k=1
for i in range(3):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta)[i, :, k], '-o', color=cols[i])
  print(np.round(pval[i, :, k], 3))
  for j in range(pval.shape[1]):
    if pval[i, j, k] < 0.05:
      plt.text(j+1, np.max(beta[..., k])+.01 + i*.05, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('DOED $\\beta_{choice}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.067 0.67  0.286 0.128 0.996 0.516]
: [0.001 0.013 0.485 0.539 0.251 0.986]
: [0.257 0.484 0.768 0.783 0.276 0.95 ]
[[./figures/landscape/figure_62.png]]
:END:

*** Behavior

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'dist_ED ~ behavior'
  # formula = 'OED_sign ~ behavior'

  results = []
  beta = []
  pval = []

  for task in tasks:
    for day in y.day.unique():
        data = y[(y['day'] == day) & (y.tasks==task)]
        glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())
        try:
            result = glm.fit()
            results.append(result)
            beta.append(result.params)
            pval.append(result.pvalues)
        except:
            beta.append(np.zeros(2))
            pval.append(np.ones(2))
            pass

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:                dist_ED   No. Observations:                  224
Model:                            GLM   Df Residuals:                      222
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.16336
Method:                          IRLS   Log-Likelihood:                -113.92
Date:                Wed, 28 Aug 2024   Deviance:                       36.266
Time:                        16:23:10   Pearson chi2:                     36.3
No. Iterations:                     3   Pseudo R-squ. (CS):          0.0004085
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.0149      0.030      0.498      0.619      -0.044       0.074
behavior      -0.0199      0.069     -0.287      0.774      -0.155       0.116
==============================================================================
#+end_example

#+begin_src ipython
beta = np.array(beta).reshape((3, options['n_days'], -1))
print(beta.shape)

pval = np.array(pval).reshape((3, options['n_days'], -1))
print(pval.shape)
#+end_src

#+RESULTS:
: (3, 6, 2)
: (3, 6, 2)

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
k=1
for i in range(3):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta)[i, :, k], '-o', color=cols[i])
  print(np.round(pval[i, :, k], 3))
  for j in range(pval.shape[1]):
    if pval[i, j, k] < 0.05:
      plt.text(j+1, np.max(beta[..., k])+.01 + i*.05, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('DOED $\\beta_{behavior}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.26  0.066 0.774 0.548 0.065 0.066]
: [0.781 0.    0.004 0.108 0.295 0.003]
: [0.919 0.407 0.185 0.043 0.951 0.409]
[[./figures/landscape/figure_65.png]]
:END:

#+begin_src ipython

#+end_src

#+RESULTS:

*** Pair

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'dist_ED ~ pair'
  # formula = 'OED_sign ~ behavior'

  results = []
  beta = []
  pval = []

  for task in tasks:
    for day in y.day.unique():
        data = y[(y['day'] == day) & (y.tasks==task)]
        glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())
        try:
            result = glm.fit()
            results.append(result)
            beta.append(result.params)
            pval.append(result.pvalues)
        except:
            beta.append(np.zeros(2))
            pval.append(np.ones(2))
            pass

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:                dist_ED   No. Observations:                  224
Model:                            GLM   Df Residuals:                      222
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.16332
Method:                          IRLS   Log-Likelihood:                -113.89
Date:                Wed, 28 Aug 2024   Deviance:                       36.256
Time:                        16:23:29   Pearson chi2:                     36.3
No. Iterations:                     3   Pseudo R-squ. (CS):          0.0006663
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.0213      0.038      0.558      0.577      -0.054       0.096
pair          -0.0202      0.054     -0.375      0.708      -0.126       0.086
==============================================================================
#+end_example

#+begin_src ipython
beta = np.array(beta).reshape((3, options['n_days'], -1))
print(beta.shape)

pval = np.array(pval).reshape((3, options['n_days'], -1))
print(pval.shape)
#+end_src

#+RESULTS:
: (3, 6, 2)
: (3, 6, 2)

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
k=1
for i in range(3):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta)[i, :, k], '-o', color=cols[i])
  print(np.round(pval[i, :, k], 3))
  for j in range(pval.shape[1]):
    if pval[i, j, k] < 0.05:
      plt.text(j+1, np.max(beta[..., k])+.01 + i*.05, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('DOED $\\beta_{pair}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.906 0.723 0.708 0.283 0.775 0.875]
: [0.348 0.565 0.568 0.566 0.823 0.881]
: [0.857 0.31  0.789 0.113 0.601 0.353]
[[./figures/landscape/figure_69.png]]
:END:

#+begin_src ipython

#+end_src

#+RESULTS:

** Distractor sign OED
*** Tasks

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  # y['choice'] = y['choice'].astype('category')

  formula = 'OED_sign ~ tasks'

  results = []
  beta = []
  pval = []

  for day in y.day.unique():
      # data = y[(y['day'] == day) & (y.pair==0)]
      data = y[(y['day'] == day)]
      glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

      result = glm.fit()
      results.append(result)
      beta.append(result.params)
      pval.append(result.pvalues)

print(result.summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:               OED_sign   No. Observations:                  288
Model:                            GLM   Df Residuals:                      285
Model Family:                Gaussian   Df Model:                            2
Link Function:               Identity   Scale:                         0.24784
Method:                          IRLS   Log-Likelihood:                -206.27
Date:                Wed, 28 Aug 2024   Deviance:                       70.635
Time:                        16:23:43   Pearson chi2:                     70.6
No. Iterations:                     3   Pseudo R-squ. (CS):           0.008193
Covariance Type:            nonrobust
=====================================================================================
                        coef    std err          z      P>|z|      [0.025      0.975]
-------------------------------------------------------------------------------------
Intercept             0.4062      0.051      7.995      0.000       0.307       0.506
tasks[T.DualGo]       0.1042      0.072      1.450      0.147      -0.037       0.245
tasks[T.DualNoGo]     0.0208      0.072      0.290      0.772      -0.120       0.162
=====================================================================================
#+end_example

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
for i in range(3):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta).T[i], '-o', color=cols[i])
  print(np.round(np.array(pval).T[i], 3))
  for j, p in enumerate(np.array(pval).T[i]):
    if p < 0.05:
      plt.text(j+1, np.max(beta)+.01 + i*.05, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('sign(DOED) $\\beta_{Task}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0. 0. 0. 0. 0. 0.]
: [0.02  0.003 0.08  0.637 0.    0.147]
: [0.042 0.001 0.001 0.22  0.021 0.772]
[[./figures/landscape/figure_72.png]]
:END:

*** Choice

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'OED_sign ~ choice'

  results = []
  beta = []
  pval = []

  for task in tasks:
    for day in y.day.unique():
        data = y[(y['day'] == day) & (y.tasks==task) ]
        glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())
        try:
            result = glm.fit()
            results.append(result)
            beta.append(result.params)
            pval.append(result.pvalues)
        except:
            beta.append(np.zeros(2))
            pval.append(np.ones(2))
            pass

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:               OED_sign   No. Observations:                  224
Model:                            GLM   Df Residuals:                      222
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.24043
Method:                          IRLS   Log-Likelihood:                -157.20
Date:                Wed, 28 Aug 2024   Deviance:                       53.376
Time:                        16:24:46   Pearson chi2:                     53.4
No. Iterations:                     3   Pseudo R-squ. (CS):           0.004901
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.6444      0.052     12.468      0.000       0.543       0.746
choice        -0.0698      0.067     -1.045      0.296      -0.201       0.061
==============================================================================
#+end_example


#+begin_src ipython
beta = np.array(beta).reshape((3, options['n_days'], -1))
print(beta.shape)

pval = np.array(pval).reshape((3, options['n_days'], -1))
print(pval.shape)
#+end_src

#+RESULTS:
: (3, 6, 2)
: (3, 6, 2)

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
k=1
for i in range(3):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta)[i, :, k], '-o', color=cols[i])
  print(np.round(pval[i, :, k], 3))
  for j in range(pval.shape[1]):
    if pval[i, j, k] < 0.05:
      plt.text(j+1, np.max(beta[..., k])+.01 + i*.05, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('sign(DOED) $\\beta_{choice}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.432 0.065 0.296 0.026 0.726 0.305]
: [0.001 0.986 0.597 0.157 0.627 0.538]
: [0.603 0.286 0.392 0.678 0.384 0.465]
[[./figures/landscape/figure_75.png]]
:END:

*** Behavior

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'OED_sign ~ behavior'

  results = []
  beta = []
  pval = []

  for task in tasks:
    for day in y.day.unique():
        data = y[(y['day'] == day) & (y.tasks==task)]
        glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())
        try:
            result = glm.fit()
            results.append(result)
            beta.append(result.params)
            pval.append(result.pvalues)
        except:
            beta.append(np.zeros(2))
            pval.append(np.ones(2))
            pass

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:               OED_sign   No. Observations:                  224
Model:                            GLM   Df Residuals:                      222
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.24155
Method:                          IRLS   Log-Likelihood:                -157.72
Date:                Wed, 28 Aug 2024   Deviance:                       53.625
Time:                        16:24:54   Pearson chi2:                     53.6
No. Iterations:                     3   Pseudo R-squ. (CS):          0.0002960
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.5989      0.036     16.439      0.000       0.527       0.670
behavior       0.0201      0.084      0.239      0.811      -0.145       0.185
==============================================================================
#+end_example

#+begin_src ipython
beta = np.array(beta).reshape((3, options['n_days'], -1))
print(beta.shape)

pval = np.array(pval).reshape((3, options['n_days'], -1))
print(pval.shape)
#+end_src

#+RESULTS:
: (3, 6, 2)
: (3, 6, 2)

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
k=1
for i in range(3):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta)[i, :, k], '-o', color=cols[i])
  print(np.round(pval[i, :, k], 3))
  for j in range(pval.shape[1]):
    if pval[i, j, k] < 0.05:
      plt.text(j+1, np.max(beta[..., k])+.01 + i*.05, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('sign(DOED) $\\beta_{behavior}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.025 0.002 0.811 0.225 0.149 0.064]
: [0.601 0.008 0.004 0.007 0.567 0.123]
: [0.849 0.64  0.836 0.043 0.293 0.426]
[[./figures/landscape/figure_78.png]]
:END:

#+begin_src ipython

#+end_src

#+RESULTS:

*** Pair

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'OED_sign ~ behavior'

  results = []
  beta = []
  pval = []

  for task in tasks:
    for day in y.day.unique():
        data = y[(y['day'] == day) & (y.tasks==task)]
        glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())
        try:
            result = glm.fit()
            results.append(result)
            beta.append(result.params)
            pval.append(result.pvalues)
        except:
            beta.append(np.zeros(2))
            pval.append(np.ones(2))
            pass

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:               OED_sign   No. Observations:                  224
Model:                            GLM   Df Residuals:                      222
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.24155
Method:                          IRLS   Log-Likelihood:                -157.72
Date:                Wed, 28 Aug 2024   Deviance:                       53.625
Time:                        16:25:27   Pearson chi2:                     53.6
No. Iterations:                     3   Pseudo R-squ. (CS):          0.0002960
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.5989      0.036     16.439      0.000       0.527       0.670
behavior       0.0201      0.084      0.239      0.811      -0.145       0.185
==============================================================================
#+end_example

#+begin_src ipython
beta = np.array(beta).reshape((3, options['n_days'], -1))
print(beta.shape)

pval = np.array(pval).reshape((3, options['n_days'], -1))
print(pval.shape)
#+end_src

#+RESULTS:
: (3, 6, 2)
: (3, 6, 2)

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
k=1
for i in range(3):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta)[i, :, k], '-o', color=cols[i])
  print(np.round(pval[i, :, k], 3))
  for j in range(pval.shape[1]):
    if pval[i, j, k] < 0.05:
      plt.text(j+1, np.max(beta[..., k])+.01 + i*.05, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('sign(DOED) $\\beta_{pair}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.025 0.002 0.811 0.225 0.149 0.064]
: [0.601 0.008 0.004 0.007 0.567 0.123]
: [0.849 0.64  0.836 0.043 0.293 0.426]
[[./figures/landscape/figure_82.png]]
:END:

#+begin_src ipython

#+end_src

#+RESULTS:

** All tasks
*** sample OLD
**** Choice

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'sample_LD ~  choice'

  results = []
  beta = []
  pval = []

  for day in y.day.unique():
      data = y[(y['day'] == day)]
      data['sample_LD'] = -(2*data.sample_odor-1) * data['sample_LD']
      glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

      result = glm.fit()
      results.append(result)
      beta.append(result.params)
      pval.append(result.pvalues)

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:              sample_LD   No. Observations:                  672
Model:                            GLM   Df Residuals:                      670
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.50343
Method:                          IRLS   Log-Likelihood:                -721.92
Date:                Wed, 28 Aug 2024   Deviance:                       337.30
Time:                        17:49:28   Pearson chi2:                     337.
No. Iterations:                     3   Pseudo R-squ. (CS):           0.005184
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.5413      0.046     11.843      0.000       0.452       0.631
choice        -0.1066      0.057     -1.868      0.062      -0.218       0.005
==============================================================================
#+end_example

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
for i in range(1, 2):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta).T[i], '-o', color=cols[i])
  print(np.round(np.array(pval).T[i]))
  # for j, p in enumerate(np.array(pval).T[i]):
  #   if p < 0.05:
  #     plt.text(j+1, max(np.array(beta).T[i]) + .01 + i * 0.05, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('SOLD $\\beta_{choice}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0. 0. 0. 0. 0. 0.]
[[./figures/landscape/figure_86.png]]
:END:

#+begin_src ipython

#+end_src

#+RESULTS:

**** Behavior

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'sample_LD ~  behavior'

  results = []
  beta = []
  pval = []

  for day in y.day.unique():
      data = y[(y['day'] == day)]
      data['sample_LD'] = -(2*data.sample_odor-1) * data['sample_LD']
      # data = y[(y['day'] == day)]
      glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

      result = glm.fit()
      results.append(result)
      beta.append(result.params)
      pval.append(result.pvalues)

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:              sample_LD   No. Observations:                  672
Model:                            GLM   Df Residuals:                      670
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.50112
Method:                          IRLS   Log-Likelihood:                -720.38
Date:                Wed, 28 Aug 2024   Deviance:                       335.75
Time:                        17:46:18   Pearson chi2:                     336.
No. Iterations:                     3   Pseudo R-squ. (CS):           0.009761
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.7626      0.116      6.568      0.000       0.535       0.990
behavior       0.0840      0.033      2.567      0.010       0.020       0.148
==============================================================================
#+end_example

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
for i in range(1, 2):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta).T[i], '-o', color=cols[i])
  print(np.round(np.array(pval).T[i], 3))
  # for j, p in enumerate(np.array(pval).T[i]):
  #   if p < 0.05:
  #     plt.text(j+1, .11 + i * 0.05, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('SOLD $\\beta_{behavior}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.466 0.096 0.01  0.008 0.036 0.171]
[[./figures/landscape/figure_89.png]]
:END:

*** sample sign OLD
**** Choice

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'OLD_sign ~  choice'

  results = []
  beta = []
  pval = []

  for day in y.day.unique():
      data = y[(y['day'] == day)]
      # data = y[(y['day'] == day)]
      glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

      result = glm.fit()
      results.append(result)
      beta.append(result.params)
      pval.append(result.pvalues)

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:               OLD_sign   No. Observations:                  672
Model:                            GLM   Df Residuals:                      670
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.18791
Method:                          IRLS   Log-Likelihood:                -390.81
Date:                Wed, 28 Aug 2024   Deviance:                       125.90
Time:                        16:28:30   Pearson chi2:                     126.
No. Iterations:                     3   Pseudo R-squ. (CS):           0.004723
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.2116      0.028      7.579      0.000       0.157       0.266
choice         0.0622      0.035      1.783      0.075      -0.006       0.131
==============================================================================
#+end_example

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
for i in range(1, 2):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta).T[i], '-o', color=cols[i])
  print(np.round(np.array(pval).T[i],3))
  for j, p in enumerate(np.array(pval).T[i]):
    if p < 0.05:
      plt.text(j+1, max(np.array(beta).T[i]) + .01 + i * 0.001, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('sign(SOLD) $\\beta_{choice}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.664 0.419 0.075 0.406 0.046 0.56 ]
[[./figures/landscape/figure_90.png]]
:END:

**** Behavior

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'OLD_sign ~  behavior'

  results = []
  beta = []
  pval = []

  for day in y.day.unique():
      data = y[(y['day'] == day)]
      # data = y[(y['day'] == day)]
      glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

      result = glm.fit()
      results.append(result)
      beta.append(result.params)
      pval.append(result.pvalues)

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:               OLD_sign   No. Observations:                  672
Model:                            GLM   Df Residuals:                      670
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.18630
Method:                          IRLS   Log-Likelihood:                -387.90
Date:                Wed, 28 Aug 2024   Deviance:                       124.82
Time:                        16:29:21   Pearson chi2:                     125.
No. Iterations:                     3   Pseudo R-squ. (CS):            0.01334
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.2246      0.019     11.876      0.000       0.188       0.262
behavior       0.1198      0.040      3.003      0.003       0.042       0.198
==============================================================================
#+end_example

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
for i in range(1, 2):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta).T[i], '-o', color=cols[i])
  print(np.round(np.array(pval).T[i], 3))
  for j, p in enumerate(np.array(pval).T[i]):
    if p < 0.05:
      plt.text(j+1, max(np.array(beta).T[i]) + .01, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('sign(SOLD) $\\beta_{behavior}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.4   0.428 0.003 0.788 0.382 0.987]
[[./figures/landscape/figure_92.png]]
:END:

*** Dist OED
**** Choice

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'dist_ED ~  choice'

  results = []
  beta = []
  pval = []

  for day in y.day.unique():
      # data = y[(y['day'] == day) & (y['pair']==0)]
      data = y[(y['day'] == day)]
      glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

      result = glm.fit()
      results.append(result)
      beta.append(result.params)
      pval.append(result.pvalues)

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:                dist_ED   No. Observations:                  672
Model:                            GLM   Df Residuals:                      670
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.17625
Method:                          IRLS   Log-Likelihood:                -369.27
Date:                Wed, 28 Aug 2024   Deviance:                       118.08
Time:                        16:29:37   Pearson chi2:                     118.
No. Iterations:                     3   Pseudo R-squ. (CS):          0.0004536
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.0022      0.027      0.083      0.934      -0.051       0.055
choice        -0.0186      0.034     -0.549      0.583      -0.085       0.048
==============================================================================
#+end_example

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
for i in range(1,2):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta).T[i], '-o', color=cols[i])
  print(np.round(np.array(pval).T[i], 3))
  for j, p in enumerate(np.array(pval).T[i]):
    if p < 0.05:
      plt.text(j+1, max(np.array(beta).T[i]) + .01, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('DOED $\\beta_{choice}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.    0.178 0.583 0.244 0.951 0.738]
[[./figures/landscape/figure_94.png]]
:END:

**** Behavior

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'dist_ED ~  behavior'

  results = []
  beta = []
  pval = []

  for day in y.day.unique():
      # data = y[(y['day'] == day) & (y['pair']==0)]
      data = y[(y['day'] == day)]
      glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

      result = glm.fit()
      results.append(result)
      beta.append(result.params)
      pval.append(result.pvalues)

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:                dist_ED   No. Observations:                  672
Model:                            GLM   Df Residuals:                      670
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.17423
Method:                          IRLS   Log-Likelihood:                -365.40
Date:                Wed, 28 Aug 2024   Deviance:                       116.73
Time:                        16:29:39   Pearson chi2:                     117.
No. Iterations:                     3   Pseudo R-squ. (CS):            0.01195
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept     -0.0343      0.018     -1.875      0.061      -0.070       0.002
behavior       0.1096      0.039      2.841      0.004       0.034       0.185
==============================================================================
#+end_example

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
for i in range(1, 2):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta).T[i], '-o', color=cols[i])
  print(np.round(np.array(pval).T[i], 3))
  for j, p in enumerate(np.array(pval).T[i]):
    if p < 0.05:
      plt.text(j+1, max(np.array(beta).T[i]) + .01, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('$\\beta_{behavior}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.715 0.    0.004 0.016 0.136 0.889]
[[./figures/landscape/figure_96.png]]
:END:

*** Dist sign OED
**** Choice

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'OED_sign ~  choice'

  results = []
  beta = []
  pval = []

  for day in y.day.unique():
      # data = y[(y['day'] == day) & (y['pair']==0)]
      data = y[(y['day'] == day)]
      glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

      result = glm.fit()
      results.append(result)
      beta.append(result.params)
      pval.append(result.pvalues)

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:               OED_sign   No. Observations:                  672
Model:                            GLM   Df Residuals:                      670
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.24450
Method:                          IRLS   Log-Likelihood:                -479.26
Date:                Wed, 28 Aug 2024   Deviance:                       163.82
Time:                        16:29:40   Pearson chi2:                     164.
No. Iterations:                     3   Pseudo R-squ. (CS):          1.458e-05
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.5768      0.032     18.108      0.000       0.514       0.639
choice         0.0033      0.040      0.083      0.934      -0.075       0.081
==============================================================================
#+end_example

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
for i in range(1,2):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta).T[i], '-o', color=cols[i])
  print(np.round(np.array(pval).T[i], 3))
  for j, p in enumerate(np.array(pval).T[i]):
    if p < 0.05:
      plt.text(j+1, max(np.array(beta).T[i]) + .01, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('sign(DOED) $\\beta_{choice}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.012 0.482 0.934 0.061 0.607 0.817]
[[./figures/landscape/figure_98.png]]
:END:

**** Behavior

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')
  y['choice'] = y['choice'].astype('int')

  formula = 'OED_sign ~  behavior'

  results = []
  beta = []
  pval = []

  for day in y.day.unique():
      # data = y[(y['day'] == day) & (y['pair']==0)]
      data = y[(y['day'] == day)]
      glm = smf.glm(formula=formula, data=data, family=sm.families.Gaussian())

      result = glm.fit()
      results.append(result)
      beta.append(result.params)
      pval.append(result.pvalues)

print(results[2].summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:               OED_sign   No. Observations:                  672
Model:                            GLM   Df Residuals:                      670
Model Family:                Gaussian   Df Model:                            1
Link Function:               Identity   Scale:                         0.24249
Method:                          IRLS   Log-Likelihood:                -476.48
Date:                Wed, 28 Aug 2024   Deviance:                       162.47
Time:                        16:29:42   Pearson chi2:                     162.
No. Iterations:                     3   Pseudo R-squ. (CS):           0.008280
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      0.5547      0.022     25.712      0.000       0.512       0.597
behavior       0.1075      0.046      2.363      0.018       0.018       0.197
==============================================================================
#+end_example

#+begin_src ipython
cols = ['r', 'b', 'g', 'r', 'b', 'g']
for i in range(1, 2):
  plt.plot(np.arange(1, options['n_days']+1), np.array(beta).T[i], '-o', color=cols[i])
  print(np.round(np.array(pval).T[i], 3))
  for j, p in enumerate(np.array(pval).T[i]):
    if p < 0.05:
      plt.text(j+1, max(np.array(beta).T[i]) + .01, '*', ha='center', va='bottom', color=cols[i])

plt.xlabel('Day')
plt.ylabel('sign(DOED) $\\beta_{behavior}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.092 0.    0.018 0.001 0.576 0.628]
[[./figures/landscape/figure_100.png]]
:END:

#+begin_src ipython

#+end_src

#+RESULTS:

** overlaps day

*** Sample LD

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf
  import os
  os.environ['R_LIBS_USER'] = '~/R/x86_64-pc-linux-gnu-library/4.3/'

  from rpy2.robjects.packages import importr
  lmer = importr("~/R/x86_64-pc-linux-gnu-library/4.3/lme4/R/lme4")
  # from pymer4.models import Lmer

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('int')

  print(y.behavior.unique())

  formula = 'behavior ~ day * tasks * sample_LD + (1|mouse)'

  results = []
  data = y.copy()
  data['sample_LD'] = -(2*data.sample_odor-1) * data['sample_LD']

  glm = lmer(formula=formula, data=data, family='binomial')
  # glm = smf.glm(formula=formula, data=data, family=sm.families.Binomial())
  # glm = smf.mixedlm(formula, data, groups=data['mouse'], re_formula='1')
  result = glm.fit()
  pval = result.pvalues

  print(result.summary())
    #+end_src


#+begin_src ipython
import rpy2.robjects as robjects
from rpy2.robjects.packages import importr

# Set the .libPaths in R
custom_r_libpath = '~/R/x86_64-pc-linux-gnu-library/4.3/'
robjects.r('.libPaths("{0}")'.format(custom_r_libpath))

from pymer4.models import Lmer
#+end_src

#+RESULTS:

#+begin_src ipython
  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('int')

  print(y.behavior.unique())

  formula = 'behavior ~ day * tasks * sample_LD + (1|mouse)'

  results = []
  data = y.copy()
  data['sample_LD'] = -(2*data.sample_odor-1) * data['sample_LD']

  formula = 'behavior ~ day * tasks * sample_LD + (1|mouse)'
  glm = Lmer(formula=formula, data=data, family='binomial')
  # glm = smf.glm(formula=formula, data=data, family=sm.families.Binomial())
  # glm = smf.mixedlm(formula, data, groups=data['mouse'], re_formula='1')
  result = glm.fit()
  pval = result.pvalues

  print(result.summary())

#+end_src

#+RESULTS:

#+begin_src ipython
  print(glm.summary())
#+end_src


#+begin_src ipython
random_effects = glm.ranef
print(random_effects)
#+end_src

#+RESULTS:
:          X.Intercept.
: ACCM03       0.202566
: ACCM04       0.726651
: ChRM04      -0.436518
: JawsM15      0.273253
: JawsM18     -0.751546

#+begin_src ipython
print(result['P-val'])
#+end_src

#+RESULTS:
#+begin_example
(Intercept)                    0.311
day                            0.000
tasksDualGo                    0.243
tasksDualNoGo                  0.952
sample_LD                      0.027
day:tasksDualGo                0.594
day:tasksDualNoGo              0.821
day:sample_LD                  0.001
tasksDualGo:sample_LD          0.202
tasksDualNoGo:sample_LD        0.525
day:tasksDualGo:sample_LD      0.023
day:tasksDualNoGo:sample_LD    0.145
Name: P-val, dtype: float64
#+end_example

#+begin_src ipython
plt.figure(figsize=(15, 5))
colors = ['blue', 'green', 'red', 'purple', 'orange']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1

keys = ['(Intercept)', 'day', 'tasksDualGo', 'tasksDualNoGo', 'sample_LD']
# keys = result.Estimate.keys()
for i, key in enumerate(keys):
     df = result.Estimate[key] + random_effects

     mean_value = df['X.Intercept.'].mean()
     std_dev = df['X.Intercept.'].std()

     if result['P-val'][key]<0.001:
          plt.text(i,   1.01, '***', ha='center', va='bottom')

     elif result['P-val'][key]<0.01:
          plt.text(i,   1.01, '**', ha='center', va='bottom')

     elif result['P-val'][key]<0.05:
          plt.text(i,   1.01, '*', ha='center', va='bottom')

     # Plot individual points
     plt.scatter(i * np.ones(df.shape[0]) + space, df['X.Intercept.'], color=colors)
     # Plot mean and stddev as error bars
     plt.plot(i, mean_value, '_k', ms=20)
     plt.errorbar(i * np.ones(df.shape[0]), [mean_value]*len(df), yerr=[std_dev]*len(df), fmt='-', linestyle='None', color='k', capsize=15)

plt.axhline(y=0, color='black', linestyle='--')
plt.xticks(np.arange(len(keys)), keys)
plt.ylabel('$\\beta$')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_109.png]]
#+RESULTS:

#+begin_src ipython
plt.figure(figsize=(15, 5))
colors = ['blue', 'green', 'red', 'purple', 'orange']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1

keys = ['day:tasksDualGo', 'day:tasksDualNoGo', 'day:sample_LD']
# keys = result.Estimate.keys()
for i, key in enumerate(keys):
     df = result.Estimate[key] + random_effects

     mean_value = df['X.Intercept.'].mean()
     std_dev = df['X.Intercept.'].std()

     # Plot individual points
     plt.scatter(i * np.ones(df.shape[0]) + space, df['X.Intercept.'], color=colors)
     # Plot mean and stddev as error bars
     plt.plot(i, mean_value, '_k', ms=20)
     plt.errorbar(i * np.ones(df.shape[0]), [mean_value]*len(df), yerr=[std_dev]*len(df), fmt='-', linestyle='None', color='k', capsize=15)

plt.axhline(y=0, color='black', linestyle='--')
plt.xticks(np.arange(len(keys)), keys)
plt.ylabel('$\\beta$')
plt.show()
#+end_src

#+RESULTS:
[[./figures/landscape/figure_110.png]]



#+begin_src ipython
df = intercepts
# Step 3: Calculate mean and standard deviation
mean_value = df['X.Intercept.'].mean()
std_dev = df['X.Intercept.'].std()

# Step 4: Plotting the data
plt.figure(figsize=(10, 5))

colors = ['blue', 'green', 'red', 'purple', 'orange']
space = np.array([-0.1,-0.05, 0.0, 0.05, 0.1]) * .1
# Plot individual points
plt.scatter(np.ones(df.shape[0]) + space, df['X.Intercept.'], label='Intercepts', color=colors)
# Plot mean and stddev as error bars
plt.plot(1, mean_value, 'ok')
plt.errorbar(np.ones(df.shape[0]), [mean_value]*len(df), yerr=[std_dev]*len(df), fmt='-', linestyle='None', color='k', capsize=15, label='Mean ± Std. Dev')

# Adding labels and title
plt.xlim([.85, 1.15])
plt.xticks([1], ['Intercept'])
plt.ylabel('$\\beta$')
plt.legend(fontsize=10)

#+end_src

#+RESULTS:
:RESULTS:
: <matplotlib.legend.Legend at 0x7fe4f3b79010>
[[./figures/landscape/figure_110.png]]
:END:

#+begin_src ipython
pval_DPA = [result.pvalues[j] for i, j in enumerate(result.params.keys()) if i<options['n_days']]
pval_Go = [result.pvalues[i] for i in result.params.keys() if 'DualGo' in i]
pval_NoGo = [result.pvalues[i] for i in result.params.keys() if 'DualNoGo' in i]
#+end_src

#+RESULTS:

#+begin_src ipython
coefs_DPA = [result.params[j] for i, j in enumerate(result.params.keys()) if i<options['n_days']]
coefs_Go = [result.params[i] for i in result.params.keys() if 'DualGo' in i]
coefs_NoGo = [result.params[i] for i in result.params.keys() if 'DualNoGo' in i]
#+end_src

#+RESULTS:

#+begin_src ipython
plt.plot(np.arange(1, options['n_days']+1), coefs_DPA, '-o', color='r')
plt.plot(np.arange(1, options['n_days']+1), coefs_Go, '-o', color='b')
plt.plot(np.arange(1, options['n_days']+1), coefs_NoGo, '-o', color='g')

print(np.round(pval_DPA, 3))
print(np.round(pval_Go, 3))
print(np.round(pval_NoGo, 3))

for i in range(len(coefs_DPA)):
    if pval_DPA[i] < 0.05:
        plt.text(i+1, .01, '*', ha='center', va='bottom', color='r')
    if pval_Go[i] < 0.05:
        plt.text(i+1, .05, '*', ha='center', va='bottom', color='b')
    if pval_NoGo[i] < 0.05:
        plt.text(i+1, .1, '*', ha='center', va='bottom', color='g')

plt.xlabel('Day')
plt.ylabel('SOLD $\\beta_{day}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.041 0.198 0.288 0.    0.014 0.002]
: [0.024 0.774 0.218 0.003 0.245 0.007]
: [0.099 0.701 0.794 0.567 0.237 0.535]
[[./figures/landscape/figure_105.png]]
:END:


#+RESULTS:
#+begin_src ipython

#+end_src

#+RESULTS:

*** sign Sample LD

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')

  formula = 'OLD_sign ~ day * tasks'

  results = []
  glm = smf.glm(formula=formula, data=y, family=sm.families.Gaussian())
  # glm = smf.mixedlm(formula, y, groups=y['day'], re_formula='1')
  result = glm.fit()
  pval = result.pvalues

  print(result.summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:               OLD_sign   No. Observations:                 3648
Model:                            GLM   Df Residuals:                     3630
Model Family:                Gaussian   Df Model:                           17
Link Function:               Identity   Scale:                         0.16200
Method:                          IRLS   Log-Likelihood:                -1847.3
Date:                Wed, 28 Aug 2024   Deviance:                       588.06
Time:                        16:34:20   Pearson chi2:                     588.
No. Iterations:                     3   Pseudo R-squ. (CS):            0.02084
Covariance Type:            nonrobust
================================================================================================
                                   coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------------------------
Intercept                        0.1875      0.027      6.972      0.000       0.135       0.240
day[T.2.0]                      -0.0312      0.038     -0.822      0.411      -0.106       0.043
day[T.3.0]                      -0.0134      0.038     -0.352      0.725      -0.088       0.061
day[T.4.0]                      -0.0536      0.038     -1.409      0.159      -0.128       0.021
day[T.5.0]                     1.01e-15      0.038   2.65e-14      1.000      -0.075       0.075
day[T.6.0]                      -0.0104      0.049     -0.212      0.832      -0.107       0.086
tasks[T.DualGo]                 -0.0313      0.038     -0.822      0.411      -0.106       0.043
tasks[T.DualNoGo]                0.0179      0.038      0.470      0.639      -0.057       0.092
day[T.2.0]:tasks[T.DualGo]       0.0580      0.054      1.079      0.281      -0.047       0.163
day[T.3.0]:tasks[T.DualGo]       0.2500      0.054      4.648      0.000       0.145       0.355
day[T.4.0]:tasks[T.DualGo]       0.0804      0.054      1.494      0.135      -0.025       0.186
day[T.5.0]:tasks[T.DualGo]       0.0759      0.054      1.411      0.158      -0.030       0.181
day[T.6.0]:tasks[T.DualGo]       0.0417      0.069      0.600      0.548      -0.094       0.178
day[T.2.0]:tasks[T.DualNoGo]     0.0670      0.054      1.245      0.213      -0.038       0.172
day[T.3.0]:tasks[T.DualNoGo]    -0.0045      0.054     -0.083      0.934      -0.110       0.101
day[T.4.0]:tasks[T.DualNoGo]     0.0893      0.054      1.660      0.097      -0.016       0.195
day[T.5.0]:tasks[T.DualNoGo]     0.0580      0.054      1.079      0.281      -0.047       0.163
day[T.6.0]:tasks[T.DualNoGo]     0.0446      0.069      0.643      0.520      -0.091       0.181
================================================================================================
#+end_example

#+begin_src ipython
pval_DPA = [result.pvalues[j] for i, j in enumerate(result.params.keys()) if i<options['n_days']]
pval_Go = [result.pvalues[i] for i in result.params.keys() if 'DualGo' in i]
pval_NoGo = [result.pvalues[i] for i in result.params.keys() if 'DualNoGo' in i]
#+end_src

#+RESULTS:

#+begin_src ipython
coefs_DPA = [result.params[j] for i, j in enumerate(result.params.keys()) if i<options['n_days']]
coefs_Go = [result.params[i] for i in result.params.keys() if 'DualGo' in i]
coefs_NoGo = [result.params[i] for i in result.params.keys() if 'DualNoGo' in i]
#+end_src

#+RESULTS:

#+begin_src ipython
plt.plot(np.arange(1, options['n_days']+1), coefs_DPA, '-o', color='r')
plt.plot(np.arange(1, options['n_days']+1), coefs_Go, '-o', color='b')
plt.plot(np.arange(1, options['n_days']+1), coefs_NoGo, '-o', color='g')

print(np.round(pval_DPA, 3))
print(np.round(pval_Go, 3))
print(np.round(pval_NoGo, 3))

for i in range(len(coefs_DPA)):
    if pval_DPA[i] < 0.05:
        plt.text(i+1, .01, '*', ha='center', va='bottom', color='r')
    if pval_Go[i] < 0.05:
        plt.text(i+1, .05, '*', ha='center', va='bottom', color='b')
    if pval_NoGo[i] < 0.05:
        plt.text(i+1, .1, '*', ha='center', va='bottom', color='g')

plt.xlabel('Day')
plt.ylabel('sign(SOLD) $\\beta_{day}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.    0.411 0.725 0.159 1.    0.832]
: [0.411 0.281 0.    0.135 0.158 0.548]
: [0.639 0.213 0.934 0.097 0.281 0.52 ]
[[./figures/landscape/figure_110.png]]
:END:


#+RESULTS:
#+begin_src ipython

#+end_src

#+RESULTS:


*** dist ED

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')

  formula = 'dist_ED ~ day * tasks'

  results = []
  glm = smf.glm(formula=formula, data=y, family=sm.families.Gaussian())
  # glm = smf.mixedlm(formula, y, groups=y['day'], re_formula='1')
  result = glm.fit()
  pval = result.pvalues

  print(result.summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:                dist_ED   No. Observations:                 3648
Model:                            GLM   Df Residuals:                     3630
Model Family:                Gaussian   Df Model:                           17
Link Function:               Identity   Scale:                         0.22039
Method:                          IRLS   Log-Likelihood:                -2408.7
Date:                Wed, 28 Aug 2024   Deviance:                       800.02
Time:                        16:34:46   Pearson chi2:                     800.
No. Iterations:                     3   Pseudo R-squ. (CS):            0.08119
Covariance Type:            nonrobust
================================================================================================
                                   coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------------------------
Intercept                        0.1062      0.031      3.384      0.001       0.045       0.168
day[T.2.0]                      -0.1781      0.044     -4.014      0.000      -0.265      -0.091
day[T.3.0]                      -0.0950      0.044     -2.141      0.032      -0.182      -0.008
day[T.4.0]                      -0.1120      0.044     -2.526      0.012      -0.199      -0.025
day[T.5.0]                      -0.2633      0.044     -5.936      0.000      -0.350      -0.176
day[T.6.0]                      -0.3259      0.057     -5.691      0.000      -0.438      -0.214
tasks[T.DualGo]                  0.1092      0.044      2.461      0.014       0.022       0.196
tasks[T.DualNoGo]               -0.1060      0.044     -2.389      0.017      -0.193      -0.019
day[T.2.0]:tasks[T.DualGo]      -0.0153      0.063     -0.243      0.808      -0.138       0.108
day[T.3.0]:tasks[T.DualGo]      -0.0646      0.063     -1.030      0.303      -0.188       0.058
day[T.4.0]:tasks[T.DualGo]      -0.1031      0.063     -1.643      0.100      -0.226       0.020
day[T.5.0]:tasks[T.DualGo]       0.1380      0.063      2.200      0.028       0.015       0.261
day[T.6.0]:tasks[T.DualGo]      -0.0212      0.081     -0.262      0.793      -0.180       0.138
day[T.2.0]:tasks[T.DualNoGo]    -0.0168      0.063     -0.267      0.789      -0.140       0.106
day[T.3.0]:tasks[T.DualNoGo]    -0.0012      0.063     -0.019      0.985      -0.124       0.122
day[T.4.0]:tasks[T.DualNoGo]    -0.0025      0.063     -0.040      0.968      -0.125       0.120
day[T.5.0]:tasks[T.DualNoGo]    -0.0667      0.063     -1.063      0.288      -0.190       0.056
day[T.6.0]:tasks[T.DualNoGo]     0.0113      0.081      0.140      0.889      -0.147       0.170
================================================================================================
#+end_example

#+begin_src ipython
pval_DPA = [result.pvalues[j] for i, j in enumerate(result.params.keys()) if i<options['n_days']]
pval_Go = [result.pvalues[i] for i in result.params.keys() if 'DualGo' in i]
pval_NoGo = [result.pvalues[i] for i in result.params.keys() if 'DualNoGo' in i]
#+end_src

#+RESULTS:

#+begin_src ipython
coefs_DPA = [result.params[j] for i, j in enumerate(result.params.keys()) if i<options['n_days']]
coefs_Go = [result.params[i] for i in result.params.keys() if 'DualGo' in i]
coefs_NoGo = [result.params[i] for i in result.params.keys() if 'DualNoGo' in i]
#+end_src

#+RESULTS:

#+begin_src ipython
plt.plot(np.arange(1, options['n_days']+1), coefs_DPA, '-o', color='r')
plt.plot(np.arange(1, options['n_days']+1), coefs_Go, '-o', color='b')
plt.plot(np.arange(1, options['n_days']+1), coefs_NoGo, '-o', color='g')

print(np.round(pval_DPA, 3))
print(np.round(pval_Go, 3))
print(np.round(pval_NoGo, 3))

for i in range(len(coefs_DPA)):
    if pval_DPA[i] < 0.05:
        plt.text(i+1, .01, '*', ha='center', va='bottom', color='r')
    if pval_Go[i] < 0.05:
        plt.text(i+1, .05, '*', ha='center', va='bottom', color='b')
    if pval_NoGo[i] < 0.05:
        plt.text(i+1, .1, '*', ha='center', va='bottom', color='g')

plt.xlabel('Day')
plt.ylabel('DOED $\\beta_{day}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.001 0.    0.032 0.012 0.    0.   ]
: [0.014 0.808 0.303 0.1   0.028 0.793]
: [0.017 0.789 0.985 0.968 0.288 0.889]
[[./figures/landscape/figure_115.png]]
:END:


#+RESULTS:
#+begin_src ipython

#+end_src

#+RESULTS:

*** sign dist ED

#+begin_src ipython
  import statsmodels.api as sm
  import statsmodels.formula.api as smf

  y['tasks'] = y['tasks'].astype('category')
  y['day'] = y['day'].astype('category')

  formula = 'OED_sign ~ day * tasks'

  results = []
  glm = smf.glm(formula=formula, data=y, family=sm.families.Gaussian())
  # glm = smf.mixedlm(formula, y, groups=y['day'], re_formula='1')
  result = glm.fit()
  pval = result.pvalues

  print(result.summary())
    #+end_src

#+RESULTS:
#+begin_example
                 Generalized Linear Model Regression Results
==============================================================================
Dep. Variable:               OED_sign   No. Observations:                 3648
Model:                            GLM   Df Residuals:                     3630
Model Family:                Gaussian   Df Model:                           17
Link Function:               Identity   Scale:                         0.23677
Method:                          IRLS   Log-Likelihood:                -2539.5
Date:                Wed, 28 Aug 2024   Deviance:                       859.47
Time:                        16:35:26   Pearson chi2:                     859.
No. Iterations:                     3   Pseudo R-squ. (CS):            0.05890
Covariance Type:            nonrobust
================================================================================================
                                   coef    std err          z      P>|z|      [0.025      0.975]
------------------------------------------------------------------------------------------------
Intercept                        0.5714      0.033     17.576      0.000       0.508       0.635
day[T.2.0]                      -0.1473      0.046     -3.204      0.001      -0.237      -0.057
day[T.3.0]                       0.0313      0.046      0.680      0.497      -0.059       0.121
day[T.4.0]                      -0.0357      0.046     -0.777      0.437      -0.126       0.054
day[T.5.0]                      -0.2054      0.046     -4.466      0.000      -0.295      -0.115
day[T.6.0]                      -0.1652      0.059     -2.783      0.005      -0.282      -0.049
tasks[T.DualGo]                  0.1071      0.046      2.330      0.020       0.017       0.197
tasks[T.DualNoGo]               -0.0938      0.046     -2.039      0.041      -0.184      -0.004
day[T.2.0]:tasks[T.DualGo]       0.0268      0.065      0.412      0.680      -0.101       0.154
day[T.3.0]:tasks[T.DualGo]      -0.0268      0.065     -0.412      0.680      -0.154       0.101
day[T.4.0]:tasks[T.DualGo]      -0.1295      0.065     -1.991      0.046      -0.257      -0.002
day[T.5.0]:tasks[T.DualGo]       0.1250      0.065      1.922      0.055      -0.002       0.252
day[T.6.0]:tasks[T.DualGo]      -0.0030      0.084     -0.035      0.972      -0.168       0.162
day[T.2.0]:tasks[T.DualNoGo]    -0.0536      0.065     -0.824      0.410      -0.181       0.074
day[T.3.0]:tasks[T.DualNoGo]    -0.0580      0.065     -0.893      0.372      -0.185       0.069
day[T.4.0]:tasks[T.DualNoGo]     0.0357      0.065      0.549      0.583      -0.092       0.163
day[T.5.0]:tasks[T.DualNoGo]    -0.0089      0.065     -0.137      0.891      -0.136       0.119
day[T.6.0]:tasks[T.DualNoGo]     0.1146      0.084      1.365      0.172      -0.050       0.279
================================================================================================
#+end_example

#+begin_src ipython
pval_DPA = [result.pvalues[j] for i, j in enumerate(result.params.keys()) if i<options['n_days']]
pval_Go = [result.pvalues[i] for i in result.params.keys() if 'DualGo' in i]
pval_NoGo = [result.pvalues[i] for i in result.params.keys() if 'DualNoGo' in i]
#+end_src

#+RESULTS:

#+begin_src ipython
coefs_DPA = [result.params[j] for i, j in enumerate(result.params.keys()) if i<options['n_days']]
coefs_Go = [result.params[i] for i in result.params.keys() if 'DualGo' in i]
coefs_NoGo = [result.params[i] for i in result.params.keys() if 'DualNoGo' in i]
#+end_src

#+RESULTS:

#+begin_src ipython
plt.plot(np.arange(1, options['n_days']+1), coefs_DPA, '-o', color='r')
plt.plot(np.arange(1, options['n_days']+1), coefs_Go, '-o', color='b')
plt.plot(np.arange(1, options['n_days']+1), coefs_NoGo, '-o', color='g')

print(np.round(pval_DPA, 3))
print(np.round(pval_Go, 3))
print(np.round(pval_NoGo, 3))

for i in range(len(coefs_DPA)):
    if pval_DPA[i] < 0.05:
        plt.text(i+1, .01, '*', ha='center', va='bottom', color='r')
    if pval_Go[i] < 0.05:
        plt.text(i+1, .05, '*', ha='center', va='bottom', color='b')
    if pval_NoGo[i] < 0.05:
        plt.text(i+1, .1, '*', ha='center', va='bottom', color='g')

plt.xlabel('Day')
plt.ylabel('sign(DOED) $\\beta_{day}$')
plt.show()
#+end_src

#+RESULTS:
:RESULTS:
: [0.    0.001 0.497 0.437 0.    0.005]
: [0.02  0.68  0.68  0.046 0.055 0.972]
: [0.041 0.41  0.372 0.583 0.891 0.172]
[[./figures/landscape/figure_120.png]]
:END:

#+RESULTS:
#+begin_src ipython

#+end_src

#+RESULTS:
