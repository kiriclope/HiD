#+TITLE: Data driven RNN
#+STARTUP: fold
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session my_session :kernel torch

* Torch model
** RNN

#+begin_src ipython
  import torch
  import torch.nn as nn
  import torch.optim as optim

  # Define the RNN model
  class MultivariateRNN(nn.Module):
      def __init__(self, input_size, hidden_size, num_layers, output_size, device='cuda'):
          super(MultivariateRNN, self).__init__()
          self.hidden_size = hidden_size
          self.num_layers = num_layers
          self.device = device

          # You can swap nn.RNN with nn.LSTM or nn.GRU depending on your requirements
          self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True,
                            device=self.device, nonlinearity='relu')
          
          self.fc = nn.Linear(hidden_size, output_size, device=self.device)

      def forward(self, x):
          # Initial hidden state (can also initialize this outside and pass it as a parameter)
          h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size, device=self.device)

          # Forward propagate the RNN
          out, _ = self.rnn(x, h0)

          out = self.fc(out)
          # Pass the output of the last time step through a fully connected layer
          # out = self.fc(out[:, -1, :])
          return out
#+end_src

#+RESULTS:

** Synthetic Data

#+begin_src ipython
  def generate_multivariate_time_series(num_series, num_steps, num_features, device='cuda'):
      np.random.seed(42)  # For reproducibility

      # Generate random frequencies and phases for the sine waves
      frequencies = np.random.uniform(low=0.1, high=2.0, size=(num_features))
      phases = np.random.uniform(low=0, high=2*np.pi, size=(num_features))
      noise = np.random.uniform(low=0, high=1, size=(num_series))

      # Generate time steps for the sine waves
      time_steps = np.linspace(0, num_steps, num_steps)

      # Initialize the data array
      data = np.zeros((num_series, num_steps, num_features))

      # Populate the data array with sine waves
      for i in range(num_series):
          for j in range(num_steps):
              for k in range(num_features):
                  data[i, j, k] = np.sin(2 * np.pi * j / num_steps - phases[k]) + np.random.uniform()
                  
      # Return as torch.FloatTensor
      return torch.FloatTensor(data).to(device)

#+end_src

#+RESULTS:

** Test on synthetic data
*** Create synthetic data

#+begin_src ipython
  num_series = 10  # Number of time series samples to generate
  num_steps = 180  # Number of time steps in each time series
  num_features = 10  # Number of features (signals) in each time series

  # Generate synthetic data
  synthetic_data = generate_multivariate_time_series(num_series, num_steps, num_features)

  # Split the data into inputs (X) and targets (Y), e.g., use previous timesteps to predict the next timestep
  X = synthetic_data[:, :-1, :]  # Using all but the last timestep as input
  Y = synthetic_data[:, 1:, :]   # Using all but the first timestep as target (shifted by one)

  print("Input shape:", X.shape)
  print("Target shape:", Y.shape)

#+end_src

#+RESULTS:
: Input shape: torch.Size([10, 179, 10])
: Target shape: torch.Size([10, 179, 10])

#+begin_src ipython
  plt.plot(np.arange(0, num_steps, 180), np.sin(num_steps))
  plt.plot(X.cpu().numpy()[0,:,2], alpha=1)
  plt.plot(X.cpu().numpy()[3,:,0], alpha=1, color='r')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/7c8537a9b6470940eb3110f146352085aa4da802.png]]

*** Set model parameters

#+begin_src ipython
  import torch
  from torch.utils.data import TensorDataset, DataLoader

  # Hyperparameters
  num_epochs = 100
  batch_size = 32
  learning_rate = 0.001
  hidden_size = 10
  num_layers = 1

  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

  # Split the dataset into training and validation sets
  train_size = int(0.8 * len(X))
  val_size = len(X) - train_size

  # Create data sets
  train_dataset = TensorDataset(X[:train_size], Y[:train_size])
  val_dataset = TensorDataset(X[train_size:], Y[train_size:])

  # Create data loaders
  train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
  val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)

  # Define Model
  model = MultivariateRNN(input_size=num_features, hidden_size=hidden_size, num_layers=num_layers, output_size=num_features)
  model = model.to(device)

  # Loss and optimizer
  criterion = nn.MSELoss()
  optimizer = optim.Adam(model.parameters(), lr=learning_rate)  
#+end_src

#+RESULTS:

*** Train model 

#+begin_src ipython
import torch.optim as optim

# Define the number of epochs.
num_epochs = 1000  # Adjust the number of epochs

# Training loop.
for epoch in range(num_epochs):
    model.train()
    for batch_idx, (data, targets) in enumerate(train_loader):
        # Forward pass.
        outputs = model(data)
        loss = criterion(outputs, targets)

        # Backward pass and optimization.
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    # Validation loop.
    model.eval()
    val_loss = 0.0
    with torch.no_grad():
        for data, targets in val_loader:
            outputs = model(data)
            loss = criterion(outputs, targets)
            val_loss += loss.item() * data.size(0)
    val_loss /= len(val_loader.dataset)

    # Print training/validation statistics.
    # You may want to save the model if it has improved.
    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')

# Don't forget to switch to CPU/GPU based on your setup:
# model.to('cuda') or model.to('cpu')
#+end_src

#+RESULTS:
#+begin_example
  Epoch 1/1000, Training Loss: 0.9546, Validation Loss: 0.9546
  Epoch 2/1000, Training Loss: 0.9445, Validation Loss: 0.9445
  Epoch 3/1000, Training Loss: 0.9347, Validation Loss: 0.9347
  Epoch 4/1000, Training Loss: 0.9252, Validation Loss: 0.9252
  Epoch 5/1000, Training Loss: 0.9159, Validation Loss: 0.9159
  Epoch 6/1000, Training Loss: 0.9070, Validation Loss: 0.9070
  Epoch 7/1000, Training Loss: 0.8982, Validation Loss: 0.8982
  Epoch 8/1000, Training Loss: 0.8897, Validation Loss: 0.8897
  Epoch 9/1000, Training Loss: 0.8815, Validation Loss: 0.8815
  Epoch 10/1000, Training Loss: 0.8735, Validation Loss: 0.8735
  Epoch 11/1000, Training Loss: 0.8657, Validation Loss: 0.8657
  Epoch 12/1000, Training Loss: 0.8582, Validation Loss: 0.8582
  Epoch 13/1000, Training Loss: 0.8508, Validation Loss: 0.8508
  Epoch 14/1000, Training Loss: 0.8436, Validation Loss: 0.8436
  Epoch 15/1000, Training Loss: 0.8366, Validation Loss: 0.8366
  Epoch 16/1000, Training Loss: 0.8298, Validation Loss: 0.8298
  Epoch 17/1000, Training Loss: 0.8231, Validation Loss: 0.8231
  Epoch 18/1000, Training Loss: 0.8166, Validation Loss: 0.8166
  Epoch 19/1000, Training Loss: 0.8102, Validation Loss: 0.8102
  Epoch 20/1000, Training Loss: 0.8040, Validation Loss: 0.8040
  Epoch 21/1000, Training Loss: 0.7980, Validation Loss: 0.7980
  Epoch 22/1000, Training Loss: 0.7921, Validation Loss: 0.7921
  Epoch 23/1000, Training Loss: 0.7863, Validation Loss: 0.7863
  Epoch 24/1000, Training Loss: 0.7806, Validation Loss: 0.7806
  Epoch 25/1000, Training Loss: 0.7750, Validation Loss: 0.7750
  Epoch 26/1000, Training Loss: 0.7696, Validation Loss: 0.7696
  Epoch 27/1000, Training Loss: 0.7642, Validation Loss: 0.7642
  Epoch 28/1000, Training Loss: 0.7589, Validation Loss: 0.7589
  Epoch 29/1000, Training Loss: 0.7537, Validation Loss: 0.7537
  Epoch 30/1000, Training Loss: 0.7485, Validation Loss: 0.7485
  Epoch 31/1000, Training Loss: 0.7435, Validation Loss: 0.7435
  Epoch 32/1000, Training Loss: 0.7384, Validation Loss: 0.7384
  Epoch 33/1000, Training Loss: 0.7334, Validation Loss: 0.7334
  Epoch 34/1000, Training Loss: 0.7284, Validation Loss: 0.7284
  Epoch 35/1000, Training Loss: 0.7235, Validation Loss: 0.7235
  Epoch 36/1000, Training Loss: 0.7185, Validation Loss: 0.7185
  Epoch 37/1000, Training Loss: 0.7136, Validation Loss: 0.7136
  Epoch 38/1000, Training Loss: 0.7087, Validation Loss: 0.7087
  Epoch 39/1000, Training Loss: 0.7038, Validation Loss: 0.7038
  Epoch 40/1000, Training Loss: 0.6989, Validation Loss: 0.6989
  Epoch 41/1000, Training Loss: 0.6940, Validation Loss: 0.6940
  Epoch 42/1000, Training Loss: 0.6891, Validation Loss: 0.6891
  Epoch 43/1000, Training Loss: 0.6842, Validation Loss: 0.6842
  Epoch 44/1000, Training Loss: 0.6792, Validation Loss: 0.6792
  Epoch 45/1000, Training Loss: 0.6743, Validation Loss: 0.6743
  Epoch 46/1000, Training Loss: 0.6693, Validation Loss: 0.6693
  Epoch 47/1000, Training Loss: 0.6644, Validation Loss: 0.6644
  Epoch 48/1000, Training Loss: 0.6593, Validation Loss: 0.6593
  Epoch 49/1000, Training Loss: 0.6543, Validation Loss: 0.6543
  Epoch 50/1000, Training Loss: 0.6492, Validation Loss: 0.6492
  Epoch 51/1000, Training Loss: 0.6441, Validation Loss: 0.6441
  Epoch 52/1000, Training Loss: 0.6390, Validation Loss: 0.6390
  Epoch 53/1000, Training Loss: 0.6338, Validation Loss: 0.6338
  Epoch 54/1000, Training Loss: 0.6286, Validation Loss: 0.6286
  Epoch 55/1000, Training Loss: 0.6233, Validation Loss: 0.6233
  Epoch 56/1000, Training Loss: 0.6180, Validation Loss: 0.6180
  Epoch 57/1000, Training Loss: 0.6127, Validation Loss: 0.6127
  Epoch 58/1000, Training Loss: 0.6073, Validation Loss: 0.6073
  Epoch 59/1000, Training Loss: 0.6018, Validation Loss: 0.6018
  Epoch 60/1000, Training Loss: 0.5963, Validation Loss: 0.5963
  Epoch 61/1000, Training Loss: 0.5907, Validation Loss: 0.5907
  Epoch 62/1000, Training Loss: 0.5850, Validation Loss: 0.5850
  Epoch 63/1000, Training Loss: 0.5793, Validation Loss: 0.5793
  Epoch 64/1000, Training Loss: 0.5735, Validation Loss: 0.5735
  Epoch 65/1000, Training Loss: 0.5676, Validation Loss: 0.5676
  Epoch 66/1000, Training Loss: 0.5616, Validation Loss: 0.5616
  Epoch 67/1000, Training Loss: 0.5556, Validation Loss: 0.5556
  Epoch 68/1000, Training Loss: 0.5495, Validation Loss: 0.5495
  Epoch 69/1000, Training Loss: 0.5434, Validation Loss: 0.5434
  Epoch 70/1000, Training Loss: 0.5371, Validation Loss: 0.5371
  Epoch 71/1000, Training Loss: 0.5308, Validation Loss: 0.5308
  Epoch 72/1000, Training Loss: 0.5245, Validation Loss: 0.5245
  Epoch 73/1000, Training Loss: 0.5181, Validation Loss: 0.5181
  Epoch 74/1000, Training Loss: 0.5117, Validation Loss: 0.5117
  Epoch 75/1000, Training Loss: 0.5053, Validation Loss: 0.5053
  Epoch 76/1000, Training Loss: 0.4988, Validation Loss: 0.4988
  Epoch 77/1000, Training Loss: 0.4924, Validation Loss: 0.4924
  Epoch 78/1000, Training Loss: 0.4859, Validation Loss: 0.4859
  Epoch 79/1000, Training Loss: 0.4793, Validation Loss: 0.4793
  Epoch 80/1000, Training Loss: 0.4727, Validation Loss: 0.4727
  Epoch 81/1000, Training Loss: 0.4661, Validation Loss: 0.4661
  Epoch 82/1000, Training Loss: 0.4595, Validation Loss: 0.4595
  Epoch 83/1000, Training Loss: 0.4529, Validation Loss: 0.4529
  Epoch 84/1000, Training Loss: 0.4463, Validation Loss: 0.4463
  Epoch 85/1000, Training Loss: 0.4397, Validation Loss: 0.4397
  Epoch 86/1000, Training Loss: 0.4331, Validation Loss: 0.4331
  Epoch 87/1000, Training Loss: 0.4265, Validation Loss: 0.4265
  Epoch 88/1000, Training Loss: 0.4199, Validation Loss: 0.4199
  Epoch 89/1000, Training Loss: 0.4134, Validation Loss: 0.4134
  Epoch 90/1000, Training Loss: 0.4069, Validation Loss: 0.4069
  Epoch 91/1000, Training Loss: 0.4005, Validation Loss: 0.4005
  Epoch 92/1000, Training Loss: 0.3941, Validation Loss: 0.3941
  Epoch 93/1000, Training Loss: 0.3878, Validation Loss: 0.3878
  Epoch 94/1000, Training Loss: 0.3815, Validation Loss: 0.3815
  Epoch 95/1000, Training Loss: 0.3752, Validation Loss: 0.3752
  Epoch 96/1000, Training Loss: 0.3690, Validation Loss: 0.3690
  Epoch 97/1000, Training Loss: 0.3629, Validation Loss: 0.3629
  Epoch 98/1000, Training Loss: 0.3568, Validation Loss: 0.3568
  Epoch 99/1000, Training Loss: 0.3507, Validation Loss: 0.3507
  Epoch 100/1000, Training Loss: 0.3447, Validation Loss: 0.3447
  Epoch 101/1000, Training Loss: 0.3388, Validation Loss: 0.3388
  Epoch 102/1000, Training Loss: 0.3329, Validation Loss: 0.3329
  Epoch 103/1000, Training Loss: 0.3271, Validation Loss: 0.3271
  Epoch 104/1000, Training Loss: 0.3213, Validation Loss: 0.3213
  Epoch 105/1000, Training Loss: 0.3156, Validation Loss: 0.3156
  Epoch 106/1000, Training Loss: 0.3100, Validation Loss: 0.3100
  Epoch 107/1000, Training Loss: 0.3044, Validation Loss: 0.3044
  Epoch 108/1000, Training Loss: 0.2989, Validation Loss: 0.2989
  Epoch 109/1000, Training Loss: 0.2935, Validation Loss: 0.2935
  Epoch 110/1000, Training Loss: 0.2882, Validation Loss: 0.2882
  Epoch 111/1000, Training Loss: 0.2830, Validation Loss: 0.2830
  Epoch 112/1000, Training Loss: 0.2778, Validation Loss: 0.2778
  Epoch 113/1000, Training Loss: 0.2728, Validation Loss: 0.2728
  Epoch 114/1000, Training Loss: 0.2678, Validation Loss: 0.2678
  Epoch 115/1000, Training Loss: 0.2629, Validation Loss: 0.2629
  Epoch 116/1000, Training Loss: 0.2580, Validation Loss: 0.2580
  Epoch 117/1000, Training Loss: 0.2533, Validation Loss: 0.2533
  Epoch 118/1000, Training Loss: 0.2486, Validation Loss: 0.2486
  Epoch 119/1000, Training Loss: 0.2441, Validation Loss: 0.2441
  Epoch 120/1000, Training Loss: 0.2396, Validation Loss: 0.2396
  Epoch 121/1000, Training Loss: 0.2352, Validation Loss: 0.2352
  Epoch 122/1000, Training Loss: 0.2308, Validation Loss: 0.2308
  Epoch 123/1000, Training Loss: 0.2266, Validation Loss: 0.2266
  Epoch 124/1000, Training Loss: 0.2225, Validation Loss: 0.2225
  Epoch 125/1000, Training Loss: 0.2184, Validation Loss: 0.2184
  Epoch 126/1000, Training Loss: 0.2145, Validation Loss: 0.2145
  Epoch 127/1000, Training Loss: 0.2107, Validation Loss: 0.2107
  Epoch 128/1000, Training Loss: 0.2069, Validation Loss: 0.2069
  Epoch 129/1000, Training Loss: 0.2032, Validation Loss: 0.2032
  Epoch 130/1000, Training Loss: 0.1997, Validation Loss: 0.1997
  Epoch 131/1000, Training Loss: 0.1962, Validation Loss: 0.1962
  Epoch 132/1000, Training Loss: 0.1929, Validation Loss: 0.1929
  Epoch 133/1000, Training Loss: 0.1897, Validation Loss: 0.1897
  Epoch 134/1000, Training Loss: 0.1866, Validation Loss: 0.1866
  Epoch 135/1000, Training Loss: 0.1836, Validation Loss: 0.1836
  Epoch 136/1000, Training Loss: 0.1808, Validation Loss: 0.1808
  Epoch 137/1000, Training Loss: 0.1780, Validation Loss: 0.1780
  Epoch 138/1000, Training Loss: 0.1753, Validation Loss: 0.1753
  Epoch 139/1000, Training Loss: 0.1726, Validation Loss: 0.1726
  Epoch 140/1000, Training Loss: 0.1701, Validation Loss: 0.1701
  Epoch 141/1000, Training Loss: 0.1677, Validation Loss: 0.1677
  Epoch 142/1000, Training Loss: 0.1652, Validation Loss: 0.1652
  Epoch 143/1000, Training Loss: 0.1629, Validation Loss: 0.1629
  Epoch 144/1000, Training Loss: 0.1606, Validation Loss: 0.1606
  Epoch 145/1000, Training Loss: 0.1584, Validation Loss: 0.1584
  Epoch 146/1000, Training Loss: 0.1562, Validation Loss: 0.1562
  Epoch 147/1000, Training Loss: 0.1540, Validation Loss: 0.1540
  Epoch 148/1000, Training Loss: 0.1520, Validation Loss: 0.1520
  Epoch 149/1000, Training Loss: 0.1500, Validation Loss: 0.1500
  Epoch 150/1000, Training Loss: 0.1480, Validation Loss: 0.1480
  Epoch 151/1000, Training Loss: 0.1461, Validation Loss: 0.1461
  Epoch 152/1000, Training Loss: 0.1442, Validation Loss: 0.1442
  Epoch 153/1000, Training Loss: 0.1424, Validation Loss: 0.1424
  Epoch 154/1000, Training Loss: 0.1406, Validation Loss: 0.1406
  Epoch 155/1000, Training Loss: 0.1389, Validation Loss: 0.1389
  Epoch 156/1000, Training Loss: 0.1373, Validation Loss: 0.1373
  Epoch 157/1000, Training Loss: 0.1356, Validation Loss: 0.1356
  Epoch 158/1000, Training Loss: 0.1341, Validation Loss: 0.1341
  Epoch 159/1000, Training Loss: 0.1326, Validation Loss: 0.1326
  Epoch 160/1000, Training Loss: 0.1311, Validation Loss: 0.1311
  Epoch 161/1000, Training Loss: 0.1297, Validation Loss: 0.1297
  Epoch 162/1000, Training Loss: 0.1283, Validation Loss: 0.1283
  Epoch 163/1000, Training Loss: 0.1270, Validation Loss: 0.1270
  Epoch 164/1000, Training Loss: 0.1257, Validation Loss: 0.1257
  Epoch 165/1000, Training Loss: 0.1244, Validation Loss: 0.1244
  Epoch 166/1000, Training Loss: 0.1233, Validation Loss: 0.1233
  Epoch 167/1000, Training Loss: 0.1221, Validation Loss: 0.1221
  Epoch 168/1000, Training Loss: 0.1210, Validation Loss: 0.1210
  Epoch 169/1000, Training Loss: 0.1199, Validation Loss: 0.1199
  Epoch 170/1000, Training Loss: 0.1189, Validation Loss: 0.1189
  Epoch 171/1000, Training Loss: 0.1180, Validation Loss: 0.1180
  Epoch 172/1000, Training Loss: 0.1170, Validation Loss: 0.1170
  Epoch 173/1000, Training Loss: 0.1161, Validation Loss: 0.1161
  Epoch 174/1000, Training Loss: 0.1153, Validation Loss: 0.1153
  Epoch 175/1000, Training Loss: 0.1145, Validation Loss: 0.1145
  Epoch 176/1000, Training Loss: 0.1137, Validation Loss: 0.1137
  Epoch 177/1000, Training Loss: 0.1130, Validation Loss: 0.1130
  Epoch 178/1000, Training Loss: 0.1123, Validation Loss: 0.1123
  Epoch 179/1000, Training Loss: 0.1116, Validation Loss: 0.1116
  Epoch 180/1000, Training Loss: 0.1110, Validation Loss: 0.1110
  Epoch 181/1000, Training Loss: 0.1104, Validation Loss: 0.1104
  Epoch 182/1000, Training Loss: 0.1098, Validation Loss: 0.1098
  Epoch 183/1000, Training Loss: 0.1093, Validation Loss: 0.1093
  Epoch 184/1000, Training Loss: 0.1088, Validation Loss: 0.1088
  Epoch 185/1000, Training Loss: 0.1083, Validation Loss: 0.1083
  Epoch 186/1000, Training Loss: 0.1078, Validation Loss: 0.1078
  Epoch 187/1000, Training Loss: 0.1074, Validation Loss: 0.1074
  Epoch 188/1000, Training Loss: 0.1070, Validation Loss: 0.1070
  Epoch 189/1000, Training Loss: 0.1066, Validation Loss: 0.1066
  Epoch 190/1000, Training Loss: 0.1062, Validation Loss: 0.1062
  Epoch 191/1000, Training Loss: 0.1059, Validation Loss: 0.1059
  Epoch 192/1000, Training Loss: 0.1055, Validation Loss: 0.1055
  Epoch 193/1000, Training Loss: 0.1052, Validation Loss: 0.1052
  Epoch 194/1000, Training Loss: 0.1049, Validation Loss: 0.1049
  Epoch 195/1000, Training Loss: 0.1046, Validation Loss: 0.1046
  Epoch 196/1000, Training Loss: 0.1044, Validation Loss: 0.1044
  Epoch 197/1000, Training Loss: 0.1041, Validation Loss: 0.1041
  Epoch 198/1000, Training Loss: 0.1038, Validation Loss: 0.1038
  Epoch 199/1000, Training Loss: 0.1036, Validation Loss: 0.1036
  Epoch 200/1000, Training Loss: 0.1034, Validation Loss: 0.1034
  Epoch 201/1000, Training Loss: 0.1032, Validation Loss: 0.1032
  Epoch 202/1000, Training Loss: 0.1030, Validation Loss: 0.1030
  Epoch 203/1000, Training Loss: 0.1028, Validation Loss: 0.1028
  Epoch 204/1000, Training Loss: 0.1026, Validation Loss: 0.1026
  Epoch 205/1000, Training Loss: 0.1024, Validation Loss: 0.1024
  Epoch 206/1000, Training Loss: 0.1022, Validation Loss: 0.1022
  Epoch 207/1000, Training Loss: 0.1021, Validation Loss: 0.1021
  Epoch 208/1000, Training Loss: 0.1019, Validation Loss: 0.1019
  Epoch 209/1000, Training Loss: 0.1017, Validation Loss: 0.1017
  Epoch 210/1000, Training Loss: 0.1016, Validation Loss: 0.1016
  Epoch 211/1000, Training Loss: 0.1015, Validation Loss: 0.1015
  Epoch 212/1000, Training Loss: 0.1013, Validation Loss: 0.1013
  Epoch 213/1000, Training Loss: 0.1012, Validation Loss: 0.1012
  Epoch 214/1000, Training Loss: 0.1011, Validation Loss: 0.1011
  Epoch 215/1000, Training Loss: 0.1010, Validation Loss: 0.1010
  Epoch 216/1000, Training Loss: 0.1009, Validation Loss: 0.1009
  Epoch 217/1000, Training Loss: 0.1008, Validation Loss: 0.1008
  Epoch 218/1000, Training Loss: 0.1007, Validation Loss: 0.1007
  Epoch 219/1000, Training Loss: 0.1006, Validation Loss: 0.1006
  Epoch 220/1000, Training Loss: 0.1005, Validation Loss: 0.1005
  Epoch 221/1000, Training Loss: 0.1004, Validation Loss: 0.1004
  Epoch 222/1000, Training Loss: 0.1004, Validation Loss: 0.1004
  Epoch 223/1000, Training Loss: 0.1003, Validation Loss: 0.1003
  Epoch 224/1000, Training Loss: 0.1002, Validation Loss: 0.1002
  Epoch 225/1000, Training Loss: 0.1001, Validation Loss: 0.1001
  Epoch 226/1000, Training Loss: 0.1001, Validation Loss: 0.1001
  Epoch 227/1000, Training Loss: 0.1000, Validation Loss: 0.1000
  Epoch 228/1000, Training Loss: 0.0999, Validation Loss: 0.0999
  Epoch 229/1000, Training Loss: 0.0999, Validation Loss: 0.0999
  Epoch 230/1000, Training Loss: 0.0998, Validation Loss: 0.0998
  Epoch 231/1000, Training Loss: 0.0998, Validation Loss: 0.0998
  Epoch 232/1000, Training Loss: 0.0997, Validation Loss: 0.0997
  Epoch 233/1000, Training Loss: 0.0997, Validation Loss: 0.0997
  Epoch 234/1000, Training Loss: 0.0996, Validation Loss: 0.0996
  Epoch 235/1000, Training Loss: 0.0996, Validation Loss: 0.0996
  Epoch 236/1000, Training Loss: 0.0995, Validation Loss: 0.0995
  Epoch 237/1000, Training Loss: 0.0995, Validation Loss: 0.0995
  Epoch 238/1000, Training Loss: 0.0994, Validation Loss: 0.0994
  Epoch 239/1000, Training Loss: 0.0994, Validation Loss: 0.0994
  Epoch 240/1000, Training Loss: 0.0994, Validation Loss: 0.0994
  Epoch 241/1000, Training Loss: 0.0993, Validation Loss: 0.0993
  Epoch 242/1000, Training Loss: 0.0993, Validation Loss: 0.0993
  Epoch 243/1000, Training Loss: 0.0992, Validation Loss: 0.0992
  Epoch 244/1000, Training Loss: 0.0992, Validation Loss: 0.0992
  Epoch 245/1000, Training Loss: 0.0992, Validation Loss: 0.0992
  Epoch 246/1000, Training Loss: 0.0991, Validation Loss: 0.0991
  Epoch 247/1000, Training Loss: 0.0991, Validation Loss: 0.0991
  Epoch 248/1000, Training Loss: 0.0991, Validation Loss: 0.0991
  Epoch 249/1000, Training Loss: 0.0990, Validation Loss: 0.0990
  Epoch 250/1000, Training Loss: 0.0990, Validation Loss: 0.0990
  Epoch 251/1000, Training Loss: 0.0990, Validation Loss: 0.0990
  Epoch 252/1000, Training Loss: 0.0989, Validation Loss: 0.0989
  Epoch 253/1000, Training Loss: 0.0989, Validation Loss: 0.0989
  Epoch 254/1000, Training Loss: 0.0989, Validation Loss: 0.0989
  Epoch 255/1000, Training Loss: 0.0988, Validation Loss: 0.0988
  Epoch 256/1000, Training Loss: 0.0988, Validation Loss: 0.0988
  Epoch 257/1000, Training Loss: 0.0988, Validation Loss: 0.0988
  Epoch 258/1000, Training Loss: 0.0987, Validation Loss: 0.0987
  Epoch 259/1000, Training Loss: 0.0987, Validation Loss: 0.0987
  Epoch 260/1000, Training Loss: 0.0987, Validation Loss: 0.0987
  Epoch 261/1000, Training Loss: 0.0987, Validation Loss: 0.0987
  Epoch 262/1000, Training Loss: 0.0986, Validation Loss: 0.0986
  Epoch 263/1000, Training Loss: 0.0986, Validation Loss: 0.0986
  Epoch 264/1000, Training Loss: 0.0986, Validation Loss: 0.0986
  Epoch 265/1000, Training Loss: 0.0985, Validation Loss: 0.0985
  Epoch 266/1000, Training Loss: 0.0985, Validation Loss: 0.0985
  Epoch 267/1000, Training Loss: 0.0985, Validation Loss: 0.0985
  Epoch 268/1000, Training Loss: 0.0984, Validation Loss: 0.0984
  Epoch 269/1000, Training Loss: 0.0984, Validation Loss: 0.0984
  Epoch 270/1000, Training Loss: 0.0984, Validation Loss: 0.0984
  Epoch 271/1000, Training Loss: 0.0984, Validation Loss: 0.0984
  Epoch 272/1000, Training Loss: 0.0983, Validation Loss: 0.0983
  Epoch 273/1000, Training Loss: 0.0983, Validation Loss: 0.0983
  Epoch 274/1000, Training Loss: 0.0983, Validation Loss: 0.0983
  Epoch 275/1000, Training Loss: 0.0982, Validation Loss: 0.0982
  Epoch 276/1000, Training Loss: 0.0982, Validation Loss: 0.0982
  Epoch 277/1000, Training Loss: 0.0982, Validation Loss: 0.0982
  Epoch 278/1000, Training Loss: 0.0982, Validation Loss: 0.0982
  Epoch 279/1000, Training Loss: 0.0981, Validation Loss: 0.0981
  Epoch 280/1000, Training Loss: 0.0981, Validation Loss: 0.0981
  Epoch 281/1000, Training Loss: 0.0981, Validation Loss: 0.0981
  Epoch 282/1000, Training Loss: 0.0980, Validation Loss: 0.0980
  Epoch 283/1000, Training Loss: 0.0980, Validation Loss: 0.0980
  Epoch 284/1000, Training Loss: 0.0980, Validation Loss: 0.0980
  Epoch 285/1000, Training Loss: 0.0980, Validation Loss: 0.0980
  Epoch 286/1000, Training Loss: 0.0979, Validation Loss: 0.0979
  Epoch 287/1000, Training Loss: 0.0979, Validation Loss: 0.0979
  Epoch 288/1000, Training Loss: 0.0979, Validation Loss: 0.0979
  Epoch 289/1000, Training Loss: 0.0978, Validation Loss: 0.0978
  Epoch 290/1000, Training Loss: 0.0978, Validation Loss: 0.0978
  Epoch 291/1000, Training Loss: 0.0978, Validation Loss: 0.0978
  Epoch 292/1000, Training Loss: 0.0978, Validation Loss: 0.0978
  Epoch 293/1000, Training Loss: 0.0977, Validation Loss: 0.0977
  Epoch 294/1000, Training Loss: 0.0977, Validation Loss: 0.0977
  Epoch 295/1000, Training Loss: 0.0977, Validation Loss: 0.0977
  Epoch 296/1000, Training Loss: 0.0977, Validation Loss: 0.0977
  Epoch 297/1000, Training Loss: 0.0976, Validation Loss: 0.0976
  Epoch 298/1000, Training Loss: 0.0976, Validation Loss: 0.0976
  Epoch 299/1000, Training Loss: 0.0976, Validation Loss: 0.0976
  Epoch 300/1000, Training Loss: 0.0975, Validation Loss: 0.0975
  Epoch 301/1000, Training Loss: 0.0975, Validation Loss: 0.0975
  Epoch 302/1000, Training Loss: 0.0975, Validation Loss: 0.0975
  Epoch 303/1000, Training Loss: 0.0975, Validation Loss: 0.0975
  Epoch 304/1000, Training Loss: 0.0974, Validation Loss: 0.0974
  Epoch 305/1000, Training Loss: 0.0974, Validation Loss: 0.0974
  Epoch 306/1000, Training Loss: 0.0974, Validation Loss: 0.0974
  Epoch 307/1000, Training Loss: 0.0973, Validation Loss: 0.0973
  Epoch 308/1000, Training Loss: 0.0973, Validation Loss: 0.0973
  Epoch 309/1000, Training Loss: 0.0973, Validation Loss: 0.0973
  Epoch 310/1000, Training Loss: 0.0973, Validation Loss: 0.0973
  Epoch 311/1000, Training Loss: 0.0972, Validation Loss: 0.0972
  Epoch 312/1000, Training Loss: 0.0972, Validation Loss: 0.0972
  Epoch 313/1000, Training Loss: 0.0972, Validation Loss: 0.0972
  Epoch 314/1000, Training Loss: 0.0971, Validation Loss: 0.0971
  Epoch 315/1000, Training Loss: 0.0971, Validation Loss: 0.0971
  Epoch 316/1000, Training Loss: 0.0971, Validation Loss: 0.0971
  Epoch 317/1000, Training Loss: 0.0971, Validation Loss: 0.0971
  Epoch 318/1000, Training Loss: 0.0970, Validation Loss: 0.0970
  Epoch 319/1000, Training Loss: 0.0970, Validation Loss: 0.0970
  Epoch 320/1000, Training Loss: 0.0970, Validation Loss: 0.0970
  Epoch 321/1000, Training Loss: 0.0970, Validation Loss: 0.0970
  Epoch 322/1000, Training Loss: 0.0969, Validation Loss: 0.0969
  Epoch 323/1000, Training Loss: 0.0969, Validation Loss: 0.0969
  Epoch 324/1000, Training Loss: 0.0969, Validation Loss: 0.0969
  Epoch 325/1000, Training Loss: 0.0968, Validation Loss: 0.0968
  Epoch 326/1000, Training Loss: 0.0968, Validation Loss: 0.0968
  Epoch 327/1000, Training Loss: 0.0968, Validation Loss: 0.0968
  Epoch 328/1000, Training Loss: 0.0968, Validation Loss: 0.0968
  Epoch 329/1000, Training Loss: 0.0967, Validation Loss: 0.0967
  Epoch 330/1000, Training Loss: 0.0967, Validation Loss: 0.0967
  Epoch 331/1000, Training Loss: 0.0967, Validation Loss: 0.0967
  Epoch 332/1000, Training Loss: 0.0967, Validation Loss: 0.0967
  Epoch 333/1000, Training Loss: 0.0966, Validation Loss: 0.0966
  Epoch 334/1000, Training Loss: 0.0966, Validation Loss: 0.0966
  Epoch 335/1000, Training Loss: 0.0966, Validation Loss: 0.0966
  Epoch 336/1000, Training Loss: 0.0966, Validation Loss: 0.0966
  Epoch 337/1000, Training Loss: 0.0965, Validation Loss: 0.0965
  Epoch 338/1000, Training Loss: 0.0965, Validation Loss: 0.0965
  Epoch 339/1000, Training Loss: 0.0965, Validation Loss: 0.0965
  Epoch 340/1000, Training Loss: 0.0965, Validation Loss: 0.0965
  Epoch 341/1000, Training Loss: 0.0964, Validation Loss: 0.0964
  Epoch 342/1000, Training Loss: 0.0964, Validation Loss: 0.0964
  Epoch 343/1000, Training Loss: 0.0964, Validation Loss: 0.0964
  Epoch 344/1000, Training Loss: 0.0964, Validation Loss: 0.0964
  Epoch 345/1000, Training Loss: 0.0963, Validation Loss: 0.0963
  Epoch 346/1000, Training Loss: 0.0963, Validation Loss: 0.0963
  Epoch 347/1000, Training Loss: 0.0963, Validation Loss: 0.0963
  Epoch 348/1000, Training Loss: 0.0963, Validation Loss: 0.0963
  Epoch 349/1000, Training Loss: 0.0962, Validation Loss: 0.0962
  Epoch 350/1000, Training Loss: 0.0962, Validation Loss: 0.0962
  Epoch 351/1000, Training Loss: 0.0962, Validation Loss: 0.0962
  Epoch 352/1000, Training Loss: 0.0962, Validation Loss: 0.0962
  Epoch 353/1000, Training Loss: 0.0962, Validation Loss: 0.0962
  Epoch 354/1000, Training Loss: 0.0961, Validation Loss: 0.0961
  Epoch 355/1000, Training Loss: 0.0961, Validation Loss: 0.0961
  Epoch 356/1000, Training Loss: 0.0961, Validation Loss: 0.0961
  Epoch 357/1000, Training Loss: 0.0961, Validation Loss: 0.0961
  Epoch 358/1000, Training Loss: 0.0960, Validation Loss: 0.0960
  Epoch 359/1000, Training Loss: 0.0960, Validation Loss: 0.0960
  Epoch 360/1000, Training Loss: 0.0960, Validation Loss: 0.0960
  Epoch 361/1000, Training Loss: 0.0960, Validation Loss: 0.0960
  Epoch 362/1000, Training Loss: 0.0959, Validation Loss: 0.0959
  Epoch 363/1000, Training Loss: 0.0959, Validation Loss: 0.0959
  Epoch 364/1000, Training Loss: 0.0959, Validation Loss: 0.0959
  Epoch 365/1000, Training Loss: 0.0959, Validation Loss: 0.0959
  Epoch 366/1000, Training Loss: 0.0958, Validation Loss: 0.0958
  Epoch 367/1000, Training Loss: 0.0958, Validation Loss: 0.0958
  Epoch 368/1000, Training Loss: 0.0958, Validation Loss: 0.0958
  Epoch 369/1000, Training Loss: 0.0958, Validation Loss: 0.0958
  Epoch 370/1000, Training Loss: 0.0958, Validation Loss: 0.0958
  Epoch 371/1000, Training Loss: 0.0957, Validation Loss: 0.0957
  Epoch 372/1000, Training Loss: 0.0957, Validation Loss: 0.0957
  Epoch 373/1000, Training Loss: 0.0957, Validation Loss: 0.0957
  Epoch 374/1000, Training Loss: 0.0957, Validation Loss: 0.0957
  Epoch 375/1000, Training Loss: 0.0956, Validation Loss: 0.0956
  Epoch 376/1000, Training Loss: 0.0956, Validation Loss: 0.0956
  Epoch 377/1000, Training Loss: 0.0956, Validation Loss: 0.0956
  Epoch 378/1000, Training Loss: 0.0956, Validation Loss: 0.0956
  Epoch 379/1000, Training Loss: 0.0955, Validation Loss: 0.0955
  Epoch 380/1000, Training Loss: 0.0955, Validation Loss: 0.0955
  Epoch 381/1000, Training Loss: 0.0955, Validation Loss: 0.0955
  Epoch 382/1000, Training Loss: 0.0955, Validation Loss: 0.0955
  Epoch 383/1000, Training Loss: 0.0954, Validation Loss: 0.0954
  Epoch 384/1000, Training Loss: 0.0954, Validation Loss: 0.0954
  Epoch 385/1000, Training Loss: 0.0954, Validation Loss: 0.0954
  Epoch 386/1000, Training Loss: 0.0954, Validation Loss: 0.0954
  Epoch 387/1000, Training Loss: 0.0953, Validation Loss: 0.0953
  Epoch 388/1000, Training Loss: 0.0953, Validation Loss: 0.0953
  Epoch 389/1000, Training Loss: 0.0953, Validation Loss: 0.0953
  Epoch 390/1000, Training Loss: 0.0953, Validation Loss: 0.0953
  Epoch 391/1000, Training Loss: 0.0952, Validation Loss: 0.0952
  Epoch 392/1000, Training Loss: 0.0952, Validation Loss: 0.0952
  Epoch 393/1000, Training Loss: 0.0952, Validation Loss: 0.0952
  Epoch 394/1000, Training Loss: 0.0952, Validation Loss: 0.0952
  Epoch 395/1000, Training Loss: 0.0951, Validation Loss: 0.0951
  Epoch 396/1000, Training Loss: 0.0951, Validation Loss: 0.0951
  Epoch 397/1000, Training Loss: 0.0951, Validation Loss: 0.0951
  Epoch 398/1000, Training Loss: 0.0951, Validation Loss: 0.0951
  Epoch 399/1000, Training Loss: 0.0950, Validation Loss: 0.0950
  Epoch 400/1000, Training Loss: 0.0950, Validation Loss: 0.0950
  Epoch 401/1000, Training Loss: 0.0950, Validation Loss: 0.0950
  Epoch 402/1000, Training Loss: 0.0950, Validation Loss: 0.0950
  Epoch 403/1000, Training Loss: 0.0950, Validation Loss: 0.0950
  Epoch 404/1000, Training Loss: 0.0949, Validation Loss: 0.0949
  Epoch 405/1000, Training Loss: 0.0949, Validation Loss: 0.0949
  Epoch 406/1000, Training Loss: 0.0949, Validation Loss: 0.0949
  Epoch 407/1000, Training Loss: 0.0949, Validation Loss: 0.0949
  Epoch 408/1000, Training Loss: 0.0948, Validation Loss: 0.0948
  Epoch 409/1000, Training Loss: 0.0948, Validation Loss: 0.0948
  Epoch 410/1000, Training Loss: 0.0948, Validation Loss: 0.0948
  Epoch 411/1000, Training Loss: 0.0948, Validation Loss: 0.0948
  Epoch 412/1000, Training Loss: 0.0947, Validation Loss: 0.0947
  Epoch 413/1000, Training Loss: 0.0947, Validation Loss: 0.0947
  Epoch 414/1000, Training Loss: 0.0947, Validation Loss: 0.0947
  Epoch 415/1000, Training Loss: 0.0947, Validation Loss: 0.0947
  Epoch 416/1000, Training Loss: 0.0946, Validation Loss: 0.0946
  Epoch 417/1000, Training Loss: 0.0946, Validation Loss: 0.0946
  Epoch 418/1000, Training Loss: 0.0946, Validation Loss: 0.0946
  Epoch 419/1000, Training Loss: 0.0946, Validation Loss: 0.0946
  Epoch 420/1000, Training Loss: 0.0945, Validation Loss: 0.0945
  Epoch 421/1000, Training Loss: 0.0945, Validation Loss: 0.0945
  Epoch 422/1000, Training Loss: 0.0945, Validation Loss: 0.0945
  Epoch 423/1000, Training Loss: 0.0945, Validation Loss: 0.0945
  Epoch 424/1000, Training Loss: 0.0944, Validation Loss: 0.0944
  Epoch 425/1000, Training Loss: 0.0944, Validation Loss: 0.0944
  Epoch 426/1000, Training Loss: 0.0944, Validation Loss: 0.0944
  Epoch 427/1000, Training Loss: 0.0944, Validation Loss: 0.0944
  Epoch 428/1000, Training Loss: 0.0944, Validation Loss: 0.0944
  Epoch 429/1000, Training Loss: 0.0943, Validation Loss: 0.0943
  Epoch 430/1000, Training Loss: 0.0943, Validation Loss: 0.0943
  Epoch 431/1000, Training Loss: 0.0943, Validation Loss: 0.0943
  Epoch 432/1000, Training Loss: 0.0943, Validation Loss: 0.0943
  Epoch 433/1000, Training Loss: 0.0942, Validation Loss: 0.0942
  Epoch 434/1000, Training Loss: 0.0942, Validation Loss: 0.0942
  Epoch 435/1000, Training Loss: 0.0942, Validation Loss: 0.0942
  Epoch 436/1000, Training Loss: 0.0942, Validation Loss: 0.0942
  Epoch 437/1000, Training Loss: 0.0942, Validation Loss: 0.0942
  Epoch 438/1000, Training Loss: 0.0941, Validation Loss: 0.0941
  Epoch 439/1000, Training Loss: 0.0941, Validation Loss: 0.0941
  Epoch 440/1000, Training Loss: 0.0941, Validation Loss: 0.0941
  Epoch 441/1000, Training Loss: 0.0941, Validation Loss: 0.0941
  Epoch 442/1000, Training Loss: 0.0941, Validation Loss: 0.0941
  Epoch 443/1000, Training Loss: 0.0940, Validation Loss: 0.0940
  Epoch 444/1000, Training Loss: 0.0940, Validation Loss: 0.0940
  Epoch 445/1000, Training Loss: 0.0940, Validation Loss: 0.0940
  Epoch 446/1000, Training Loss: 0.0940, Validation Loss: 0.0940
  Epoch 447/1000, Training Loss: 0.0940, Validation Loss: 0.0940
  Epoch 448/1000, Training Loss: 0.0939, Validation Loss: 0.0939
  Epoch 449/1000, Training Loss: 0.0939, Validation Loss: 0.0939
  Epoch 450/1000, Training Loss: 0.0939, Validation Loss: 0.0939
  Epoch 451/1000, Training Loss: 0.0939, Validation Loss: 0.0939
  Epoch 452/1000, Training Loss: 0.0939, Validation Loss: 0.0939
  Epoch 453/1000, Training Loss: 0.0938, Validation Loss: 0.0938
  Epoch 454/1000, Training Loss: 0.0938, Validation Loss: 0.0938
  Epoch 455/1000, Training Loss: 0.0938, Validation Loss: 0.0938
  Epoch 456/1000, Training Loss: 0.0938, Validation Loss: 0.0938
  Epoch 457/1000, Training Loss: 0.0938, Validation Loss: 0.0938
  Epoch 458/1000, Training Loss: 0.0937, Validation Loss: 0.0937
  Epoch 459/1000, Training Loss: 0.0937, Validation Loss: 0.0937
  Epoch 460/1000, Training Loss: 0.0937, Validation Loss: 0.0937
  Epoch 461/1000, Training Loss: 0.0937, Validation Loss: 0.0937
  Epoch 462/1000, Training Loss: 0.0937, Validation Loss: 0.0937
  Epoch 463/1000, Training Loss: 0.0937, Validation Loss: 0.0937
  Epoch 464/1000, Training Loss: 0.0936, Validation Loss: 0.0936
  Epoch 465/1000, Training Loss: 0.0936, Validation Loss: 0.0936
  Epoch 466/1000, Training Loss: 0.0936, Validation Loss: 0.0936
  Epoch 467/1000, Training Loss: 0.0936, Validation Loss: 0.0936
  Epoch 468/1000, Training Loss: 0.0936, Validation Loss: 0.0936
  Epoch 469/1000, Training Loss: 0.0936, Validation Loss: 0.0936
  Epoch 470/1000, Training Loss: 0.0935, Validation Loss: 0.0935
  Epoch 471/1000, Training Loss: 0.0935, Validation Loss: 0.0935
  Epoch 472/1000, Training Loss: 0.0935, Validation Loss: 0.0935
  Epoch 473/1000, Training Loss: 0.0935, Validation Loss: 0.0935
  Epoch 474/1000, Training Loss: 0.0935, Validation Loss: 0.0935
  Epoch 475/1000, Training Loss: 0.0934, Validation Loss: 0.0934
  Epoch 476/1000, Training Loss: 0.0934, Validation Loss: 0.0934
  Epoch 477/1000, Training Loss: 0.0934, Validation Loss: 0.0934
  Epoch 478/1000, Training Loss: 0.0934, Validation Loss: 0.0934
  Epoch 479/1000, Training Loss: 0.0934, Validation Loss: 0.0934
  Epoch 480/1000, Training Loss: 0.0934, Validation Loss: 0.0934
  Epoch 481/1000, Training Loss: 0.0933, Validation Loss: 0.0933
  Epoch 482/1000, Training Loss: 0.0933, Validation Loss: 0.0933
  Epoch 483/1000, Training Loss: 0.0933, Validation Loss: 0.0933
  Epoch 484/1000, Training Loss: 0.0933, Validation Loss: 0.0933
  Epoch 485/1000, Training Loss: 0.0933, Validation Loss: 0.0933
  Epoch 486/1000, Training Loss: 0.0933, Validation Loss: 0.0933
  Epoch 487/1000, Training Loss: 0.0932, Validation Loss: 0.0932
  Epoch 488/1000, Training Loss: 0.0932, Validation Loss: 0.0932
  Epoch 489/1000, Training Loss: 0.0932, Validation Loss: 0.0932
  Epoch 490/1000, Training Loss: 0.0932, Validation Loss: 0.0932
  Epoch 491/1000, Training Loss: 0.0932, Validation Loss: 0.0932
  Epoch 492/1000, Training Loss: 0.0932, Validation Loss: 0.0932
  Epoch 493/1000, Training Loss: 0.0931, Validation Loss: 0.0931
  Epoch 494/1000, Training Loss: 0.0931, Validation Loss: 0.0931
  Epoch 495/1000, Training Loss: 0.0931, Validation Loss: 0.0931
  Epoch 496/1000, Training Loss: 0.0931, Validation Loss: 0.0931
  Epoch 497/1000, Training Loss: 0.0931, Validation Loss: 0.0931
  Epoch 498/1000, Training Loss: 0.0931, Validation Loss: 0.0931
  Epoch 499/1000, Training Loss: 0.0930, Validation Loss: 0.0930
  Epoch 500/1000, Training Loss: 0.0930, Validation Loss: 0.0930
  Epoch 501/1000, Training Loss: 0.0930, Validation Loss: 0.0930
  Epoch 502/1000, Training Loss: 0.0930, Validation Loss: 0.0930
  Epoch 503/1000, Training Loss: 0.0930, Validation Loss: 0.0930
  Epoch 504/1000, Training Loss: 0.0930, Validation Loss: 0.0930
  Epoch 505/1000, Training Loss: 0.0929, Validation Loss: 0.0929
  Epoch 506/1000, Training Loss: 0.0929, Validation Loss: 0.0929
  Epoch 507/1000, Training Loss: 0.0929, Validation Loss: 0.0929
  Epoch 508/1000, Training Loss: 0.0929, Validation Loss: 0.0929
  Epoch 509/1000, Training Loss: 0.0929, Validation Loss: 0.0929
  Epoch 510/1000, Training Loss: 0.0929, Validation Loss: 0.0929
  Epoch 511/1000, Training Loss: 0.0928, Validation Loss: 0.0928
  Epoch 512/1000, Training Loss: 0.0928, Validation Loss: 0.0928
  Epoch 513/1000, Training Loss: 0.0928, Validation Loss: 0.0928
  Epoch 514/1000, Training Loss: 0.0928, Validation Loss: 0.0928
  Epoch 515/1000, Training Loss: 0.0928, Validation Loss: 0.0928
  Epoch 516/1000, Training Loss: 0.0928, Validation Loss: 0.0928
  Epoch 517/1000, Training Loss: 0.0927, Validation Loss: 0.0927
  Epoch 518/1000, Training Loss: 0.0927, Validation Loss: 0.0927
  Epoch 519/1000, Training Loss: 0.0927, Validation Loss: 0.0927
  Epoch 520/1000, Training Loss: 0.0927, Validation Loss: 0.0927
  Epoch 521/1000, Training Loss: 0.0927, Validation Loss: 0.0927
  Epoch 522/1000, Training Loss: 0.0927, Validation Loss: 0.0927
  Epoch 523/1000, Training Loss: 0.0926, Validation Loss: 0.0926
  Epoch 524/1000, Training Loss: 0.0926, Validation Loss: 0.0926
  Epoch 525/1000, Training Loss: 0.0926, Validation Loss: 0.0926
  Epoch 526/1000, Training Loss: 0.0926, Validation Loss: 0.0926
  Epoch 527/1000, Training Loss: 0.0926, Validation Loss: 0.0926
  Epoch 528/1000, Training Loss: 0.0926, Validation Loss: 0.0926
  Epoch 529/1000, Training Loss: 0.0925, Validation Loss: 0.0925
  Epoch 530/1000, Training Loss: 0.0925, Validation Loss: 0.0925
  Epoch 531/1000, Training Loss: 0.0925, Validation Loss: 0.0925
  Epoch 532/1000, Training Loss: 0.0925, Validation Loss: 0.0925
  Epoch 533/1000, Training Loss: 0.0925, Validation Loss: 0.0925
  Epoch 534/1000, Training Loss: 0.0925, Validation Loss: 0.0925
  Epoch 535/1000, Training Loss: 0.0925, Validation Loss: 0.0925
  Epoch 536/1000, Training Loss: 0.0924, Validation Loss: 0.0924
  Epoch 537/1000, Training Loss: 0.0924, Validation Loss: 0.0924
  Epoch 538/1000, Training Loss: 0.0924, Validation Loss: 0.0924
  Epoch 539/1000, Training Loss: 0.0924, Validation Loss: 0.0924
  Epoch 540/1000, Training Loss: 0.0924, Validation Loss: 0.0924
  Epoch 541/1000, Training Loss: 0.0924, Validation Loss: 0.0924
  Epoch 542/1000, Training Loss: 0.0923, Validation Loss: 0.0923
  Epoch 543/1000, Training Loss: 0.0923, Validation Loss: 0.0923
  Epoch 544/1000, Training Loss: 0.0923, Validation Loss: 0.0923
  Epoch 545/1000, Training Loss: 0.0923, Validation Loss: 0.0923
  Epoch 546/1000, Training Loss: 0.0923, Validation Loss: 0.0923
  Epoch 547/1000, Training Loss: 0.0923, Validation Loss: 0.0923
  Epoch 548/1000, Training Loss: 0.0922, Validation Loss: 0.0922
  Epoch 549/1000, Training Loss: 0.0922, Validation Loss: 0.0922
  Epoch 550/1000, Training Loss: 0.0922, Validation Loss: 0.0922
  Epoch 551/1000, Training Loss: 0.0922, Validation Loss: 0.0922
  Epoch 552/1000, Training Loss: 0.0922, Validation Loss: 0.0922
  Epoch 553/1000, Training Loss: 0.0922, Validation Loss: 0.0922
  Epoch 554/1000, Training Loss: 0.0921, Validation Loss: 0.0921
  Epoch 555/1000, Training Loss: 0.0921, Validation Loss: 0.0921
  Epoch 556/1000, Training Loss: 0.0921, Validation Loss: 0.0921
  Epoch 557/1000, Training Loss: 0.0921, Validation Loss: 0.0921
  Epoch 558/1000, Training Loss: 0.0921, Validation Loss: 0.0921
  Epoch 559/1000, Training Loss: 0.0921, Validation Loss: 0.0921
  Epoch 560/1000, Training Loss: 0.0920, Validation Loss: 0.0920
  Epoch 561/1000, Training Loss: 0.0920, Validation Loss: 0.0920
  Epoch 562/1000, Training Loss: 0.0920, Validation Loss: 0.0920
  Epoch 563/1000, Training Loss: 0.0920, Validation Loss: 0.0920
  Epoch 564/1000, Training Loss: 0.0920, Validation Loss: 0.0920
  Epoch 565/1000, Training Loss: 0.0920, Validation Loss: 0.0920
  Epoch 566/1000, Training Loss: 0.0920, Validation Loss: 0.0920
  Epoch 567/1000, Training Loss: 0.0919, Validation Loss: 0.0919
  Epoch 568/1000, Training Loss: 0.0919, Validation Loss: 0.0919
  Epoch 569/1000, Training Loss: 0.0919, Validation Loss: 0.0919
  Epoch 570/1000, Training Loss: 0.0919, Validation Loss: 0.0919
  Epoch 571/1000, Training Loss: 0.0919, Validation Loss: 0.0919
  Epoch 572/1000, Training Loss: 0.0919, Validation Loss: 0.0919
  Epoch 573/1000, Training Loss: 0.0918, Validation Loss: 0.0918
  Epoch 574/1000, Training Loss: 0.0918, Validation Loss: 0.0918
  Epoch 575/1000, Training Loss: 0.0918, Validation Loss: 0.0918
  Epoch 576/1000, Training Loss: 0.0918, Validation Loss: 0.0918
  Epoch 577/1000, Training Loss: 0.0918, Validation Loss: 0.0918
  Epoch 578/1000, Training Loss: 0.0918, Validation Loss: 0.0918
  Epoch 579/1000, Training Loss: 0.0918, Validation Loss: 0.0918
  Epoch 580/1000, Training Loss: 0.0917, Validation Loss: 0.0917
  Epoch 581/1000, Training Loss: 0.0917, Validation Loss: 0.0917
  Epoch 582/1000, Training Loss: 0.0917, Validation Loss: 0.0917
  Epoch 583/1000, Training Loss: 0.0917, Validation Loss: 0.0917
  Epoch 584/1000, Training Loss: 0.0917, Validation Loss: 0.0917
  Epoch 585/1000, Training Loss: 0.0917, Validation Loss: 0.0917
  Epoch 586/1000, Training Loss: 0.0917, Validation Loss: 0.0917
  Epoch 587/1000, Training Loss: 0.0916, Validation Loss: 0.0916
  Epoch 588/1000, Training Loss: 0.0916, Validation Loss: 0.0916
  Epoch 589/1000, Training Loss: 0.0916, Validation Loss: 0.0916
  Epoch 590/1000, Training Loss: 0.0916, Validation Loss: 0.0916
  Epoch 591/1000, Training Loss: 0.0916, Validation Loss: 0.0916
  Epoch 592/1000, Training Loss: 0.0916, Validation Loss: 0.0916
  Epoch 593/1000, Training Loss: 0.0916, Validation Loss: 0.0916
  Epoch 594/1000, Training Loss: 0.0915, Validation Loss: 0.0915
  Epoch 595/1000, Training Loss: 0.0915, Validation Loss: 0.0915
  Epoch 596/1000, Training Loss: 0.0915, Validation Loss: 0.0915
  Epoch 597/1000, Training Loss: 0.0915, Validation Loss: 0.0915
  Epoch 598/1000, Training Loss: 0.0915, Validation Loss: 0.0915
  Epoch 599/1000, Training Loss: 0.0915, Validation Loss: 0.0915
  Epoch 600/1000, Training Loss: 0.0915, Validation Loss: 0.0915
  Epoch 601/1000, Training Loss: 0.0915, Validation Loss: 0.0915
  Epoch 602/1000, Training Loss: 0.0914, Validation Loss: 0.0914
  Epoch 603/1000, Training Loss: 0.0914, Validation Loss: 0.0914
  Epoch 604/1000, Training Loss: 0.0914, Validation Loss: 0.0914
  Epoch 605/1000, Training Loss: 0.0914, Validation Loss: 0.0914
  Epoch 606/1000, Training Loss: 0.0914, Validation Loss: 0.0914
  Epoch 607/1000, Training Loss: 0.0914, Validation Loss: 0.0914
  Epoch 608/1000, Training Loss: 0.0914, Validation Loss: 0.0914
  Epoch 609/1000, Training Loss: 0.0913, Validation Loss: 0.0913
  Epoch 610/1000, Training Loss: 0.0913, Validation Loss: 0.0913
  Epoch 611/1000, Training Loss: 0.0913, Validation Loss: 0.0913
  Epoch 612/1000, Training Loss: 0.0913, Validation Loss: 0.0913
  Epoch 613/1000, Training Loss: 0.0913, Validation Loss: 0.0913
  Epoch 614/1000, Training Loss: 0.0913, Validation Loss: 0.0913
  Epoch 615/1000, Training Loss: 0.0913, Validation Loss: 0.0913
  Epoch 616/1000, Training Loss: 0.0912, Validation Loss: 0.0912
  Epoch 617/1000, Training Loss: 0.0912, Validation Loss: 0.0912
  Epoch 618/1000, Training Loss: 0.0912, Validation Loss: 0.0912
  Epoch 619/1000, Training Loss: 0.0912, Validation Loss: 0.0912
  Epoch 620/1000, Training Loss: 0.0912, Validation Loss: 0.0912
  Epoch 621/1000, Training Loss: 0.0912, Validation Loss: 0.0912
  Epoch 622/1000, Training Loss: 0.0912, Validation Loss: 0.0912
  Epoch 623/1000, Training Loss: 0.0911, Validation Loss: 0.0911
  Epoch 624/1000, Training Loss: 0.0911, Validation Loss: 0.0911
  Epoch 625/1000, Training Loss: 0.0911, Validation Loss: 0.0911
  Epoch 626/1000, Training Loss: 0.0911, Validation Loss: 0.0911
  Epoch 627/1000, Training Loss: 0.0911, Validation Loss: 0.0911
  Epoch 628/1000, Training Loss: 0.0911, Validation Loss: 0.0911
  Epoch 629/1000, Training Loss: 0.0911, Validation Loss: 0.0911
  Epoch 630/1000, Training Loss: 0.0911, Validation Loss: 0.0911
  Epoch 631/1000, Training Loss: 0.0911, Validation Loss: 0.0911
  Epoch 632/1000, Training Loss: 0.0910, Validation Loss: 0.0910
  Epoch 633/1000, Training Loss: 0.0910, Validation Loss: 0.0910
  Epoch 634/1000, Training Loss: 0.0910, Validation Loss: 0.0910
  Epoch 635/1000, Training Loss: 0.0910, Validation Loss: 0.0910
  Epoch 636/1000, Training Loss: 0.0910, Validation Loss: 0.0910
  Epoch 637/1000, Training Loss: 0.0910, Validation Loss: 0.0910
  Epoch 638/1000, Training Loss: 0.0910, Validation Loss: 0.0910
  Epoch 639/1000, Training Loss: 0.0910, Validation Loss: 0.0910
  Epoch 640/1000, Training Loss: 0.0909, Validation Loss: 0.0909
  Epoch 641/1000, Training Loss: 0.0909, Validation Loss: 0.0909
  Epoch 642/1000, Training Loss: 0.0909, Validation Loss: 0.0909
  Epoch 643/1000, Training Loss: 0.0909, Validation Loss: 0.0909
  Epoch 644/1000, Training Loss: 0.0909, Validation Loss: 0.0909
  Epoch 645/1000, Training Loss: 0.0909, Validation Loss: 0.0909
  Epoch 646/1000, Training Loss: 0.0909, Validation Loss: 0.0909
  Epoch 647/1000, Training Loss: 0.0908, Validation Loss: 0.0908
  Epoch 648/1000, Training Loss: 0.0908, Validation Loss: 0.0908
  Epoch 649/1000, Training Loss: 0.0908, Validation Loss: 0.0908
  Epoch 650/1000, Training Loss: 0.0908, Validation Loss: 0.0908
  Epoch 651/1000, Training Loss: 0.0908, Validation Loss: 0.0908
  Epoch 652/1000, Training Loss: 0.0908, Validation Loss: 0.0908
  Epoch 653/1000, Training Loss: 0.0908, Validation Loss: 0.0908
  Epoch 654/1000, Training Loss: 0.0908, Validation Loss: 0.0908
  Epoch 655/1000, Training Loss: 0.0907, Validation Loss: 0.0907
  Epoch 656/1000, Training Loss: 0.0907, Validation Loss: 0.0907
  Epoch 657/1000, Training Loss: 0.0907, Validation Loss: 0.0907
  Epoch 658/1000, Training Loss: 0.0907, Validation Loss: 0.0907
  Epoch 659/1000, Training Loss: 0.0907, Validation Loss: 0.0907
  Epoch 660/1000, Training Loss: 0.0907, Validation Loss: 0.0907
  Epoch 661/1000, Training Loss: 0.0907, Validation Loss: 0.0907
  Epoch 662/1000, Training Loss: 0.0906, Validation Loss: 0.0906
  Epoch 663/1000, Training Loss: 0.0906, Validation Loss: 0.0906
  Epoch 664/1000, Training Loss: 0.0906, Validation Loss: 0.0906
  Epoch 665/1000, Training Loss: 0.0906, Validation Loss: 0.0906
  Epoch 666/1000, Training Loss: 0.0906, Validation Loss: 0.0906
  Epoch 667/1000, Training Loss: 0.0906, Validation Loss: 0.0906
  Epoch 668/1000, Training Loss: 0.0906, Validation Loss: 0.0906
  Epoch 669/1000, Training Loss: 0.0905, Validation Loss: 0.0905
  Epoch 670/1000, Training Loss: 0.0905, Validation Loss: 0.0905
  Epoch 671/1000, Training Loss: 0.0905, Validation Loss: 0.0905
  Epoch 672/1000, Training Loss: 0.0905, Validation Loss: 0.0905
  Epoch 673/1000, Training Loss: 0.0905, Validation Loss: 0.0905
  Epoch 674/1000, Training Loss: 0.0905, Validation Loss: 0.0905
  Epoch 675/1000, Training Loss: 0.0905, Validation Loss: 0.0905
  Epoch 676/1000, Training Loss: 0.0904, Validation Loss: 0.0904
  Epoch 677/1000, Training Loss: 0.0904, Validation Loss: 0.0904
  Epoch 678/1000, Training Loss: 0.0904, Validation Loss: 0.0904
  Epoch 679/1000, Training Loss: 0.0904, Validation Loss: 0.0904
  Epoch 680/1000, Training Loss: 0.0904, Validation Loss: 0.0904
  Epoch 681/1000, Training Loss: 0.0904, Validation Loss: 0.0904
  Epoch 682/1000, Training Loss: 0.0904, Validation Loss: 0.0904
  Epoch 683/1000, Training Loss: 0.0903, Validation Loss: 0.0903
  Epoch 684/1000, Training Loss: 0.0903, Validation Loss: 0.0903
  Epoch 685/1000, Training Loss: 0.0903, Validation Loss: 0.0903
  Epoch 686/1000, Training Loss: 0.0903, Validation Loss: 0.0903
  Epoch 687/1000, Training Loss: 0.0903, Validation Loss: 0.0903
  Epoch 688/1000, Training Loss: 0.0903, Validation Loss: 0.0903
  Epoch 689/1000, Training Loss: 0.0903, Validation Loss: 0.0903
  Epoch 690/1000, Training Loss: 0.0902, Validation Loss: 0.0902
  Epoch 691/1000, Training Loss: 0.0902, Validation Loss: 0.0902
  Epoch 692/1000, Training Loss: 0.0902, Validation Loss: 0.0902
  Epoch 693/1000, Training Loss: 0.0902, Validation Loss: 0.0902
  Epoch 694/1000, Training Loss: 0.0902, Validation Loss: 0.0902
  Epoch 695/1000, Training Loss: 0.0902, Validation Loss: 0.0902
  Epoch 696/1000, Training Loss: 0.0902, Validation Loss: 0.0902
  Epoch 697/1000, Training Loss: 0.0902, Validation Loss: 0.0902
  Epoch 698/1000, Training Loss: 0.0902, Validation Loss: 0.0902
  Epoch 699/1000, Training Loss: 0.0901, Validation Loss: 0.0901
  Epoch 700/1000, Training Loss: 0.0901, Validation Loss: 0.0901
  Epoch 701/1000, Training Loss: 0.0901, Validation Loss: 0.0901
  Epoch 702/1000, Training Loss: 0.0901, Validation Loss: 0.0901
  Epoch 703/1000, Training Loss: 0.0901, Validation Loss: 0.0901
  Epoch 704/1000, Training Loss: 0.0901, Validation Loss: 0.0901
  Epoch 705/1000, Training Loss: 0.0901, Validation Loss: 0.0901
  Epoch 706/1000, Training Loss: 0.0900, Validation Loss: 0.0900
  Epoch 707/1000, Training Loss: 0.0900, Validation Loss: 0.0900
  Epoch 708/1000, Training Loss: 0.0900, Validation Loss: 0.0900
  Epoch 709/1000, Training Loss: 0.0900, Validation Loss: 0.0900
  Epoch 710/1000, Training Loss: 0.0900, Validation Loss: 0.0900
  Epoch 711/1000, Training Loss: 0.0900, Validation Loss: 0.0900
  Epoch 712/1000, Training Loss: 0.0900, Validation Loss: 0.0900
  Epoch 713/1000, Training Loss: 0.0900, Validation Loss: 0.0900
  Epoch 714/1000, Training Loss: 0.0900, Validation Loss: 0.0900
  Epoch 715/1000, Training Loss: 0.0899, Validation Loss: 0.0899
  Epoch 716/1000, Training Loss: 0.0899, Validation Loss: 0.0899
  Epoch 717/1000, Training Loss: 0.0899, Validation Loss: 0.0899
  Epoch 718/1000, Training Loss: 0.0899, Validation Loss: 0.0899
  Epoch 719/1000, Training Loss: 0.0899, Validation Loss: 0.0899
  Epoch 720/1000, Training Loss: 0.0899, Validation Loss: 0.0899
  Epoch 721/1000, Training Loss: 0.0899, Validation Loss: 0.0899
  Epoch 722/1000, Training Loss: 0.0899, Validation Loss: 0.0899
  Epoch 723/1000, Training Loss: 0.0898, Validation Loss: 0.0898
  Epoch 724/1000, Training Loss: 0.0898, Validation Loss: 0.0898
  Epoch 725/1000, Training Loss: 0.0898, Validation Loss: 0.0898
  Epoch 726/1000, Training Loss: 0.0898, Validation Loss: 0.0898
  Epoch 727/1000, Training Loss: 0.0898, Validation Loss: 0.0898
  Epoch 728/1000, Training Loss: 0.0898, Validation Loss: 0.0898
  Epoch 729/1000, Training Loss: 0.0898, Validation Loss: 0.0898
  Epoch 730/1000, Training Loss: 0.0898, Validation Loss: 0.0898
  Epoch 731/1000, Training Loss: 0.0898, Validation Loss: 0.0898
  Epoch 732/1000, Training Loss: 0.0897, Validation Loss: 0.0897
  Epoch 733/1000, Training Loss: 0.0897, Validation Loss: 0.0897
  Epoch 734/1000, Training Loss: 0.0897, Validation Loss: 0.0897
  Epoch 735/1000, Training Loss: 0.0897, Validation Loss: 0.0897
  Epoch 736/1000, Training Loss: 0.0897, Validation Loss: 0.0897
  Epoch 737/1000, Training Loss: 0.0897, Validation Loss: 0.0897
  Epoch 738/1000, Training Loss: 0.0897, Validation Loss: 0.0897
  Epoch 739/1000, Training Loss: 0.0897, Validation Loss: 0.0897
  Epoch 740/1000, Training Loss: 0.0897, Validation Loss: 0.0897
  Epoch 741/1000, Training Loss: 0.0896, Validation Loss: 0.0896
  Epoch 742/1000, Training Loss: 0.0896, Validation Loss: 0.0896
  Epoch 743/1000, Training Loss: 0.0896, Validation Loss: 0.0896
  Epoch 744/1000, Training Loss: 0.0896, Validation Loss: 0.0896
  Epoch 745/1000, Training Loss: 0.0896, Validation Loss: 0.0896
  Epoch 746/1000, Training Loss: 0.0896, Validation Loss: 0.0896
  Epoch 747/1000, Training Loss: 0.0896, Validation Loss: 0.0896
  Epoch 748/1000, Training Loss: 0.0896, Validation Loss: 0.0896
  Epoch 749/1000, Training Loss: 0.0896, Validation Loss: 0.0896
  Epoch 750/1000, Training Loss: 0.0896, Validation Loss: 0.0896
  Epoch 751/1000, Training Loss: 0.0895, Validation Loss: 0.0895
  Epoch 752/1000, Training Loss: 0.0895, Validation Loss: 0.0895
  Epoch 753/1000, Training Loss: 0.0895, Validation Loss: 0.0895
  Epoch 754/1000, Training Loss: 0.0895, Validation Loss: 0.0895
  Epoch 755/1000, Training Loss: 0.0895, Validation Loss: 0.0895
  Epoch 756/1000, Training Loss: 0.0895, Validation Loss: 0.0895
  Epoch 757/1000, Training Loss: 0.0895, Validation Loss: 0.0895
  Epoch 758/1000, Training Loss: 0.0895, Validation Loss: 0.0895
  Epoch 759/1000, Training Loss: 0.0895, Validation Loss: 0.0895
  Epoch 760/1000, Training Loss: 0.0894, Validation Loss: 0.0894
  Epoch 761/1000, Training Loss: 0.0894, Validation Loss: 0.0894
  Epoch 762/1000, Training Loss: 0.0894, Validation Loss: 0.0894
  Epoch 763/1000, Training Loss: 0.0894, Validation Loss: 0.0894
  Epoch 764/1000, Training Loss: 0.0894, Validation Loss: 0.0894
  Epoch 765/1000, Training Loss: 0.0894, Validation Loss: 0.0894
  Epoch 766/1000, Training Loss: 0.0894, Validation Loss: 0.0894
  Epoch 767/1000, Training Loss: 0.0894, Validation Loss: 0.0894
  Epoch 768/1000, Training Loss: 0.0893, Validation Loss: 0.0893
  Epoch 769/1000, Training Loss: 0.0893, Validation Loss: 0.0893
  Epoch 770/1000, Training Loss: 0.0893, Validation Loss: 0.0893
  Epoch 771/1000, Training Loss: 0.0893, Validation Loss: 0.0893
  Epoch 772/1000, Training Loss: 0.0893, Validation Loss: 0.0893
  Epoch 773/1000, Training Loss: 0.0893, Validation Loss: 0.0893
  Epoch 774/1000, Training Loss: 0.0893, Validation Loss: 0.0893
  Epoch 775/1000, Training Loss: 0.0893, Validation Loss: 0.0893
  Epoch 776/1000, Training Loss: 0.0893, Validation Loss: 0.0893
  Epoch 777/1000, Training Loss: 0.0893, Validation Loss: 0.0893
  Epoch 778/1000, Training Loss: 0.0892, Validation Loss: 0.0892
  Epoch 779/1000, Training Loss: 0.0892, Validation Loss: 0.0892
  Epoch 780/1000, Training Loss: 0.0892, Validation Loss: 0.0892
  Epoch 781/1000, Training Loss: 0.0892, Validation Loss: 0.0892
  Epoch 782/1000, Training Loss: 0.0892, Validation Loss: 0.0892
  Epoch 783/1000, Training Loss: 0.0892, Validation Loss: 0.0892
  Epoch 784/1000, Training Loss: 0.0892, Validation Loss: 0.0892
  Epoch 785/1000, Training Loss: 0.0892, Validation Loss: 0.0892
  Epoch 786/1000, Training Loss: 0.0892, Validation Loss: 0.0892
  Epoch 787/1000, Training Loss: 0.0891, Validation Loss: 0.0891
  Epoch 788/1000, Training Loss: 0.0891, Validation Loss: 0.0891
  Epoch 789/1000, Training Loss: 0.0891, Validation Loss: 0.0891
  Epoch 790/1000, Training Loss: 0.0891, Validation Loss: 0.0891
  Epoch 791/1000, Training Loss: 0.0891, Validation Loss: 0.0891
  Epoch 792/1000, Training Loss: 0.0891, Validation Loss: 0.0891
  Epoch 793/1000, Training Loss: 0.0891, Validation Loss: 0.0891
  Epoch 794/1000, Training Loss: 0.0891, Validation Loss: 0.0891
  Epoch 795/1000, Training Loss: 0.0891, Validation Loss: 0.0891
  Epoch 796/1000, Training Loss: 0.0890, Validation Loss: 0.0890
  Epoch 797/1000, Training Loss: 0.0890, Validation Loss: 0.0890
  Epoch 798/1000, Training Loss: 0.0890, Validation Loss: 0.0890
  Epoch 799/1000, Training Loss: 0.0890, Validation Loss: 0.0890
  Epoch 800/1000, Training Loss: 0.0890, Validation Loss: 0.0890
  Epoch 801/1000, Training Loss: 0.0890, Validation Loss: 0.0890
  Epoch 802/1000, Training Loss: 0.0890, Validation Loss: 0.0890
  Epoch 803/1000, Training Loss: 0.0890, Validation Loss: 0.0890
  Epoch 804/1000, Training Loss: 0.0890, Validation Loss: 0.0890
  Epoch 805/1000, Training Loss: 0.0890, Validation Loss: 0.0890
  Epoch 806/1000, Training Loss: 0.0890, Validation Loss: 0.0890
  Epoch 807/1000, Training Loss: 0.0889, Validation Loss: 0.0889
  Epoch 808/1000, Training Loss: 0.0889, Validation Loss: 0.0889
  Epoch 809/1000, Training Loss: 0.0889, Validation Loss: 0.0889
  Epoch 810/1000, Training Loss: 0.0889, Validation Loss: 0.0889
  Epoch 811/1000, Training Loss: 0.0889, Validation Loss: 0.0889
  Epoch 812/1000, Training Loss: 0.0889, Validation Loss: 0.0889
  Epoch 813/1000, Training Loss: 0.0889, Validation Loss: 0.0889
  Epoch 814/1000, Training Loss: 0.0889, Validation Loss: 0.0889
  Epoch 815/1000, Training Loss: 0.0889, Validation Loss: 0.0889
  Epoch 816/1000, Training Loss: 0.0888, Validation Loss: 0.0888
  Epoch 817/1000, Training Loss: 0.0888, Validation Loss: 0.0888
  Epoch 818/1000, Training Loss: 0.0888, Validation Loss: 0.0888
  Epoch 819/1000, Training Loss: 0.0888, Validation Loss: 0.0888
  Epoch 820/1000, Training Loss: 0.0888, Validation Loss: 0.0888
  Epoch 821/1000, Training Loss: 0.0888, Validation Loss: 0.0888
  Epoch 822/1000, Training Loss: 0.0888, Validation Loss: 0.0888
  Epoch 823/1000, Training Loss: 0.0888, Validation Loss: 0.0888
  Epoch 824/1000, Training Loss: 0.0888, Validation Loss: 0.0888
  Epoch 825/1000, Training Loss: 0.0888, Validation Loss: 0.0888
  Epoch 826/1000, Training Loss: 0.0887, Validation Loss: 0.0887
  Epoch 827/1000, Training Loss: 0.0887, Validation Loss: 0.0887
  Epoch 828/1000, Training Loss: 0.0887, Validation Loss: 0.0887
  Epoch 829/1000, Training Loss: 0.0887, Validation Loss: 0.0887
  Epoch 830/1000, Training Loss: 0.0887, Validation Loss: 0.0887
  Epoch 831/1000, Training Loss: 0.0887, Validation Loss: 0.0887
  Epoch 832/1000, Training Loss: 0.0887, Validation Loss: 0.0887
  Epoch 833/1000, Training Loss: 0.0887, Validation Loss: 0.0887
  Epoch 834/1000, Training Loss: 0.0887, Validation Loss: 0.0887
  Epoch 835/1000, Training Loss: 0.0887, Validation Loss: 0.0887
  Epoch 836/1000, Training Loss: 0.0887, Validation Loss: 0.0887
  Epoch 837/1000, Training Loss: 0.0886, Validation Loss: 0.0886
  Epoch 838/1000, Training Loss: 0.0886, Validation Loss: 0.0886
  Epoch 839/1000, Training Loss: 0.0886, Validation Loss: 0.0886
  Epoch 840/1000, Training Loss: 0.0886, Validation Loss: 0.0886
  Epoch 841/1000, Training Loss: 0.0886, Validation Loss: 0.0886
  Epoch 842/1000, Training Loss: 0.0886, Validation Loss: 0.0886
  Epoch 843/1000, Training Loss: 0.0886, Validation Loss: 0.0886
  Epoch 844/1000, Training Loss: 0.0886, Validation Loss: 0.0886
  Epoch 845/1000, Training Loss: 0.0886, Validation Loss: 0.0886
  Epoch 846/1000, Training Loss: 0.0886, Validation Loss: 0.0886
  Epoch 847/1000, Training Loss: 0.0886, Validation Loss: 0.0886
  Epoch 848/1000, Training Loss: 0.0885, Validation Loss: 0.0885
  Epoch 849/1000, Training Loss: 0.0885, Validation Loss: 0.0885
  Epoch 850/1000, Training Loss: 0.0885, Validation Loss: 0.0885
  Epoch 851/1000, Training Loss: 0.0885, Validation Loss: 0.0885
  Epoch 852/1000, Training Loss: 0.0885, Validation Loss: 0.0885
  Epoch 853/1000, Training Loss: 0.0885, Validation Loss: 0.0885
  Epoch 854/1000, Training Loss: 0.0885, Validation Loss: 0.0885
  Epoch 855/1000, Training Loss: 0.0885, Validation Loss: 0.0885
  Epoch 856/1000, Training Loss: 0.0885, Validation Loss: 0.0885
  Epoch 857/1000, Training Loss: 0.0885, Validation Loss: 0.0885
  Epoch 858/1000, Training Loss: 0.0885, Validation Loss: 0.0885
  Epoch 859/1000, Training Loss: 0.0885, Validation Loss: 0.0885
  Epoch 860/1000, Training Loss: 0.0884, Validation Loss: 0.0884
  Epoch 861/1000, Training Loss: 0.0884, Validation Loss: 0.0884
  Epoch 862/1000, Training Loss: 0.0884, Validation Loss: 0.0884
  Epoch 863/1000, Training Loss: 0.0884, Validation Loss: 0.0884
  Epoch 864/1000, Training Loss: 0.0884, Validation Loss: 0.0884
  Epoch 865/1000, Training Loss: 0.0884, Validation Loss: 0.0884
  Epoch 866/1000, Training Loss: 0.0884, Validation Loss: 0.0884
  Epoch 867/1000, Training Loss: 0.0884, Validation Loss: 0.0884
  Epoch 868/1000, Training Loss: 0.0884, Validation Loss: 0.0884
  Epoch 869/1000, Training Loss: 0.0884, Validation Loss: 0.0884
  Epoch 870/1000, Training Loss: 0.0884, Validation Loss: 0.0884
  Epoch 871/1000, Training Loss: 0.0883, Validation Loss: 0.0883
  Epoch 872/1000, Training Loss: 0.0883, Validation Loss: 0.0883
  Epoch 873/1000, Training Loss: 0.0883, Validation Loss: 0.0883
  Epoch 874/1000, Training Loss: 0.0883, Validation Loss: 0.0883
  Epoch 875/1000, Training Loss: 0.0883, Validation Loss: 0.0883
  Epoch 876/1000, Training Loss: 0.0883, Validation Loss: 0.0883
  Epoch 877/1000, Training Loss: 0.0883, Validation Loss: 0.0883
  Epoch 878/1000, Training Loss: 0.0883, Validation Loss: 0.0883
  Epoch 879/1000, Training Loss: 0.0883, Validation Loss: 0.0883
  Epoch 880/1000, Training Loss: 0.0883, Validation Loss: 0.0883
  Epoch 881/1000, Training Loss: 0.0883, Validation Loss: 0.0883
  Epoch 882/1000, Training Loss: 0.0883, Validation Loss: 0.0883
  Epoch 883/1000, Training Loss: 0.0882, Validation Loss: 0.0882
  Epoch 884/1000, Training Loss: 0.0882, Validation Loss: 0.0882
  Epoch 885/1000, Training Loss: 0.0882, Validation Loss: 0.0882
  Epoch 886/1000, Training Loss: 0.0882, Validation Loss: 0.0882
  Epoch 887/1000, Training Loss: 0.0882, Validation Loss: 0.0882
  Epoch 888/1000, Training Loss: 0.0882, Validation Loss: 0.0882
  Epoch 889/1000, Training Loss: 0.0882, Validation Loss: 0.0882
  Epoch 890/1000, Training Loss: 0.0882, Validation Loss: 0.0882
  Epoch 891/1000, Training Loss: 0.0882, Validation Loss: 0.0882
  Epoch 892/1000, Training Loss: 0.0882, Validation Loss: 0.0882
  Epoch 893/1000, Training Loss: 0.0882, Validation Loss: 0.0882
  Epoch 894/1000, Training Loss: 0.0882, Validation Loss: 0.0882
  Epoch 895/1000, Training Loss: 0.0881, Validation Loss: 0.0881
  Epoch 896/1000, Training Loss: 0.0881, Validation Loss: 0.0881
  Epoch 897/1000, Training Loss: 0.0881, Validation Loss: 0.0881
  Epoch 898/1000, Training Loss: 0.0881, Validation Loss: 0.0881
  Epoch 899/1000, Training Loss: 0.0881, Validation Loss: 0.0881
  Epoch 900/1000, Training Loss: 0.0881, Validation Loss: 0.0881
  Epoch 901/1000, Training Loss: 0.0881, Validation Loss: 0.0881
  Epoch 902/1000, Training Loss: 0.0881, Validation Loss: 0.0881
  Epoch 903/1000, Training Loss: 0.0881, Validation Loss: 0.0881
  Epoch 904/1000, Training Loss: 0.0881, Validation Loss: 0.0881
  Epoch 905/1000, Training Loss: 0.0881, Validation Loss: 0.0881
  Epoch 906/1000, Training Loss: 0.0881, Validation Loss: 0.0881
  Epoch 907/1000, Training Loss: 0.0880, Validation Loss: 0.0880
  Epoch 908/1000, Training Loss: 0.0880, Validation Loss: 0.0880
  Epoch 909/1000, Training Loss: 0.0880, Validation Loss: 0.0880
  Epoch 910/1000, Training Loss: 0.0880, Validation Loss: 0.0880
  Epoch 911/1000, Training Loss: 0.0880, Validation Loss: 0.0880
  Epoch 912/1000, Training Loss: 0.0880, Validation Loss: 0.0880
  Epoch 913/1000, Training Loss: 0.0880, Validation Loss: 0.0880
  Epoch 914/1000, Training Loss: 0.0880, Validation Loss: 0.0880
  Epoch 915/1000, Training Loss: 0.0880, Validation Loss: 0.0880
  Epoch 916/1000, Training Loss: 0.0880, Validation Loss: 0.0880
  Epoch 917/1000, Training Loss: 0.0880, Validation Loss: 0.0880
  Epoch 918/1000, Training Loss: 0.0880, Validation Loss: 0.0880
  Epoch 919/1000, Training Loss: 0.0880, Validation Loss: 0.0880
  Epoch 920/1000, Training Loss: 0.0879, Validation Loss: 0.0879
  Epoch 921/1000, Training Loss: 0.0879, Validation Loss: 0.0879
  Epoch 922/1000, Training Loss: 0.0879, Validation Loss: 0.0879
  Epoch 923/1000, Training Loss: 0.0879, Validation Loss: 0.0879
  Epoch 924/1000, Training Loss: 0.0879, Validation Loss: 0.0879
  Epoch 925/1000, Training Loss: 0.0879, Validation Loss: 0.0879
  Epoch 926/1000, Training Loss: 0.0879, Validation Loss: 0.0879
  Epoch 927/1000, Training Loss: 0.0879, Validation Loss: 0.0879
  Epoch 928/1000, Training Loss: 0.0879, Validation Loss: 0.0879
  Epoch 929/1000, Training Loss: 0.0879, Validation Loss: 0.0879
  Epoch 930/1000, Training Loss: 0.0879, Validation Loss: 0.0879
  Epoch 931/1000, Training Loss: 0.0879, Validation Loss: 0.0879
  Epoch 932/1000, Training Loss: 0.0879, Validation Loss: 0.0879
  Epoch 933/1000, Training Loss: 0.0878, Validation Loss: 0.0878
  Epoch 934/1000, Training Loss: 0.0878, Validation Loss: 0.0878
  Epoch 935/1000, Training Loss: 0.0878, Validation Loss: 0.0878
  Epoch 936/1000, Training Loss: 0.0878, Validation Loss: 0.0878
  Epoch 937/1000, Training Loss: 0.0878, Validation Loss: 0.0878
  Epoch 938/1000, Training Loss: 0.0878, Validation Loss: 0.0878
  Epoch 939/1000, Training Loss: 0.0878, Validation Loss: 0.0878
  Epoch 940/1000, Training Loss: 0.0878, Validation Loss: 0.0878
  Epoch 941/1000, Training Loss: 0.0878, Validation Loss: 0.0878
  Epoch 942/1000, Training Loss: 0.0878, Validation Loss: 0.0878
  Epoch 943/1000, Training Loss: 0.0878, Validation Loss: 0.0878
  Epoch 944/1000, Training Loss: 0.0878, Validation Loss: 0.0878
  Epoch 945/1000, Training Loss: 0.0878, Validation Loss: 0.0878
  Epoch 946/1000, Training Loss: 0.0878, Validation Loss: 0.0878
  Epoch 947/1000, Training Loss: 0.0877, Validation Loss: 0.0877
  Epoch 948/1000, Training Loss: 0.0877, Validation Loss: 0.0877
  Epoch 949/1000, Training Loss: 0.0877, Validation Loss: 0.0877
  Epoch 950/1000, Training Loss: 0.0877, Validation Loss: 0.0877
  Epoch 951/1000, Training Loss: 0.0877, Validation Loss: 0.0877
  Epoch 952/1000, Training Loss: 0.0877, Validation Loss: 0.0877
  Epoch 953/1000, Training Loss: 0.0877, Validation Loss: 0.0877
  Epoch 954/1000, Training Loss: 0.0877, Validation Loss: 0.0877
  Epoch 955/1000, Training Loss: 0.0877, Validation Loss: 0.0877
  Epoch 956/1000, Training Loss: 0.0877, Validation Loss: 0.0877
  Epoch 957/1000, Training Loss: 0.0877, Validation Loss: 0.0877
  Epoch 958/1000, Training Loss: 0.0877, Validation Loss: 0.0877
  Epoch 959/1000, Training Loss: 0.0877, Validation Loss: 0.0877
  Epoch 960/1000, Training Loss: 0.0876, Validation Loss: 0.0876
  Epoch 961/1000, Training Loss: 0.0877, Validation Loss: 0.0877
  Epoch 962/1000, Training Loss: 0.0876, Validation Loss: 0.0876
  Epoch 963/1000, Training Loss: 0.0876, Validation Loss: 0.0876
  Epoch 964/1000, Training Loss: 0.0876, Validation Loss: 0.0876
  Epoch 965/1000, Training Loss: 0.0876, Validation Loss: 0.0876
  Epoch 966/1000, Training Loss: 0.0876, Validation Loss: 0.0876
  Epoch 967/1000, Training Loss: 0.0876, Validation Loss: 0.0876
  Epoch 968/1000, Training Loss: 0.0876, Validation Loss: 0.0876
  Epoch 969/1000, Training Loss: 0.0876, Validation Loss: 0.0876
  Epoch 970/1000, Training Loss: 0.0876, Validation Loss: 0.0876
  Epoch 971/1000, Training Loss: 0.0876, Validation Loss: 0.0876
  Epoch 972/1000, Training Loss: 0.0876, Validation Loss: 0.0876
  Epoch 973/1000, Training Loss: 0.0875, Validation Loss: 0.0875
  Epoch 974/1000, Training Loss: 0.0875, Validation Loss: 0.0875
  Epoch 975/1000, Training Loss: 0.0875, Validation Loss: 0.0875
  Epoch 976/1000, Training Loss: 0.0875, Validation Loss: 0.0875
  Epoch 977/1000, Training Loss: 0.0875, Validation Loss: 0.0875
  Epoch 978/1000, Training Loss: 0.0875, Validation Loss: 0.0875
  Epoch 979/1000, Training Loss: 0.0875, Validation Loss: 0.0875
  Epoch 980/1000, Training Loss: 0.0875, Validation Loss: 0.0875
  Epoch 981/1000, Training Loss: 0.0875, Validation Loss: 0.0875
  Epoch 982/1000, Training Loss: 0.0875, Validation Loss: 0.0875
  Epoch 983/1000, Training Loss: 0.0875, Validation Loss: 0.0875
  Epoch 984/1000, Training Loss: 0.0875, Validation Loss: 0.0875
  Epoch 985/1000, Training Loss: 0.0875, Validation Loss: 0.0875
  Epoch 986/1000, Training Loss: 0.0875, Validation Loss: 0.0875
  Epoch 987/1000, Training Loss: 0.0875, Validation Loss: 0.0875
  Epoch 988/1000, Training Loss: 0.0875, Validation Loss: 0.0875
  Epoch 989/1000, Training Loss: 0.0874, Validation Loss: 0.0874
  Epoch 990/1000, Training Loss: 0.0874, Validation Loss: 0.0874
  Epoch 991/1000, Training Loss: 0.0874, Validation Loss: 0.0874
  Epoch 992/1000, Training Loss: 0.0874, Validation Loss: 0.0874
  Epoch 993/1000, Training Loss: 0.0875, Validation Loss: 0.0875
  Epoch 994/1000, Training Loss: 0.0874, Validation Loss: 0.0874
  Epoch 995/1000, Training Loss: 0.0874, Validation Loss: 0.0874
  Epoch 996/1000, Training Loss: 0.0874, Validation Loss: 0.0874
  Epoch 997/1000, Training Loss: 0.0874, Validation Loss: 0.0874
  Epoch 998/1000, Training Loss: 0.0874, Validation Loss: 0.0874
  Epoch 999/1000, Training Loss: 0.0874, Validation Loss: 0.0874
  Epoch 1000/1000, Training Loss: 0.0874, Validation Loss: 0.0874
#+end_example

*** See data

#+begin_src ipython
  model.eval()  # Set the model to evaluation mode

  input_size = X.shape[-1]  # Number of features
  sequence_length = 180

  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

  # Start with an initial seed sequence
  seed_sequence = torch.randn(1, sequence_length, input_size).to(device)  # Replace with your actual seed

  # Decide how many future steps you want to predict
  future_steps = 180

  # Collect predictions
  predictions = []

  # Initialize the hidden state (optional, depends on your model architecture)
  hidden = torch.zeros(num_layers, 1, hidden_size).to(device)

  # Generate time series
  for _ in range(future_steps):
      # Forward pass
      with torch.no_grad():  # No need to track gradients
          out, hidden = model.rnn(seed_sequence, hidden)
          next_step = model.fc(out[:, -1, :])  # Output for the last time step

      predictions.append(next_step.cpu().numpy())

      # Use the predicted next step as the input for the next iteration
      next_step = next_step.unsqueeze(1)  # Add the sequence length dimension
      seed_sequence = torch.cat((seed_sequence[:, 1:, :], next_step), 1)  # Move the window

  # # Convert predictions to a numpy array for further analysis
  predicted_time_series = np.concatenate(predictions, axis=0)

#+end_src

#+RESULTS:

#+begin_src ipython
  import numpy as np
  import matplotlib.pyplot as plt

  # Assuming 'predicted_time_series' is a numpy array containing your generated data
  # Each column in 'predicted_time_series' corresponds to a different feature in the time series

  # Plot each feature of the time series
  num_features = predicted_time_series.shape[1]
  plt.figure(figsize=(12, 6))
  for i in range(5):
      plt.plot(predicted_time_series[:, i], lw=5)
      plt.plot(X.cpu().numpy()[0, :, i], alpha=.2)

  plt.xlabel('Time')
  plt.ylabel('Feature Value')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/3ef750e215650126afe0e85fca627e35e06b2033.png]]

* Data
** imports

#+begin_src ipython
  import sys
  sys.path.insert(0, '../')
  
  from dual_data.common.get_data import get_X_y_days, get_X_y_S1_S2
  from dual_data.common.options import set_options
#+end_src

#+RESULTS:

** parameters

#+begin_src ipython
  mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
  tasks = ['DPA', 'DualGo', 'DualNoGo']
  days = ['first', 'last']

  kwargs = dict()
  kwargs = {'prescreen': 'fpr', 'pval': 0.05, 'trials': '', 'balance': 'under',
            'method': 'bootstrap', 'bolasso_pval':0.05, 'bolasso_penalty': 'l2',
            'bootstrap': True, 'n_boots': 1000,
            'preprocess': True, 'scaler_BL': None, 'avg_noise':True, 'unit_var_BL':False,
            'clf':'log_loss', 'scaler': None, 'tol':0.001, 'penalty':'l2',
            'out_fold': 'stratified', 'n_out': 5,
            'in_fold': 'stratified', 'n_in': 5,
            'random_state': None, 'n_repeats': 10,
            'n_lambda': 20, 'T_WINDOW': 0.5,
            }
  
#+end_src

#+RESULTS:

** load

#+begin_src ipython
  options = set_options(**kwargs)
  options['reload'] = False
  options['data_type'] = 'raw'
  options['DCVL'] = 1
  X_days, y_days = get_X_y_days(**options)
  X_data, y_data = get_X_y_S1_S2(X_days, y_days, **options)
#+end_src

#+RESULTS:
#+begin_example
  loading files from /home/leon/dual_task/dual_data/data/JawsM15
  X_days (1152, 693, 84) y_days (1152, 6)
  ##########################################
  PREPROCESSING: SCALER None AVG MEAN False AVG NOISE True UNIT VAR False
  ##########################################
  Deconvolve Fluo
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/scipy/signal/_spectral_py.py:2017: UserWarning: nperseg = 256 is greater than input length  = 84, using nperseg = 84
    warnings.warn('nperseg = {0:d} is greater than input length '
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  /home/leon/mambaforge/envs/torch/lib/python3.10/site-packages/oasis/functions.py:166: RuntimeWarning: invalid value encountered in multiply
    return constrained_oasisAR1(y, g[0], sn, optimize_b=True if b is None else False,
  ##########################################
  DATA: FEATURES sample TASK DualGo TRIALS  DAYS first LASER 0
  ##########################################
  multiple days 0 3 0
  X_S1 (48, 693, 84) X_S2 (48, 693, 84)
#+end_example


#+begin_src ipython
  import numpy as np
  from scipy.ndimage import convolve1d
  
  def moving_average_multidim(data, window_size, axis=-1):
      """
      Apply a 1D moving average across a specified axis of a multi-dimensional array.

      :param data: multi-dimensional array of data
      :param window_size: size of the moving window 
      :param axis: axis along which to apply the moving average
      :return: smoothed data with the same shape as input data
      """
      # Create a moving average filter window
      window = np.ones(window_size) / window_size
      # Apply 1D convolution along the specified axis
      smoothed_data = convolve1d(data, weights=window, axis=axis, mode='reflect')
      return smoothed_data

#+end_src

#+RESULTS:

#+begin_src ipython
  from dual_data.decode.bump import circcvl
  # smoothed_data = circcvl(X_data, windowSize=2, axis=-1)
  print(X_data.shape)
  window_size = 6
  # from scipy.ndimage import gaussian_filter1d
  # smoothed_data = gaussian_filter1d(X_data, axis=-1, sigma=2)
  smoothed_data = moving_average_multidim(X_data[..., :52], window_size, axis=-1)
#+end_src

#+RESULTS:
: (96, 693, 84)

#+begin_src ipython
  for i in range(30):
      plt.plot(smoothed_data[0, i,:])
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/ed5f136cd3684651470e9a84eba9bcf85b1342e4.png]]

* Training

#+begin_src ipython
  # y = np.roll(X_data, -1)
  # y = y[..., :-1]

  Y = smoothed_data[..., 1:]
  X = smoothed_data[..., :-1]
  
  X = np.swapaxes(X, 1, -1)
  Y = np.swapaxes(Y, 1, -1)

  print(X.shape, Y.shape)
#+end_src

#+RESULTS:
: (96, 51, 693) (96, 51, 693)

#+begin_src ipython
  X = torch.tensor(X, dtype=torch.float32, device=device)
  Y = torch.tensor(Y, dtype=torch.float32, device=device)
  print(X.shape, Y.shape)
#+end_src

#+RESULTS:
: torch.Size([96, 51, 693]) torch.Size([96, 51, 693])

#+begin_src ipython
  import torch
  from torch.utils.data import TensorDataset, DataLoader
  from sklearn.preprocessing import StandardScaler, MinMaxScaler

  # Hyperparameters
  num_epochs = 52
  num_features=X.shape[-1]
  batch_size = 5
  learning_rate = 0.001
  hidden_size = X.shape[-1]
  num_layers = 1

  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

  # Split the dataset into training and validation sets
  train_size = int(0.8 * X.shape[0])
  val_size = X.shape[0] - train_size

  X_train = X[:train_size]
  X_test = X[train_size:]

  Y_train = Y[:train_size]
  Y_test = Y[train_size:]

  # scaler = StandardScaler() # or MinMaxScaler()
  # scaler.fit(X_train)
  # X_train_scaled = scaler.transform(X_train)
  # X_test_scaled = scaler.transform(X_test)

  # scaler.fit(Y_train)
  # Y_train_scaled = scaler.transform(Y_train)
  # Y_test_scaled = scaler.transform(Y_test)

  # Create data sets
  train_dataset = TensorDataset(X_train, Y_train)
  val_dataset = TensorDataset(X_test, Y_test)

  # # Create data loaders
  train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)
  val_loader = DataLoader(dataset=val_dataset, batch_size=batch_size, shuffle=False)

  # # Define Model
  model = MultivariateRNN(input_size=num_features, hidden_size=hidden_size, num_layers=num_layers, output_size=num_features)
  model = model.to(device)

  # # Loss and optimizer
  criterion = nn.MSELoss()
  optimizer = optim.Adam(model.parameters(), lr=learning_rate)
#+end_src

#+RESULTS:

#+begin_src ipython
  import torch.optim as optim

  # Define the number of epochs.
  num_epochs = 100  # Adjust the number of epochs

  # Training loop.
  for epoch in range(num_epochs):
      model.train()
      for batch_idx, (data, targets) in enumerate(train_loader):
          # Forward pass.
          outputs = model(data)
          loss = criterion(outputs, targets)

          # Backward pass and optimization.
          optimizer.zero_grad()
          loss.backward()
          optimizer.step()

      # Validation loop.
      model.eval()
      val_loss = 0.0
      with torch.no_grad():
          for data, targets in val_loader:
              outputs = model(data)
              loss = criterion(outputs, targets)
              val_loss += loss.item() * data.size(0)
      val_loss /= len(val_loader.dataset)

      # Print training/validation statistics.
      # You may want to save the model if it has improved.
      if epoch % 10:
          print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {loss.item():.4f}, Validation Loss: {val_loss:.4f}')
  # Don't forget to switch to CPU/GPU based on your setup:
  # model.to('cuda') or model.to('cpu')
#+end_src

#+RESULTS:
#+begin_example
  Epoch 2/100, Training Loss: 0.0014, Validation Loss: 0.0014
  Epoch 3/100, Training Loss: 0.0013, Validation Loss: 0.0013
  Epoch 4/100, Training Loss: 0.0012, Validation Loss: 0.0012
  Epoch 5/100, Training Loss: 0.0012, Validation Loss: 0.0011
  Epoch 6/100, Training Loss: 0.0011, Validation Loss: 0.0011
  Epoch 7/100, Training Loss: 0.0010, Validation Loss: 0.0010
  Epoch 8/100, Training Loss: 0.0010, Validation Loss: 0.0010
  Epoch 9/100, Training Loss: 0.0009, Validation Loss: 0.0009
  Epoch 10/100, Training Loss: 0.0009, Validation Loss: 0.0009
  Epoch 12/100, Training Loss: 0.0009, Validation Loss: 0.0009
  Epoch 13/100, Training Loss: 0.0008, Validation Loss: 0.0008
  Epoch 14/100, Training Loss: 0.0008, Validation Loss: 0.0008
  Epoch 15/100, Training Loss: 0.0008, Validation Loss: 0.0008
  Epoch 16/100, Training Loss: 0.0008, Validation Loss: 0.0008
  Epoch 17/100, Training Loss: 0.0008, Validation Loss: 0.0008
  Epoch 18/100, Training Loss: 0.0007, Validation Loss: 0.0007
  Epoch 19/100, Training Loss: 0.0007, Validation Loss: 0.0007
  Epoch 20/100, Training Loss: 0.0007, Validation Loss: 0.0007
  Epoch 22/100, Training Loss: 0.0007, Validation Loss: 0.0007
  Epoch 23/100, Training Loss: 0.0007, Validation Loss: 0.0007
  Epoch 24/100, Training Loss: 0.0007, Validation Loss: 0.0007
  Epoch 25/100, Training Loss: 0.0007, Validation Loss: 0.0007
  Epoch 26/100, Training Loss: 0.0007, Validation Loss: 0.0007
  Epoch 27/100, Training Loss: 0.0007, Validation Loss: 0.0007
  Epoch 28/100, Training Loss: 0.0007, Validation Loss: 0.0007
  Epoch 29/100, Training Loss: 0.0007, Validation Loss: 0.0007
  Epoch 30/100, Training Loss: 0.0007, Validation Loss: 0.0007
  Epoch 32/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 33/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 34/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 35/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 36/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 37/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 38/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 39/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 40/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 42/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 43/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 44/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 45/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 46/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 47/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 48/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 49/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 50/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 52/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 53/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 54/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 55/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 56/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 57/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 58/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 59/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 60/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 62/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 63/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 64/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 65/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 66/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 67/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 68/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 69/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 70/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 72/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 73/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 74/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 75/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 76/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 77/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 78/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 79/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 80/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 82/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 83/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 84/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 85/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 86/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 87/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 88/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 89/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 90/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 92/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 93/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 94/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 95/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 96/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 97/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 98/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 99/100, Training Loss: 0.0006, Validation Loss: 0.0006
  Epoch 100/100, Training Loss: 0.0006, Validation Loss: 0.0006
#+end_example

* Reverse Engineering
** Generate series

#+begin_src ipython
  model.eval()  # Set the model to evaluation mode

  input_size = X.shape[-1]  # Number of features
  sequence_length = X.shape[1]

  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

  # Start with an initial seed sequence
  seed_sequence = torch.randn(1, sequence_length, input_size).to(device)  # Replace with your actual seed

  # Decide how many future steps you want to predict
  future_steps = X.shape[1]

  # Collect predictions
  predictions = []

  # Initialize the hidden state (optional, depends on your model architecture)
  hidden = torch.zeros(num_layers, 1, hidden_size).to(device)

  # Generate time series
  for _ in range(future_steps):
      # Forward pass
      with torch.no_grad():  # No need to track gradients
          out, hidden = model.rnn(seed_sequence, hidden)
          next_step = model.fc(out[:, -1, :])  # Output for the last time step

      predictions.append(next_step.cpu().numpy())

      # Use the predicted next step as the input for the next iteration
      next_step = next_step.unsqueeze(1)  # Add the sequence length dimension
      seed_sequence = torch.cat((seed_sequence[:, 1:, :], next_step), 1)  # Move the window

  # # Convert predictions to a numpy array for further analysis
  predicted_time_series = np.concatenate(predictions, axis=0)
#+end_src

#+RESULTS:

#+begin_src ipython
  import numpy as np
  import matplotlib.pyplot as plt

  # Assuming 'predicted_time_series' is a numpy array containing your generated data
  # Each column in 'predicted_time_series' corresponds to a different feature in the time series

  # Plot each feature of the time series
  num_features = predicted_time_series.shape[1]
  plt.figure(figsize=(12, 6))
  for i in range(20):
      # plt.plot(predicted_time_series[:, i], lw=2)
      plt.plot(X.cpu().numpy()[0, :, i])

  plt.xlabel('Time')
  plt.ylabel('Feature Value')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/741c6d8bf6a0f7d550ff9875b6ba285050af3fce.png]]

** Connectivity

#+begin_src ipython
  weights = model.rnn.weight_hh_l0.data.cpu().numpy()  # Get the recurring weights of the RNN

  print(weights.shape)
  # Perform singular value decomposition<
  U, S, Vt = np.linalg.svd(weights, full_matrices=False)

  u1, u2 = U[:, 0], U[:, 1]  # First two left singular vectors
  v1, v2 = Vt[0, :], Vt[1, :]  # First two right singular vectors
#+end_src

#+RESULTS:
: (693, 693)

#+begin_src ipython
  ksi = S[0] * u1 * v1
  ksi2 = S[1] * u2 * v2
  print(ksi.shape)
#+end_src

#+RESULTS:
: (693,)

#+begin_src ipython
  plt.imshow(abs(weights), cmap='jet')
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/94d03529e296cc3a3af156858d1637bf192ba089.png]]

#+begin_src ipython
print(S[:10])
#+end_src

#+RESULTS:
: [2.7084596 2.3548892 1.9583218 1.8420836 1.6692601 1.6253643 1.5145588
:  1.4847671 1.4561393 1.4049715]

#+begin_src ipython
  theta = np.arctan2(ksi2, ksi)
  index = theta.argsort()
  print(index.shape)
#+end_src

#+RESULTS:
: (693,)

#+begin_src ipython
  Jij = weights[index][index]
  print(Jij.shape)
#+end_src

#+RESULTS:
: (693, 693)

#+begin_src ipython
  plt.imshow(abs(Jij), cmap='jet', vmin=0.0)
  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/b9b0a606195576a6830adb182a59efa4e2f4b577.png]]

#+begin_src ipython
  # Plot the singular values
  plt.figure(figsize=(10, 5))
  plt.plot(S)
  plt.yscale('log')  # Log scale can be helpful to see the drop-off more clearly
  plt.title('Singular Values of the RNN Hidden-to-Hidden Weight Matrix')
  plt.ylabel('Singular values (log scale)')
  plt.xlabel('Index')
  plt.grid(True)
  plt.show()

  # To see the cumulative energy, plot the cumulative sum of squares of singular values
  cumulative_energy = np.cumsum(S*2) / np.sum(S*2)
  plt.figure(figsize=(10, 5))
  plt.plot(cumulative_energy)
  plt.title('Cumulative Sum of Squares of Singular Values')
  plt.ylabel('Cumulative energy')
  plt.xlabel('Index')
  plt.grid(True)
  plt.show()

#+end_src

#+RESULTS:
:RESULTS:
[[file:./.ob-jupyter/e9ec87f1b3cc21caac822193a5d0ed3b23d4ed32.png]]
[[file:./.ob-jupyter/1d180e0f8f7187364dfd7331c8f80f7cd749b0b8.png]]
:END:
