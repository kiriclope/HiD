#+STARTUP: fold
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session decoder :kernel dual_data

* Notebook Settings

#+begin_src ipython
%load_ext autoreload
%autoreload 2
%reload_ext autoreload

%run /home/leon/dual_task/dual_data/notebooks/setup.py
%matplotlib inline
%config InlineBackend.figure_format = 'png'
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/dual_data/bin/python

* Imports

#+begin_src ipython
  import sys
  sys.path.insert(0, '/home/leon/dual_task/dual_data/')

  import pickle as pkl
  import numpy as np
  import matplotlib.pyplot as plt
  from scipy.stats import circmean
  from time import perf_counter

  import torch
  import torch.nn as nn
  import torch.optim as optim
  from torch.utils.data import Dataset, TensorDataset, DataLoader
  from skorch import NeuralNetClassifier

  from sklearn.metrics import make_scorer
  from sklearn.metrics import roc_auc_score

  from sklearn.base import clone
  from sklearn.ensemble import BaggingClassifier
  from sklearn.preprocessing import StandardScaler, RobustScaler
  from sklearn.pipeline import Pipeline
  from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, RepeatedStratifiedKFold, StratifiedKFold, LeaveOneOut
  from sklearn.decomposition import PCA

  from mne.decoding import SlidingEstimator, cross_val_multiscore, GeneralizingEstimator, get_coef

  from src.common.plot_utils import add_vlines, add_vdashed
  from src.attractor.energy import run_energy, plot_energy
  from src.common.options import set_options
  from src.stats.bootstrap import my_boots_ci
  from src.decode.bump import decode_bump, circcvl
  from src.common.get_data import get_X_y_days, get_X_y_S1_S2
  from src.common.options import set_options
  from src.preprocess.helpers import avg_epochs
#+end_src

#+RESULTS:

* Helpers
** Perceptron
#+begin_src ipython
class CustomBCEWithLogitsLoss(nn.BCEWithLogitsLoss):
    def __init__(self, pos_weight=None, weight=None, reduction='mean'):
        super(CustomBCEWithLogitsLoss, self).__init__(weight=weight, reduction=reduction, pos_weight=pos_weight)

    def forward(self, input, target):
        target = target.view(-1, 1)  # Make sure target shape is (n_samples, 1)
        return super().forward(input.to(torch.float32), target.to(torch.float32))
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/decode/perceptron.py
class Perceptron(nn.Module):
    def __init__(self, num_features, dropout_rate=0.0):
        super(Perceptron, self).__init__()
        self.linear = nn.Linear(num_features, 1)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, x):
        x = self.dropout(x)
        hidden = self.linear(x)
        return hidden
#+end_src

#+RESULTS:

#+begin_src ipython
  class MLP(nn.Module):
      def __init__(self, num_features, hidden_units=64, dropout_rate=0.5):
          super(MLP, self).__init__()
          self.linear = nn.Linear(num_features, hidden_units)
          self.dropout = nn.Dropout(dropout_rate)
          self.relu = nn.ReLU()
          self.linear2 = nn.Linear(hidden_units, 1)

      def forward(self, x):
          x = self.dropout(x)
          x = self.relu(self.linear(x))
          x = self.dropout(x)
          hidden = self.linear2(x)
          return hidden
#+end_src

#+RESULTS:


#+begin_src ipython
from skorch.callbacks import Callback
from skorch.callbacks import EarlyStopping

early_stopping = EarlyStopping(
    monitor='train_loss',    # Metric to monitor
    patience=5,              # Number of epochs to wait for improvement
    threshold=0.001,       # Minimum change to qualify as an improvement
    threshold_mode='rel',    # 'rel' for relative change, 'abs' for absolute change
    lower_is_better=True     # Set to True if lower metric values are better
)

#+end_src

#+RESULTS:


#+begin_src ipython
class RegularizedNet(NeuralNetClassifier):
    def __init__(self, module, alpha=0.001, l1_ratio=0.95, **kwargs):
        self.alpha = alpha  # Regularization strength
        self.l1_ratio = l1_ratio # Balance between L1 and L2 regularization

        super().__init__(module, **kwargs)

    def get_loss(self, y_pred, y_true, X=None, training=False):
        # Call super method to compute primary loss
        if y_pred.shape != y_true.shape:
            y_true = y_true.unsqueeze(-1)

        loss = super().get_loss(y_pred, y_true, X=X, training=training)

        if self.alpha>0:
            elastic_net_reg = 0
            for param in self.module_.parameters():
                elastic_net_reg += self.alpha * self.l1_ratio * torch.sum(torch.abs(param))
                elastic_net_reg += self.alpha * (1 - self.l1_ratio) * torch.sum(param ** 2)

        # Add the elastic net regularization term to the primary loss
        return loss + elastic_net_reg
#+end_src

#+RESULTS:

** Other

#+begin_src ipython
  def safe_roc_auc_score(y_true, y_score):
      y_true = np.asarray(y_true)
      if len(np.unique(y_true)) == 1:
          return np.nan  # return np.nan where the score cannot be calculated
      return roc_auc_score(y_true, y_score)
#+end_src

#+RESULTS:

#+begin_src ipython
  def get_bagged_coefs(clf, n_estimators):
      coefs_list = []
      bias_list = []
      for i in range(n_estimators):
          model = clf.estimators_[i]
          coefs = model.named_steps['net'].module_.linear.weight.data.cpu().detach().numpy()[0]
          bias = model.named_steps['net'].module_.linear.bias.data.cpu().detach().numpy()[0]

          coefs, bias = rescale_coefs(model, coefs, bias)

          coefs_list.append(coefs)
          bias_list.append(bias)

      return np.array(coefs_list).mean(0), np.array(bias_list).mean(0)
#+end_src

#+RESULTS:

#+begin_src ipython
  def rescale_coefs(model, coefs, bias):

          try:
                  means = model.named_steps["scaler"].mean_
                  scales = model.named_steps["scaler"].scale_

                  # Rescale the coefficients
                  rescaled_coefs = np.true_divide(coefs, scales)

                  # Adjust the intercept
                  rescaled_bias = bias - np.sum(rescaled_coefs * means)

                  return rescaled_coefs, rescaled_bias
          except:
                  return coefs, bias

#+end_src

#+RESULTS:

#+begin_src ipython
  from scipy.stats import bootstrap

  def get_bootstrap_ci(data, statistic=np.mean, confidence_level=0.95, n_resamples=1000, random_state=None):
      result = bootstrap((data,), statistic)
      ci_lower, ci_upper = result.confidence_interval
      return np.array([ci_lower, ci_upper])
#+end_src

#+RESULTS:

#+begin_src ipython
def convert_seconds(seconds):
    h = seconds // 3600
    m = (seconds % 3600) // 60
    s = seconds % 60
    return h, m, s
#+end_src

#+RESULTS:

#+begin_src ipython
def pkl_save(obj, name, path="."):
    pkl.dump(obj, open(path + "/" + name + ".pkl", "wb"))


def pkl_load(name, path="."):
    return pkl.load(open(path + "/" + name, "rb"))

#+end_src

#+RESULTS:

* Parameters

#+begin_src ipython
  DEVICE = 'cuda:1'

  mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
  N_NEURONS = [668, 693, 444, 361, 113]

  tasks = ['DPA', 'DualGo', 'DualNoGo']

  kwargs = {
      'mouse': 'JawsM15',
      'trials': '', 'reload': 0, 'data_type': 'dF', 'preprocess': True,
      'scaler_BL': 'robust', 'avg_noise': True, 'unit_var_BL': False,
      'random_state': None, 'T_WINDOW': 0.0,
      'l1_ratio': 0.95, 'laser':0,
      'n_comp': None, 'scaler': None,
      'n_splits': 3, 'n_repeats': 16,
      'class_weight': 1,
  }

  options = set_options(**kwargs)
  options['fname'] = '_tasks'
#+end_src

#+RESULTS:

* Decoding vs days
** Helpers

#+begin_src ipython
  def hyper_tune(model, epoch, params, scoring, **options):

      # load data
      X_days, y_days = get_X_y_days(**options)
      X, y = get_X_y_S1_S2(X_days, y_days, **options)
      y[y==-1] = 0

      if options['class_weight']:
          pos_weight = torch.tensor(np.sum(y==0) / np.sum(y==1), device=DEVICE).to(torch.float32)
          print('imbalance', pos_weight)
          model.criterion__pos_weight = pos_weight

      options['epochs'] = [epoch]
      X_avg = avg_epochs(X, **options).astype('float32')
      print('X', X.shape, 'y', y.shape)

      if options['n_splits']==-1:
           cv = LeaveOneOut()
           scoring = 'accuracy'
      else:
          cv = RepeatedStratifiedKFold(n_splits=options['n_splits'], n_repeats=options['n_repeats'])

      # Perform grid search
      grid = GridSearchCV(model, params, refit=True, cv=cv, scoring=scoring, n_jobs=30)
      start = perf_counter()
      print('hyperparam fitting ...')
      grid.fit(X_avg, y)
      end = perf_counter()
      print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))

      best_model = grid.best_estimator_
      best_params = grid.best_params_
      print(best_params)

      # cross validated scores
      print('Computing cv scores ...')
      estimator = SlidingEstimator(clone(best_model), n_jobs=1,
                                  scoring=scoring, verbose=False)
      scores = cross_val_multiscore(estimator, X.astype('float32'), y,
                                  cv=cv, n_jobs=-1, verbose=False)
      end = perf_counter()
      print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))

      return scores
#+end_src

#+RESULTS:

** Fit

#+begin_src ipython
  net = RegularizedNet(
      module=Perceptron,
      module__num_features=693,
      module__dropout_rate=0.0,
      alpha=0.01,
      l1_ratio=options['l1_ratio'],
      criterion=CustomBCEWithLogitsLoss,
      criterion__pos_weight=torch.tensor(1.0, device=DEVICE).to(torch.float32),
      optimizer=optim.Adam,
      optimizer__lr=0.1,
      max_epochs=1000,
      callbacks=[early_stopping],
      train_split=None,
      iterator_train__shuffle=False,  # Ensure the data is shuffled each epoch
      verbose=0,
      device= DEVICE if torch.cuda.is_available() else 'cpu',  # Assuming you might want to use CUDA
  )

  pipe = []
  if options['scaler'] is not None:
      pipe.append(("scaler", StandardScaler()))
  if options['n_comp'] is not None:
      pipe.append(("pca", PCA(n_components=options['n_comp'])))

  pipe.append(("net", net))
  pipe = Pipeline(pipe)
#+end_src

#+RESULTS:

#+begin_src ipython
  params = {
    'net__alpha': np.logspace(-4, 4, 10),
    # 'net__l1_ratio': np.linspace(0, 1, 10),
    # 'net__module__dropout_rate': np.linspace(0, 1, 10),
  }

  options['verbose'] = 1
  options['reload'] = 0

  safe_roc_auc = make_scorer(safe_roc_auc_score, needs_proba=True)
  scoring = safe_roc_auc

  days = np.arange(1, options['n_days']+1)
  options = set_options(**options)

  if options['n_comp'] is None:
      index = mice.index(options['mouse'])
      pipe['net'].module__num_features = N_NEURONS[index]
  else:
      pipe['net'].module__num_features = options['n_comp']

  scores_sample = []
  scores_choice = []

  for task in tasks:
    options['task'] = task

    scores_sample_task = []
    scores_choice_task = []

    for day in days:
        options['day'] = day

        options['features'] = 'sample'
        scores = hyper_tune(pipe, epoch='ED', params=params, scoring=scoring, **options)
        scores_sample_task.append(scores)

        # options['features'] = 'distractor'
        # scores = hyper_tune(pipe, epoch='MD', params=params, scoring=scoring, **options)
        # scores_dist_task.append(scores)

        options['features'] = 'choice'
        # try:
        scores = hyper_tune(pipe, epoch='CHOICE', params=params, scoring=scoring, **options)
        # except:
        #     scores = np.zeros((options['n_days'] , options['n_splits'] * options['n_repeats'], 84))
        scores_choice_task.append(scores)


    scores_sample.append(scores_sample_task)
    scores_choice.append(scores_choice_task)

    scores_save = np.stack((scores_sample, scores_choice))
    print(scores_save.shape)
    pkl_save(scores_save, '%s_scores_tasks_%.2f_l1_ratio%s' % (options['mouse'], options['l1_ratio'], options['fname']), path="../data/%s/" % options['mouse'])

    #+end_src

#+RESULTS:
#+begin_example
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DPA TRIALS  DAYS 1 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 14s
  {'net__alpha': 0.005994842503189409}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 3s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DPA TRIALS  DAYS 1 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 19s
  {'net__alpha': 0.046415888336127774}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 2s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DPA TRIALS  DAYS 2 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 20s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 3s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DPA TRIALS  DAYS 2 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 18s
  {'net__alpha': 0.046415888336127774}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 1s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DPA TRIALS  DAYS 3 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 0m 59s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DPA TRIALS  DAYS 3 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 0m 59s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DPA TRIALS  DAYS 4 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 18s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 0s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DPA TRIALS  DAYS 4 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.046415888336127774}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 0s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DPA TRIALS  DAYS 5 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 18s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 0s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DPA TRIALS  DAYS 5 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 0m 59s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DPA TRIALS  DAYS 6 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 0m 59s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DPA TRIALS  DAYS 6 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 18s
  {'net__alpha': 0.046415888336127774}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 0s
  (2, 1, 6, 48, 84)
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DualGo TRIALS  DAYS 1 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 18s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 0s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DualGo TRIALS  DAYS 1 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 0m 59s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DualGo TRIALS  DAYS 2 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 0s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DualGo TRIALS  DAYS 2 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 18s
  {'net__alpha': 0.046415888336127774}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 0s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DualGo TRIALS  DAYS 3 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 0m 59s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DualGo TRIALS  DAYS 3 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 0m 59s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DualGo TRIALS  DAYS 4 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 0m 59s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DualGo TRIALS  DAYS 4 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.046415888336127774}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 0m 59s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DualGo TRIALS  DAYS 5 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 18s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 0s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DualGo TRIALS  DAYS 5 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 166.81005372000556}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 0m 59s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DualGo TRIALS  DAYS 6 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 0s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DualGo TRIALS  DAYS 6 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.000774263682681127}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 8s
  (2, 2, 6, 48, 84)
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS 1 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 0m 59s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DualNoGo TRIALS  DAYS 1 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 18s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 0s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS 2 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 0s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DualNoGo TRIALS  DAYS 2 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 0s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS 3 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 18s
  {'net__alpha': 0.046415888336127774}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 0s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DualNoGo TRIALS  DAYS 3 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 0m 59s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS 4 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 18s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 0s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DualNoGo TRIALS  DAYS 4 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 19s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 1s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS 5 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 0m 59s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DualNoGo TRIALS  DAYS 5 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 18s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 0m 59s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS 6 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 0s
  Loading files from /home/leon/dual_task/dual_data/data/JawsM15
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
  DATA: FEATURES choice TASK DualNoGo TRIALS  DAYS 6 LASER 0
  X (32, 693, 84) y (32,)
  hyperparam fitting ...
  Elapsed (with compilation) = 0h 0m 17s
  {'net__alpha': 0.3593813663804626}
  Computing cv scores ...
  Elapsed (with compilation) = 0h 1m 0s
  (2, 3, 6, 48, 84)
#+end_example

#+begin_src ipython
  print(np.array(scores_sample).shape)
  print(np.array(scores_choice).shape)
#+end_src

#+RESULTS:
: (3, 6, 48, 84)
: (3, 6, 48, 84)

* Scores

#+begin_src ipython
  filename = '%s_scores_tasks_%.2f_l1_ratio%s.pkl' % (options['mouse'], options['l1_ratio'], options['fname'])
  print(filename)
  try:
      scores = pkl_load(filename, path="../data/%s/" % options['mouse'])
      print('scores', scores.shape)
  except:
      print('file not found')
#+end_src

#+RESULTS:
: JawsM15_scores_tasks_0.95_l1_ratio_tasks.pkl
: scores (2, 3, 6, 48, 84)

#+begin_src ipython
  scores_sample = scores[0]
  scores_choice = scores[1]
#+end_src

#+RESULTS:

#+begin_src ipython
    options['epochs'] = ['POST_DIST']

    colors = ['r', 'b', 'g']
    for task in range(len(tasks)):
        sample_avg = []
        sample_ci = []
        for i in range(options['n_days']):
            sample_epoch = avg_epochs(scores_sample[task][i], **options)
            sample_avg.append(sample_epoch.mean(0))
            sample_ci.append(get_bootstrap_ci(sample_epoch))

        sample_avg = np.array(sample_avg)
        sample_ci = np.array(sample_ci).T

        plt.plot(np.arange(1, options['n_days']+1), sample_avg, '-o', label='%s' % options['tasks'][task], color=colors[task])
        plt.fill_between(np.arange(1, options['n_days']+1), sample_ci[0], sample_ci[1], color=colors[task], alpha=0.1)

    plt.axhline(y=0.5, color='k', linestyle='--')

    plt.legend(fontsize=10)
    plt.xticks(np.arange(1, options['n_days']+1))
    plt.yticks([0.4, 0.6, 0.8, 1.0])
    plt.xlabel('Day')
    plt.ylabel('Sample Score')
    plt.savefig('%s_scores_avg.svg' % options['mouse'], dpi=300)
    plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/3b4de508ddfeca579bbf97c63ded703c29e5b74d.png]]

#+begin_src ipython
  options['epochs'] = ['LD']
  for task in range(len(tasks)):
      choice_avg = []
      choice_ci = []
      for i in range(options['n_days']):
          choice_epoch = avg_epochs(scores_choice[task][i], **options)
          choice_avg.append(np.nanmean(choice_epoch, axis=0))
          choice_ci.append(get_bootstrap_ci(choice_epoch))

      choice_avg = np.array(choice_avg)
      choice_ci = np.array(choice_ci).T

      plt.plot(np.arange(1, options['n_days']+1), choice_avg, '-o', label='%s' % options['tasks'][task], color=colors[task])
      plt.fill_between(np.arange(1, options['n_days']+1), choice_ci[0], choice_ci[1], color=colors[task], alpha=0.1)
  plt.axhline(y=0.5, color='k', linestyle='--')

  plt.legend(fontsize=10)
  plt.xticks(np.arange(1, options['n_days']+1))
  plt.yticks([0.4, 0.6, 0.8, 1.0])
  plt.xlabel('Day')
  plt.ylabel('Choice Score')
  plt.savefig('%s_scores_avg.svg' % options['mouse'], dpi=300)
  plt.show()

#+end_src

#+RESULTS:
[[file:./.ob-jupyter/1167050d9d849b11b008d6aaafbe6b23b770983d.png]]

* Scores mice

#+begin_src ipython
  mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
  # mice = ['JawsM15', 'JawsM18']
  mouse = 'JawsM15'

  l1_ratio = 0.95

  tasks = ['DPA', 'DualGo', 'DualNoGo']
  N_NEURONS = [668, 693, 444, 361, 113]

  kwargs = {
      'mouse': 'ACCM03',
      'trials': '', 'reload': 0, 'data_type': 'dF', 'preprocess': False,
      'scaler_BL': 'robust', 'avg_noise':True, 'unit_var_BL':False,
      'random_state': None, 'T_WINDOW': 0.0,
      'l1_ratio': 0.95,
  }

  options = set_options(**options)
  fname = options['fname']
  print(fname)
#+end_src

#+RESULTS:
: _tasks

#+begin_src ipython
  scores_mice = []

  for mouse in mice:
      filename = '%s_scores_tasks_%.2f_l1_ratio%s.pkl' % (mouse, l1_ratio, fname)
      print(filename)
      try:
          scores = pkl_load(filename, path="../data/%s/" % mouse)
          print('scores', scores.shape)
          scores_mice.append(scores)
      except:
          print('file not found')
          scores_mice.append(np.nan * np.ones((3, 6, 2, 84)))
#+end_src

#+RESULTS:
: ChRM04_scores_tasks_0.95_l1_ratio_tasks.pkl
: scores (2, 3, 6, 30, 84)
: JawsM15_scores_tasks_0.95_l1_ratio_tasks.pkl
: scores (2, 3, 6, 30, 84)
: JawsM18_scores_tasks_0.95_l1_ratio_tasks.pkl
: scores (2, 3, 6, 30, 84)
: ACCM03_scores_tasks_0.95_l1_ratio_tasks.pkl
: scores (2, 3, 5, 30, 84)
: ACCM04_scores_tasks_0.95_l1_ratio_tasks.pkl
: scores (2, 3, 5, 30, 84)

#+begin_src ipython
  colors = ['r', 'b', 'g']
  options = set_options(**kwargs)
  options['T_WINDOW'] = 0
  options['epochs'] = ['POST_DIST']

  for task in range(3):
    sample_mice = []
    for i in range(len(mice)):
        scores_sample = scores_mice[i][0][task]
        sample_avg = []
        # sample_ci = []
        for j in range(scores_sample.shape[0]):
            sample_epoch = avg_epochs(scores_sample[j], **options)
            sample_avg.append(sample_epoch.mean(0))
            # sample_ci.append(get_bootstrap_ci(sample_epoch))

        sample_avg = np.array(sample_avg)
        while sample_avg.shape[0] !=6:
            sample_avg = np.append(sample_avg, np.nan)

        sample_mice.append(sample_avg)

    sample_mice = np.array(sample_mice)
    sample_ci = get_bootstrap_ci(sample_mice)
    sample_ci_last = get_bootstrap_ci(sample_mice[:3][-1])
    sample_ci[0][-1] = sample_ci_last[0]
    sample_ci[1][-1] = sample_ci_last[1]

    plt.plot(np.arange(1, 7), np.nanmean(sample_mice, 0), '-o', label='%s' % options['tasks'][task], color=colors[task])
    plt.fill_between(np.arange(1, 7), sample_ci[0], sample_ci[1], color=colors[task], alpha=0.05)
    plt.axhline(y=0.5, color='k', linestyle='--')
  plt.legend(fontsize=16, frameon=0)
  plt.xlabel('Day')
  plt.ylabel('Sample Score')
  plt.xticks(np.arange(1,7))
  plt.yticks([0.5, 0.6, 0.7, 0.8])
  plt.savefig('./figs/mice_scores_tasks_sample%s.svg' % fname, dpi=300)

  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/96869e7a27345008277a877803c90c09e3239eaf.png]]

#+begin_src ipython
  colors = ['r', 'b', 'g']
  options = set_options(**kwargs)
  options['T_WINDOW'] = 0
  options['epochs'] = ['ED']

  for task in range(3):
    choice_mice = []
    for i in range(len(mice)):
        scores_choice = scores_mice[i][1][task]
        choice_avg = []
        for j in range(scores_choice.shape[0]):
            choice_epoch = avg_epochs(scores_choice[j], **options)
            choice_avg.append(choice_epoch.mean(0))

        choice_avg = np.array(choice_avg)
        while choice_avg.shape[0] !=6:
            choice_avg = np.append(choice_avg, np.nan)
        choice_mice.append(choice_avg)

    choice_mice = np.array(choice_mice)
    choice_ci = get_bootstrap_ci(choice_mice)
    choice_ci_last = get_bootstrap_ci(choice_mice[:3][-1])
    choice_ci[0][-1] = choice_ci_last[0]
    choice_ci[1][-1] = choice_ci_last[1]

    plt.plot(np.arange(1, 7), np.nanmean(choice_mice, 0), '-o', label='%s' % options['tasks'][task], color=colors[task])
    plt.fill_between(np.arange(1, 7), choice_ci[0], choice_ci[1], color=colors[task], alpha=0.05)
    plt.axhline(y=0.5, color='k', linestyle='--')
  plt.legend(fontsize=16, frameon=0)
  plt.xlabel('Day')
  plt.ylabel('Choice Score')
  plt.xticks(np.arange(1,7))
  plt.yticks([0.5, 0.6, 0.7, 0.8])
  plt.savefig('./figs/mice_scores_tasks_choice%s.svg' % fname, dpi=300)

  plt.show()
#+end_src

#+RESULTS:
[[file:./.ob-jupyter/74b944587a0da69150fca547c389e9c2505ca1f8.png]]


#+begin_src ipython

#+end_src

#+RESULTS:
