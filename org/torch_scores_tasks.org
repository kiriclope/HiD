#+STARTUP: fold
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session decoder :kernel dual_data

* Notebook Settings

#+begin_src ipython
%load_ext autoreload
%autoreload 2
%reload_ext autoreload

%run /home/leon/dual_task/dual_data/notebooks/setup.py
%matplotlib inline
%config InlineBackend.figure_format = 'png'
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/dual_data/bin/python

* Imports

#+begin_src ipython
  import warnings
  from sklearn.exceptions import ConvergenceWarning
  warnings.filterwarnings("ignore")

  import sys
  sys.path.insert(0, '/home/leon/dual_task/dual_data/')

  import os
  if not sys.warnoptions:
    warnings.simplefilter("ignore")
    os.environ["PYTHONWARNINGS"] = "ignore"

  import pickle as pkl
  import numpy as np
  import matplotlib.pyplot as plt
  from time import perf_counter

  import torch
  import torch.nn as nn
  import torch.optim as optim
  from skorch import NeuralNetClassifier

  from sklearn.base import clone
  from sklearn.metrics import make_scorer, roc_auc_score
  from sklearn.ensemble import BaggingClassifier
  from sklearn.preprocessing import StandardScaler, RobustScaler
  from sklearn.pipeline import Pipeline
  from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, LeaveOneOut
  from sklearn.decomposition import PCA

  from mne.decoding import SlidingEstimator, cross_val_multiscore, GeneralizingEstimator, get_coef
  from src.decode.my_mne import my_cross_val_multiscore

  from src.common.plot_utils import add_vlines, add_vdashed
  from src.common.options import set_options
  from src.stats.bootstrap import my_boots_ci
  from src.decode.bump import decode_bump, circcvl
  from src.decode.classifiers import safeSelector
  from src.common.get_data import get_X_y_days, get_X_y_S1_S2
  from src.preprocess.helpers import avg_epochs
#+end_src

#+RESULTS:

#+begin_src ipython
from src.torch.perceptron import CustomBCEWithLogitsLoss, Perceptron, MLP
from src.torch.skorch import early_stopping, RegularizedNet
from src.torch.classificationCV import ClassificationCV
from src.torch.main import get_classification
#+end_src

#+RESULTS:

* Helpers

#+begin_src ipython :tangle ../src/torch/utils.py
import numpy as np

def safe_roc_auc_score(y_true, y_score, multi_class='ovr'):
      y_true = np.asarray(y_true)
      if len(np.unique(y_true)) == 1:
          return np.nan  # return np.nan where the score cannot be calculated
      return roc_auc_score(y_true, y_score, multi_class=multi_class)
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  def rescale_coefs(model, coefs, bias):

          try:
                  means = model.named_steps["scaler"].mean_
                  scales = model.named_steps["scaler"].scale_

                  # Rescale the coefficients
                  rescaled_coefs = np.true_divide(coefs, scales)

                  # Adjust the intercept
                  rescaled_bias = bias - np.sum(rescaled_coefs * means)

                  return rescaled_coefs, rescaled_bias
          except:
                  return coefs, bias

#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  from scipy.stats import bootstrap

  def get_bootstrap_ci(data, statistic=np.mean, confidence_level=0.95, n_resamples=1000, random_state=None):
      result = bootstrap((data,), statistic)
      ci_lower, ci_upper = result.confidence_interval
      return np.array([ci_lower, ci_upper])
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  def convert_seconds(seconds):
      h = seconds // 3600
      m = (seconds % 3600) // 60
      s = seconds % 60
      return h, m, s
#+end_src

#+RESULTS:

#+begin_src ipython :tangle ../src/torch/utils.py
  import pickle as pkl

  def pkl_save(obj, name, path="."):
      pkl.dump(obj, open(path + "/" + name + ".pkl", "wb"))


  def pkl_load(name, path="."):
      return pkl.load(open(path + "/" + name, "rb"))

#+end_src

#+RESULTS:

* Parameters

#+begin_src ipython
  DEVICE = 'cuda:0'
  mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
  N_NEURONS = [668, 693, 444, 361, 113]

  tasks = ['DPA', 'DualGo', 'DualNoGo']
  params = { 'net__alpha': np.logspace(-4, 4, 10),
             # 'net__l1_ratio': np.linspace(0, 1, 10),
             # 'net__module__dropout_rate': np.linspace(0, 1, 10),
            }

  kwargs = {
      'mouse': 'AP02',
      'trials': '', 'reload': 0, 'data_type': 'dF',
      'preprocess': True, 'scaler_BL': 'robust',
      'avg_noise':True, 'unit_var_BL': True,
      'random_state': None, 'T_WINDOW': 0.0,
      'l1_ratio': 0.95,
      'n_comp': None, 'scaler': None,
      'bootstrap': 0, 'n_boots': 32,
      'n_splits': 3, 'n_repeats': 1,
      'class_weight': 0,
      'multilabel': 1,
  }

  options = set_options(**kwargs)
  days = np.arange(1,  options['n_days']+1)
  # days = np.arange(1,  2)

  safe_roc_auc = make_scorer(safe_roc_auc_score, needs_proba=True)

  options['scoring'] = safe_roc_auc
  options['n_jobs'] = 30
#+end_src

#+RESULTS:

* Decoding vs days

#+begin_src ipython
  net = RegularizedNet(
      module=Perceptron,
      module__num_features=693,
      module__dropout_rate=0.0,
      alpha=0.01,
      l1_ratio=options['l1_ratio'],
      criterion=CustomBCEWithLogitsLoss,
      criterion__pos_weight=torch.tensor(1.0, device=DEVICE).to(torch.float32),
      optimizer=optim.Adam,
      optimizer__lr=0.1,
      max_epochs=100,
      callbacks=[early_stopping],
      train_split=None,
      iterator_train__shuffle=False,  # Ensure the data is shuffled each epoch
      verbose=0,
      device= DEVICE if torch.cuda.is_available() else 'cpu',  # Assuming you might want to use CUDA
      compile=False,
      warm_start=True,
  )

  options['verbose'] = 0
  model = ClassificationCV(net, params, **options)
  options['verbose'] = 1
  #+end_src

#+RESULTS:

#+begin_src ipython
  from sklearn.linear_model import LogisticRegression
  # net = LogisticRegression(penalty='l1', solver='liblinear', class_weight='balanced', n_jobs=None)
  net = LogisticRegression(penalty='elasticnet', solver='saga', class_weight='balanced', n_jobs=None, l1_ratio=0.95, max_iter=100, tol=.001, multi_class='multinomial')

  params = {'net__C': np.logspace(-4, 4, 10)}
  params = {}

  options['n_jobs'] = -1
  options['verbose'] = 0
  model = ClassificationCV(net, params, **options)
  options['verbose'] = 1

#+end_src

#+RESULTS:

#+begin_src ipython
scores_sample = []
scores_dist = []
scores_choice = []

for task in tasks:
    options['task'] = task

    scores_sample_task = []
    scores_dist_task = []
    scores_choice_task = []

    for day in days:
        options['day'] = day

        options['class_weight'] = 1
        options['features'] = 'sample'
        options['epochs'] = ['ED']
        scores = get_classification(model, RETURN='scores', **options)
        scores_sample_task.append(scores)

        # options['features'] = 'distractor'
        # options['epochs'] = ['MD']
        # scores = get_classification(model, RETURN='scores', **options)
        # scores_dist_task.append(scores)

        # options['class_weight'] = 1
        # options['features'] = 'choice'
        # options['epochs'] = ['CHOICE']
        # scores = get_classification(model, RETURN='scores', **options)
        # scores_choice_task.append(scores)

    scores_sample.append(scores_sample_task)
    # scores_dist.append(scores_dist_task)
    # scores_choice.append(scores_choice_task)

scores_save = np.array(scores_sample)
# scores_save = np.stack((scores_sample, scores_dist))
# scores_save = np.stack((scores_sample, scores_dist, scores_choice))
print(scores_save.shape)
pkl_save(scores_save, '%s_scores_tasks_%.2f_l1_ratio%s' % (options['mouse'], options['l1_ratio'], options['fname']), path="../data/%s/" % options['mouse'])
    #+end_src

#+RESULTS:
#+begin_example
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DPA TRIALS  DAYS 1 LASER 0
X_S1 (12, 702, 115) X_S2 (12, 702, 115)
X_S3 (12, 702, 115) X_S4 (12, 702, 115)
X (48, 702, 115) y (48,)
Elapsed (with compilation) = 0h 0m 27s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DPA TRIALS  DAYS 2 LASER 0
X_S1 (8, 702, 115) X_S2 (8, 702, 115)
X_S3 (8, 702, 115) X_S4 (8, 702, 115)
X (32, 702, 115) y (32,)
Elapsed (with compilation) = 0h 0m 22s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DPA TRIALS  DAYS 3 LASER 0
X_S1 (8, 702, 115) X_S2 (8, 702, 115)
X_S3 (8, 702, 115) X_S4 (8, 702, 115)
X (32, 702, 115) y (32,)
Elapsed (with compilation) = 0h 0m 18s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DPA TRIALS  DAYS 4 LASER 0
X_S1 (8, 702, 115) X_S2 (8, 702, 115)
X_S3 (8, 702, 115) X_S4 (8, 702, 115)
X (32, 702, 115) y (32,)
Elapsed (with compilation) = 0h 0m 22s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DPA TRIALS  DAYS 5 LASER 0
X_S1 (8, 702, 115) X_S2 (8, 702, 115)
X_S3 (8, 702, 115) X_S4 (8, 702, 115)
X (32, 702, 115) y (32,)
Elapsed (with compilation) = 0h 0m 20s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DPA TRIALS  DAYS 6 LASER 0
X_S1 (8, 702, 115) X_S2 (8, 702, 115)
X_S3 (8, 702, 115) X_S4 (8, 702, 115)
X (32, 702, 115) y (32,)
Elapsed (with compilation) = 0h 0m 22s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DPA TRIALS  DAYS 7 LASER 0
X_S1 (8, 702, 115) X_S2 (8, 702, 115)
X_S3 (8, 702, 115) X_S4 (8, 702, 115)
X (32, 702, 115) y (32,)
Elapsed (with compilation) = 0h 0m 20s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DPA TRIALS  DAYS 8 LASER 0
X_S1 (8, 702, 115) X_S2 (8, 702, 115)
X_S3 (8, 702, 115) X_S4 (8, 702, 115)
X (32, 702, 115) y (32,)
Elapsed (with compilation) = 0h 0m 20s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DPA TRIALS  DAYS 9 LASER 0
X_S1 (8, 702, 115) X_S2 (8, 702, 115)
X_S3 (8, 702, 115) X_S4 (8, 702, 115)
X (32, 702, 115) y (32,)
Elapsed (with compilation) = 0h 0m 20s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DPA TRIALS  DAYS 10 LASER 0
X_S1 (8, 702, 115) X_S2 (8, 702, 115)
X_S3 (8, 702, 115) X_S4 (8, 702, 115)
X (32, 702, 115) y (32,)
Elapsed (with compilation) = 0h 0m 21s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualGo TRIALS  DAYS 1 LASER 0
X_S1 (24, 702, 115) X_S2 (24, 702, 115)
X_S3 (24, 702, 115) X_S4 (24, 702, 115)
X (96, 702, 115) y (96,)
Elapsed (with compilation) = 0h 0m 35s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualGo TRIALS  DAYS 2 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 28s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualGo TRIALS  DAYS 3 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 29s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualGo TRIALS  DAYS 4 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 27s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualGo TRIALS  DAYS 5 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 27s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualGo TRIALS  DAYS 6 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 31s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualGo TRIALS  DAYS 7 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 27s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualGo TRIALS  DAYS 8 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 27s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualGo TRIALS  DAYS 9 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 27s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualGo TRIALS  DAYS 10 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 27s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS 1 LASER 0
X_S1 (24, 702, 115) X_S2 (24, 702, 115)
X_S3 (24, 702, 115) X_S4 (24, 702, 115)
X (96, 702, 115) y (96,)
Elapsed (with compilation) = 0h 0m 35s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS 2 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 31s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS 3 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 30s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS 4 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 27s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS 5 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 27s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS 6 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 24s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS 7 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 27s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS 8 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 24s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS 9 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 29s
Loading files from /home/leon/dual_task/dual_data/data/AP02
PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR True
DATA: FEATURES sample TASK DualNoGo TRIALS  DAYS 10 LASER 0
X_S1 (16, 702, 115) X_S2 (16, 702, 115)
X_S3 (16, 702, 115) X_S4 (16, 702, 115)
X (64, 702, 115) y (64,)
Elapsed (with compilation) = 0h 0m 27s
(3, 10, 3, 115)
#+end_example

    #+begin_src ipython
scores_sample = np.array(scores_sample)
print(scores_sample.shape)
    #+end_src

#+RESULTS:
: (3, 10, 3, 115)

* Scores

 #+begin_src ipython
  filename = '%s_scores_tasks_%.2f_l1_ratio%s.pkl' % (options['mouse'], options['l1_ratio'], options['fname'])
  print(filename)
  try:
      scores = pkl_load(filename, path="../data/%s/" % options['mouse'])
      print('scores', scores.shape)
  except:
      print('file not found')
#+end_src

#+RESULTS:
: AP02_scores_tasks_0.95_l1_ratio.pkl
: scores (3, 10, 3, 115)

#+begin_src ipython
  scores_sample = scores[0]
  # scores_dist = scores[1]
  scores_choice = scores[1]
    #+end_src

#+RESULTS:
: da63f3d2-747f-43ef-9a4f-38ccf1cccf1e

  #+begin_src ipython
      options['epochs'] = ['LD']

      colors = ['r', 'b', 'g']
      for task in range(len(tasks)):
          sample_avg = []
          sample_ci = []
          for i in range(options['n_days']):
              sample_epoch = avg_epochs(scores_sample[task][i], **options)
              sample_avg.append(sample_epoch.mean(0))
              sample_ci.append(get_bootstrap_ci(sample_epoch))

          sample_avg = np.array(sample_avg)
          sample_ci = np.array(sample_ci).T

          plt.plot(np.arange(1, options['n_days']+1), sample_avg, '-o', label='%s' % options['tasks'][task], color=colors[task])
          plt.fill_between(np.arange(1, options['n_days']+1), sample_ci[0], sample_ci[1], color=colors[task], alpha=0.1)

      plt.axhline(y=0.5, color='k', linestyle='--')

      plt.legend(fontsize=10)
      plt.xticks(np.arange(1, options['n_days']+1))
      plt.yticks([0.4, 0.6, 0.8, 1.0])
      plt.xlabel('Day')
      plt.ylabel('Sample Score')
      plt.savefig('%s_scores_avg.svg' % options['mouse'], dpi=300)
      plt.show()
#+end_src

#+RESULTS:
[[./.ob-jupyter/c235122dfdf2db929a5b6f8cb619100b2f1d4f91.png]]

#+begin_src ipython
  options['epochs'] = ['LD']
  for task in range(len(tasks)):
      dist_avg = []
      dist_ci = []
      for i in range(options['n_days']):
          dist_epoch = avg_epochs(scores_dist[task][i], **options)
          dist_avg.append(np.nanmean(dist_epoch, axis=0))
          dist_ci.append(get_bootstrap_ci(dist_epoch))

      dist_avg = np.array(dist_avg)
      dist_ci = np.array(dist_ci).T

      plt.plot(np.arange(1, options['n_days']+1), dist_avg, '-o', label='%s' % options['tasks'][task], color=colors[task])
      plt.fill_between(np.arange(1, options['n_days']+1), dist_ci[0], dist_ci[1], color=colors[task], alpha=0.1)
  plt.axhline(y=0.5, color='k', linestyle='--')

  plt.legend(fontsize=10)
  plt.xticks(np.arange(1, options['n_days']+1))
  plt.yticks([0.4, 0.6, 0.8, 1.0])
  plt.xlabel('Day')
  plt.ylabel('Dist Score')
  plt.savefig('%s_scores_avg.svg' % options['mouse'], dpi=300)
  plt.show()

#+end_src

#+RESULTS:
: 0f73b541-a712-4919-a873-feb9457c9d18

#+begin_src ipython
  options['epochs'] = ['LD']
  for task in range(len(tasks)):
      choice_avg = []
      choice_ci = []
      for i in range(options['n_days']):
          choice_epoch = avg_epochs(scores_choice[task][i], **options)
          choice_avg.append(np.nanmean(choice_epoch, axis=0))
          choice_ci.append(get_bootstrap_ci(choice_epoch))

      choice_avg = np.array(choice_avg)
      choice_ci = np.array(choice_ci).T

      plt.plot(np.arange(1, options['n_days']+1), choice_avg, '-o', label='%s' % options['tasks'][task], color=colors[task])
      plt.fill_between(np.arange(1, options['n_days']+1), choice_ci[0], choice_ci[1], color=colors[task], alpha=0.1)
  plt.axhline(y=0.5, color='k', linestyle='--')

  plt.legend(fontsize=10)
  plt.xticks(np.arange(1, options['n_days']+1))
  plt.yticks([0.4, 0.6, 0.8, 1.0])
  plt.xlabel('Day')
  plt.ylabel('Choice Score')
  plt.savefig('%s_scores_avg.svg' % options['mouse'], dpi=300)
  plt.show()
#+end_src

#+RESULTS:
: 65f3cddb-577d-4349-940b-0957197a5931

* Scores mice

#+begin_src ipython
  mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
  # mice = ['JawsM15', 'JawsM18']
  mouse = 'JawsM15'

  l1_ratio = 0.95

  tasks = ['DPA', 'DualGo', 'DualNoGo']
  N_NEURONS = [668, 693, 444, 361, 113]

  kwargs = {
      'mouse': 'ACCM03',
      'trials': '', 'reload': 0, 'data_type': 'dF', 'preprocess': False,
      'scaler_BL': 'robust', 'avg_noise':True, 'unit_var_BL':False,
      'random_state': None, 'T_WINDOW': 0.0,
      'l1_ratio': 0.95,
  }

  options = set_options(**options)
  fname = options['fname']
  print(fname)
#+end_src

#+RESULTS:
: 45e9ef53-b77c-4617-9d1d-8bf22285c2bb

#+begin_src ipython
  scores_mice = []

  for mouse in mice:
      filename = '%s_scores_tasks_%.2f_l1_ratio%s.pkl' % (mouse, l1_ratio, fname)
      print(filename)
      try:
          scores = pkl_load(filename, path="../data/%s/" % mouse)
          print('scores', scores.shape)
          scores_mice.append(scores)
      except:
          print('file not found')
          scores_mice.append(np.nan * np.ones((3, 6, 2, 84)))
#+end_src

#+RESULTS:
: 6274a70c-cc37-4ef0-b4af-46358824d9fd

#+begin_src ipython
  colors = ['r', 'b', 'g']
  options = set_options(**kwargs)
  options['T_WINDOW'] = 0
  options['epochs'] = ['POST_DIST']

  for task in range(3):
    sample_mice = []
    for i in range(len(mice)):
        scores_sample = scores_mice[i][0][task]
        sample_avg = []
        # sample_ci = []
        for j in range(scores_sample.shape[0]):
            sample_epoch = avg_epochs(scores_sample[j], **options)
            sample_avg.append(sample_epoch.mean(0))
            # sample_ci.append(get_bootstrap_ci(sample_epoch))

        sample_avg = np.array(sample_avg)
        while sample_avg.shape[0] !=6:
            sample_avg = np.append(sample_avg, np.nan)

        sample_mice.append(sample_avg)

    sample_mice = np.array(sample_mice)
    sample_ci = get_bootstrap_ci(sample_mice)
    sample_ci_last = get_bootstrap_ci(sample_mice[:3][-1])
    sample_ci[0][-1] = sample_ci_last[0]
    sample_ci[1][-1] = sample_ci_last[1]

    plt.plot(np.arange(1, 7), np.nanmean(sample_mice, 0), '-o', label='%s' % options['tasks'][task], color=colors[task])
    plt.fill_between(np.arange(1, 7), sample_ci[0], sample_ci[1], color=colors[task], alpha=0.05)
    plt.axhline(y=0.5, color='k', linestyle='--')
  plt.legend(fontsize=16, frameon=0)
  plt.xlabel('Day')
  plt.ylabel('Sample Score')
  plt.xticks(np.arange(1,7))
  plt.yticks([0.5, 0.6, 0.7, 0.8])
  plt.savefig('./figs/mice_scores_tasks_sample%s.svg' % fname, dpi=300)

  plt.show()
#+end_src

#+RESULTS:
: 55ea4658-ef27-4f66-96c3-ea3c7d373ceb

#+begin_src ipython
  colors = ['r', 'b', 'g']
  options = set_options(**kwargs)
  options['T_WINDOW'] = 0
  options['epochs'] = ['ED']

  for task in range(3):
    choice_mice = []
    for i in range(len(mice)):
        scores_choice = scores_mice[i][1][task]
        choice_avg = []
        for j in range(scores_choice.shape[0]):
            choice_epoch = avg_epochs(scores_choice[j], **options)
            choice_avg.append(choice_epoch.mean(0))

        choice_avg = np.array(choice_avg)
        while choice_avg.shape[0] !=6:
            choice_avg = np.append(choice_avg, np.nan)
        choice_mice.append(choice_avg)

    choice_mice = np.array(choice_mice)
    choice_ci = get_bootstrap_ci(choice_mice)
    choice_ci_last = get_bootstrap_ci(choice_mice[:3][-1])
    choice_ci[0][-1] = choice_ci_last[0]
    choice_ci[1][-1] = choice_ci_last[1]

    plt.plot(np.arange(1, 7), np.nanmean(choice_mice, 0), '-o', label='%s' % options['tasks'][task], color=colors[task])
    plt.fill_between(np.arange(1, 7), choice_ci[0], choice_ci[1], color=colors[task], alpha=0.05)
    plt.axhline(y=0.5, color='k', linestyle='--')
  plt.legend(fontsize=16, frameon=0)
  plt.xlabel('Day')
  plt.ylabel('Choice Score')
  plt.xticks(np.arange(1,7))
  plt.yticks([0.5, 0.6, 0.7, 0.8])
  plt.savefig('./figs/mice_scores_tasks_choice%s.svg' % fname, dpi=300)

  plt.show()
#+end_src

#+RESULTS:
: 5f07f196-f629-4707-9a3f-b8c2870290a5


#+begin_src ipython

#+end_src

#+RESULTS:
: 9b541ac4-1227-418a-86ad-c4ed72fddb27
