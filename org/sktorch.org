#+STARTUP: fold
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session skorch :kernel dual_data


* Notebook Settings

#+begin_src ipython
  %load_ext autoreload
  %autoreload 2
  %reload_ext autoreload
  
  %run /home/leon/dual_task/dual_data/notebooks/setup.py
  %matplotlib inline
  %config InlineBackend.figure_format = 'png'
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/dual_data/bin/python

* Imports

#+begin_src ipython
  import sys
  sys.path.insert(0, '/home/leon/dual_task/dual_data/')

  import pickle as pkl
  import numpy as np
  import matplotlib.pyplot as plt
  from scipy.stats import circmean
#+end_src

#+RESULTS:

* skorch

#+begin_src ipython
  from torch import nn
  from skorch import NeuralNetClassifier
  import torch.nn.functional as F
  import torch
#+end_src

#+RESULTS:

#+begin_src ipython
  class BinaryClassifier(nn.Module):
      def __init__(self, n_in=693, n_rec=693):
          super(BinaryClassifier, self).__init__()
          # Define layers
          self.fc1 = nn.Linear(n_in, n_rec)
          self.fc2 = nn.Linear(n_rec, 1)
          
      def forward(self, x):
          x = torch.relu(self.fc1(x))
          x = torch.sigmoid(self.fc2(x))  # Output between 0 and 1
          return x
#+end_src

#+RESULTS:

#+begin_src ipython
  net = NeuralNetClassifier(
      BinaryClassifier,
      max_epochs=20,
      lr=0.1,
      # device='cuda',  # uncomment this to train with CUDA
  )
#+end_src

#+RESULTS:

* Data
** imports

#+begin_src ipython
  import sys
  sys.path.insert(0, '../')
  
  from src.common.get_data import get_X_y_days, get_X_y_S1_S2
  from src.common.options import set_options
#+end_src

#+RESULTS:

** parameters

#+begin_src ipython
  mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
  tasks = ['DPA', 'DualGo', 'DualNoGo']
  days = ['first', 'last']

  kwargs = dict()
  kwargs = {'prescreen': 'fpr', 'pval': 0.05, 'trials': '', 'balance': 'under',
            'method': 'bootstrap', 'bolasso_pval':0.05, 'bolasso_penalty': 'l2',
            'bootstrap': True, 'n_boots': 1000,
            'preprocess': True, 'scaler_BL': 'robust', 'avg_noise':True, 'unit_var_BL':False,
            'clf':'log_loss', 'scaler': None, 'tol':0.001, 'penalty':'l2',
            'out_fold': 'stratified', 'n_out': 5,
            'in_fold': 'stratified', 'n_in': 5,
            'random_state': None, 'n_repeats': 10,
            'n_lambda': 20, 'T_WINDOW': 0.5,
            }
  
#+end_src

#+RESULTS:

** Load X, y

#+begin_src ipython
  options = set_options(**kwargs)
  options['reload'] = False
  options['data_type'] = 'raw'
  options['DCVL'] = 0
  X_days, y_days = get_X_y_days(**options)
  X_data, y_data = get_X_y_S1_S2(X_days, y_days, **options)
#+end_src

#+RESULTS:
#+begin_example
  loading files from /home/leon/dual_task/dual_data/data/JawsM15
  X_days (1152, 693, 84) y_days (1152, 6)
  ##########################################
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR False
  ##########################################
  ##########################################
  DATA: FEATURES sample TASK DualGo TRIALS  DAYS first LASER 0
  ##########################################
  multiple days 0 3 0
  X_S1 (48, 693, 84) X_S2 (48, 693, 84)
#+end_example

#+begin_src ipython
  print(X_data[..., 0].shape, y_data.shape)
#+end_src

#+RESULTS:
: (96, 693) (96,)

* fit

#+begin_src ipython  
  net.fit(np.float32(X_data[..., 0]), np.int64(y_data));
#+end_src

#+RESULTS:
:RESULTS:
# [goto error]
#+begin_example
  [0;31m---------------------------------------------------------------------------[0m
  [0;31mRuntimeError[0m                              Traceback (most recent call last)
  Cell [0;32mIn[90], line 1[0m
  [0;32m----> 1[0m [43mnet[49m[38;5;241;43m.[39;49m[43mfit[49m[43m([49m[43mnp[49m[38;5;241;43m.[39;49m[43mfloat32[49m[43m([49m[43mX_data[49m[43m[[49m[38;5;241;43m.[39;49m[38;5;241;43m.[39;49m[38;5;241;43m.[39;49m[43m,[49m[43m [49m[38;5;241;43m0[39;49m[43m][49m[43m)[49m[43m,[49m[43m [49m[43mnp[49m[38;5;241;43m.[39;49m[43mint64[49m[43m([49m[43my_data[49m[43m)[49m[43m)[49m;

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/skorch/classifier.py:348[0m, in [0;36mNeuralNetBinaryClassifier.fit[0;34m(self, X, y, **fit_params)[0m
  [1;32m    337[0m [38;5;250m[39m[38;5;124;03m"""See ``NeuralNet.fit``.[39;00m
  [1;32m    338[0m 
  [1;32m    339[0m [38;5;124;03mIn contrast to ``NeuralNet.fit``, ``y`` is non-optional to[39;00m
  [0;32m   (...)[0m
  [1;32m    343[0m 
  [1;32m    344[0m [38;5;124;03m"""[39;00m
  [1;32m    345[0m [38;5;66;03m# pylint: disable=useless-super-delegation[39;00m
  [1;32m    346[0m [38;5;66;03m# this is actually a pylint bug:[39;00m
  [1;32m    347[0m [38;5;66;03m# https://github.com/PyCQA/pylint/issues/1085[39;00m
  [0;32m--> 348[0m [38;5;28;01mreturn[39;00m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[38;5;241;43m.[39;49m[43mfit[49m[43m([49m[43mX[49m[43m,[49m[43m [49m[43my[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mfit_params[49m[43m)[49m

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/skorch/net.py:1317[0m, in [0;36mNeuralNet.fit[0;34m(self, X, y, **fit_params)[0m
  [1;32m   1285[0m [38;5;250m[39m[38;5;124;03m"""Initialize and fit the module.[39;00m
  [1;32m   1286[0m 
  [1;32m   1287[0m [38;5;124;03mIf the module was already initialized, by calling fit, the[39;00m
  [0;32m   (...)[0m
  [1;32m   1314[0m 
  [1;32m   1315[0m [38;5;124;03m"""[39;00m
  [1;32m   1316[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m[38;5;241m.[39mwarm_start [38;5;129;01mor[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m[38;5;241m.[39minitialized_:
  [0;32m-> 1317[0m     [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43minitialize[49m[43m([49m[43m)[49m
  [1;32m   1319[0m [38;5;28mself[39m[38;5;241m.[39mpartial_fit(X, y, [38;5;241m*[39m[38;5;241m*[39mfit_params)
  [1;32m   1320[0m [38;5;28;01mreturn[39;00m [38;5;28mself[39m

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/skorch/net.py:903[0m, in [0;36mNeuralNet.initialize[0;34m(self)[0m
  [1;32m    901[0m [38;5;28mself[39m[38;5;241m.[39m_initialize_virtual_params()
  [1;32m    902[0m [38;5;28mself[39m[38;5;241m.[39m_initialize_callbacks()
  [0;32m--> 903[0m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_initialize_module[49m[43m([49m[43m)[49m
  [1;32m    904[0m [38;5;28mself[39m[38;5;241m.[39m_initialize_criterion()
  [1;32m    905[0m [38;5;28mself[39m[38;5;241m.[39m_initialize_optimizer()

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/skorch/net.py:753[0m, in [0;36mNeuralNet._initialize_module[0;34m(self, reason)[0m
  [1;32m    751[0m module [38;5;241m=[39m [38;5;28mgetattr[39m([38;5;28mself[39m, name [38;5;241m+[39m [38;5;124m'[39m[38;5;124m_[39m[38;5;124m'[39m)
  [1;32m    752[0m [38;5;28;01mif[39;00m [38;5;28misinstance[39m(module, torch[38;5;241m.[39mnn[38;5;241m.[39mModule):
  [0;32m--> 753[0m     module [38;5;241m=[39m [43mto_device[49m[43m([49m[43mmodule[49m[43m,[49m[43m [49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mdevice[49m[43m)[49m
  [1;32m    754[0m     module [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mtorch_compile(module, name[38;5;241m=[39mname)
  [1;32m    755[0m     [38;5;28msetattr[39m([38;5;28mself[39m, name [38;5;241m+[39m [38;5;124m'[39m[38;5;124m_[39m[38;5;124m'[39m, module)

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/skorch/utils.py:202[0m, in [0;36mto_device[0;34m(X, device)[0m
  [1;32m    199[0m [38;5;28;01mif[39;00m [38;5;28misinstance[39m(X, torch[38;5;241m.[39mdistributions[38;5;241m.[39mdistribution[38;5;241m.[39mDistribution):
  [1;32m    200[0m     [38;5;28;01mreturn[39;00m X
  [0;32m--> 202[0m [38;5;28;01mreturn[39;00m [43mX[49m[38;5;241;43m.[39;49m[43mto[49m[43m([49m[43mdevice[49m[43m)[49m

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/torch/nn/modules/module.py:1160[0m, in [0;36mModule.to[0;34m(self, *args, **kwargs)[0m
  [1;32m   1156[0m         [38;5;28;01mreturn[39;00m t[38;5;241m.[39mto(device, dtype [38;5;28;01mif[39;00m t[38;5;241m.[39mis_floating_point() [38;5;129;01mor[39;00m t[38;5;241m.[39mis_complex() [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m,
  [1;32m   1157[0m                     non_blocking, memory_format[38;5;241m=[39mconvert_to_format)
  [1;32m   1158[0m     [38;5;28;01mreturn[39;00m t[38;5;241m.[39mto(device, dtype [38;5;28;01mif[39;00m t[38;5;241m.[39mis_floating_point() [38;5;129;01mor[39;00m t[38;5;241m.[39mis_complex() [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m, non_blocking)
  [0;32m-> 1160[0m [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_apply[49m[43m([49m[43mconvert[49m[43m)[49m

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/torch/nn/modules/module.py:810[0m, in [0;36mModule._apply[0;34m(self, fn, recurse)[0m
  [1;32m    808[0m [38;5;28;01mif[39;00m recurse:
  [1;32m    809[0m     [38;5;28;01mfor[39;00m module [38;5;129;01min[39;00m [38;5;28mself[39m[38;5;241m.[39mchildren():
  [0;32m--> 810[0m         [43mmodule[49m[38;5;241;43m.[39;49m[43m_apply[49m[43m([49m[43mfn[49m[43m)[49m
  [1;32m    812[0m [38;5;28;01mdef[39;00m [38;5;21mcompute_should_use_set_data[39m(tensor, tensor_applied):
  [1;32m    813[0m     [38;5;28;01mif[39;00m torch[38;5;241m.[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):
  [1;32m    814[0m         [38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,[39;00m
  [1;32m    815[0m         [38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,[39;00m
  [0;32m   (...)[0m
  [1;32m    820[0m         [38;5;66;03m# global flag to let the user control whether they want the future[39;00m
  [1;32m    821[0m         [38;5;66;03m# behavior of overwriting the existing tensor or not.[39;00m

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/torch/nn/modules/module.py:833[0m, in [0;36mModule._apply[0;34m(self, fn, recurse)[0m
  [1;32m    829[0m [38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to[39;00m
  [1;32m    830[0m [38;5;66;03m# track autograd history of `param_applied`, so we have to use[39;00m
  [1;32m    831[0m [38;5;66;03m# `with torch.no_grad():`[39;00m
  [1;32m    832[0m [38;5;28;01mwith[39;00m torch[38;5;241m.[39mno_grad():
  [0;32m--> 833[0m     param_applied [38;5;241m=[39m [43mfn[49m[43m([49m[43mparam[49m[43m)[49m
  [1;32m    834[0m should_use_set_data [38;5;241m=[39m compute_should_use_set_data(param, param_applied)
  [1;32m    835[0m [38;5;28;01mif[39;00m should_use_set_data:

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/torch/nn/modules/module.py:1158[0m, in [0;36mModule.to.<locals>.convert[0;34m(t)[0m
  [1;32m   1155[0m [38;5;28;01mif[39;00m convert_to_format [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m [38;5;129;01mand[39;00m t[38;5;241m.[39mdim() [38;5;129;01min[39;00m ([38;5;241m4[39m, [38;5;241m5[39m):
  [1;32m   1156[0m     [38;5;28;01mreturn[39;00m t[38;5;241m.[39mto(device, dtype [38;5;28;01mif[39;00m t[38;5;241m.[39mis_floating_point() [38;5;129;01mor[39;00m t[38;5;241m.[39mis_complex() [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m,
  [1;32m   1157[0m                 non_blocking, memory_format[38;5;241m=[39mconvert_to_format)
  [0;32m-> 1158[0m [38;5;28;01mreturn[39;00m [43mt[49m[38;5;241;43m.[39;49m[43mto[49m[43m([49m[43mdevice[49m[43m,[49m[43m [49m[43mdtype[49m[43m [49m[38;5;28;43;01mif[39;49;00m[43m [49m[43mt[49m[38;5;241;43m.[39;49m[43mis_floating_point[49m[43m([49m[43m)[49m[43m [49m[38;5;129;43;01mor[39;49;00m[43m [49m[43mt[49m[38;5;241;43m.[39;49m[43mis_complex[49m[43m([49m[43m)[49m[43m [49m[38;5;28;43;01melse[39;49;00m[43m [49m[38;5;28;43;01mNone[39;49;00m[43m,[49m[43m [49m[43mnon_blocking[49m[43m)[49m

  [0;31mRuntimeError[0m: CUDA error: device-side assert triggered
  CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
  For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
#+end_example
:END:

#+begin_src ipython
  import torch
  from torch import nn

  import skorch
  from skorch import NeuralNetBinaryClassifier
  from sklearn.datasets import make_classification
  from sklearn.model_selection import train_test_split

  # Generate synthetic binary classification data
  X, y = make_classification(1000, 20, n_informative=10, n_classes=2)

  X = X.astype(np.float32)
  y = y.astype(np.float32)

  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

  print(X_train.dtype, y_train.dtype)
  # Define the PyTorch Module (Neural Network)
  class BinaryClassifierModule(nn.Module):
      def __init__(self, num_units=10, nonlin=nn.ReLU()):
          super(BinaryClassifierModule, self).__init__()
          self.dense0 = nn.Linear(20, num_units)  # 20 input features
          self.nonlin = nonlin
          self.dropout = nn.Dropout(0.5)
          self.dense1 = nn.Linear(num_units, 1)
          self.output = nn.Sigmoid()

      def forward(self, X, **kwargs):
          X = self.nonlin(self.dense0(X))
          X = self.dropout(X)
          X = self.dense1(X)
          return self.output(X)

  # Create a Skorch binary classifier using the defined module
  net = NeuralNetBinaryClassifier(
      BinaryClassifierModule,
      max_epochs=10,
      lr=0.1,
      device='cuda:0' # You can also set other neural network parameters or training parameters here
  )

  print(X_train.shape, y_train.shape)
  # Train the model on the training data
  net.fit(X_train, y_train)

  # Evaluate the trained model on the test data
  # accuracy = net.score(X_test, y_test)
  print(f'Test Accuracy: {accuracy}')
#+end_src

#+RESULTS:
:RESULTS:
: float32 float32
: (800, 20) (800,)
# [goto error]
#+begin_example
  [0;31m---------------------------------------------------------------------------[0m
  [0;31mRuntimeError[0m                              Traceback (most recent call last)
  Cell [0;32mIn[92], line 44[0m
  [1;32m     42[0m [38;5;28mprint[39m(X_train[38;5;241m.[39mshape, y_train[38;5;241m.[39mshape)
  [1;32m     43[0m [38;5;66;03m# Train the model on the training data[39;00m
  [0;32m---> 44[0m [43mnet[49m[38;5;241;43m.[39;49m[43mfit[49m[43m([49m[43mX_train[49m[43m,[49m[43m [49m[43my_train[49m[43m)[49m
  [1;32m     46[0m [38;5;66;03m# Evaluate the trained model on the test data[39;00m
  [1;32m     47[0m [38;5;66;03m# accuracy = net.score(X_test, y_test)[39;00m
  [1;32m     48[0m [38;5;28mprint[39m([38;5;124mf[39m[38;5;124m'[39m[38;5;124mTest Accuracy: [39m[38;5;132;01m{[39;00maccuracy[38;5;132;01m}[39;00m[38;5;124m'[39m)

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/skorch/classifier.py:348[0m, in [0;36mNeuralNetBinaryClassifier.fit[0;34m(self, X, y, **fit_params)[0m
  [1;32m    337[0m [38;5;250m[39m[38;5;124;03m"""See ``NeuralNet.fit``.[39;00m
  [1;32m    338[0m 
  [1;32m    339[0m [38;5;124;03mIn contrast to ``NeuralNet.fit``, ``y`` is non-optional to[39;00m
  [0;32m   (...)[0m
  [1;32m    343[0m 
  [1;32m    344[0m [38;5;124;03m"""[39;00m
  [1;32m    345[0m [38;5;66;03m# pylint: disable=useless-super-delegation[39;00m
  [1;32m    346[0m [38;5;66;03m# this is actually a pylint bug:[39;00m
  [1;32m    347[0m [38;5;66;03m# https://github.com/PyCQA/pylint/issues/1085[39;00m
  [0;32m--> 348[0m [38;5;28;01mreturn[39;00m [38;5;28;43msuper[39;49m[43m([49m[43m)[49m[38;5;241;43m.[39;49m[43mfit[49m[43m([49m[43mX[49m[43m,[49m[43m [49m[43my[49m[43m,[49m[43m [49m[38;5;241;43m*[39;49m[38;5;241;43m*[39;49m[43mfit_params[49m[43m)[49m

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/skorch/net.py:1317[0m, in [0;36mNeuralNet.fit[0;34m(self, X, y, **fit_params)[0m
  [1;32m   1285[0m [38;5;250m[39m[38;5;124;03m"""Initialize and fit the module.[39;00m
  [1;32m   1286[0m 
  [1;32m   1287[0m [38;5;124;03mIf the module was already initialized, by calling fit, the[39;00m
  [0;32m   (...)[0m
  [1;32m   1314[0m 
  [1;32m   1315[0m [38;5;124;03m"""[39;00m
  [1;32m   1316[0m [38;5;28;01mif[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m[38;5;241m.[39mwarm_start [38;5;129;01mor[39;00m [38;5;129;01mnot[39;00m [38;5;28mself[39m[38;5;241m.[39minitialized_:
  [0;32m-> 1317[0m     [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43minitialize[49m[43m([49m[43m)[49m
  [1;32m   1319[0m [38;5;28mself[39m[38;5;241m.[39mpartial_fit(X, y, [38;5;241m*[39m[38;5;241m*[39mfit_params)
  [1;32m   1320[0m [38;5;28;01mreturn[39;00m [38;5;28mself[39m

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/skorch/net.py:903[0m, in [0;36mNeuralNet.initialize[0;34m(self)[0m
  [1;32m    901[0m [38;5;28mself[39m[38;5;241m.[39m_initialize_virtual_params()
  [1;32m    902[0m [38;5;28mself[39m[38;5;241m.[39m_initialize_callbacks()
  [0;32m--> 903[0m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_initialize_module[49m[43m([49m[43m)[49m
  [1;32m    904[0m [38;5;28mself[39m[38;5;241m.[39m_initialize_criterion()
  [1;32m    905[0m [38;5;28mself[39m[38;5;241m.[39m_initialize_optimizer()

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/skorch/net.py:753[0m, in [0;36mNeuralNet._initialize_module[0;34m(self, reason)[0m
  [1;32m    751[0m module [38;5;241m=[39m [38;5;28mgetattr[39m([38;5;28mself[39m, name [38;5;241m+[39m [38;5;124m'[39m[38;5;124m_[39m[38;5;124m'[39m)
  [1;32m    752[0m [38;5;28;01mif[39;00m [38;5;28misinstance[39m(module, torch[38;5;241m.[39mnn[38;5;241m.[39mModule):
  [0;32m--> 753[0m     module [38;5;241m=[39m [43mto_device[49m[43m([49m[43mmodule[49m[43m,[49m[43m [49m[38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43mdevice[49m[43m)[49m
  [1;32m    754[0m     module [38;5;241m=[39m [38;5;28mself[39m[38;5;241m.[39mtorch_compile(module, name[38;5;241m=[39mname)
  [1;32m    755[0m     [38;5;28msetattr[39m([38;5;28mself[39m, name [38;5;241m+[39m [38;5;124m'[39m[38;5;124m_[39m[38;5;124m'[39m, module)

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/skorch/utils.py:202[0m, in [0;36mto_device[0;34m(X, device)[0m
  [1;32m    199[0m [38;5;28;01mif[39;00m [38;5;28misinstance[39m(X, torch[38;5;241m.[39mdistributions[38;5;241m.[39mdistribution[38;5;241m.[39mDistribution):
  [1;32m    200[0m     [38;5;28;01mreturn[39;00m X
  [0;32m--> 202[0m [38;5;28;01mreturn[39;00m [43mX[49m[38;5;241;43m.[39;49m[43mto[49m[43m([49m[43mdevice[49m[43m)[49m

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/torch/nn/modules/module.py:1160[0m, in [0;36mModule.to[0;34m(self, *args, **kwargs)[0m
  [1;32m   1156[0m         [38;5;28;01mreturn[39;00m t[38;5;241m.[39mto(device, dtype [38;5;28;01mif[39;00m t[38;5;241m.[39mis_floating_point() [38;5;129;01mor[39;00m t[38;5;241m.[39mis_complex() [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m,
  [1;32m   1157[0m                     non_blocking, memory_format[38;5;241m=[39mconvert_to_format)
  [1;32m   1158[0m     [38;5;28;01mreturn[39;00m t[38;5;241m.[39mto(device, dtype [38;5;28;01mif[39;00m t[38;5;241m.[39mis_floating_point() [38;5;129;01mor[39;00m t[38;5;241m.[39mis_complex() [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m, non_blocking)
  [0;32m-> 1160[0m [38;5;28;01mreturn[39;00m [38;5;28;43mself[39;49m[38;5;241;43m.[39;49m[43m_apply[49m[43m([49m[43mconvert[49m[43m)[49m

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/torch/nn/modules/module.py:810[0m, in [0;36mModule._apply[0;34m(self, fn, recurse)[0m
  [1;32m    808[0m [38;5;28;01mif[39;00m recurse:
  [1;32m    809[0m     [38;5;28;01mfor[39;00m module [38;5;129;01min[39;00m [38;5;28mself[39m[38;5;241m.[39mchildren():
  [0;32m--> 810[0m         [43mmodule[49m[38;5;241;43m.[39;49m[43m_apply[49m[43m([49m[43mfn[49m[43m)[49m
  [1;32m    812[0m [38;5;28;01mdef[39;00m [38;5;21mcompute_should_use_set_data[39m(tensor, tensor_applied):
  [1;32m    813[0m     [38;5;28;01mif[39;00m torch[38;5;241m.[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):
  [1;32m    814[0m         [38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,[39;00m
  [1;32m    815[0m         [38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,[39;00m
  [0;32m   (...)[0m
  [1;32m    820[0m         [38;5;66;03m# global flag to let the user control whether they want the future[39;00m
  [1;32m    821[0m         [38;5;66;03m# behavior of overwriting the existing tensor or not.[39;00m

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/torch/nn/modules/module.py:833[0m, in [0;36mModule._apply[0;34m(self, fn, recurse)[0m
  [1;32m    829[0m [38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to[39;00m
  [1;32m    830[0m [38;5;66;03m# track autograd history of `param_applied`, so we have to use[39;00m
  [1;32m    831[0m [38;5;66;03m# `with torch.no_grad():`[39;00m
  [1;32m    832[0m [38;5;28;01mwith[39;00m torch[38;5;241m.[39mno_grad():
  [0;32m--> 833[0m     param_applied [38;5;241m=[39m [43mfn[49m[43m([49m[43mparam[49m[43m)[49m
  [1;32m    834[0m should_use_set_data [38;5;241m=[39m compute_should_use_set_data(param, param_applied)
  [1;32m    835[0m [38;5;28;01mif[39;00m should_use_set_data:

  File [0;32m~/mambaforge/envs/dual_data/lib/python3.11/site-packages/torch/nn/modules/module.py:1158[0m, in [0;36mModule.to.<locals>.convert[0;34m(t)[0m
  [1;32m   1155[0m [38;5;28;01mif[39;00m convert_to_format [38;5;129;01mis[39;00m [38;5;129;01mnot[39;00m [38;5;28;01mNone[39;00m [38;5;129;01mand[39;00m t[38;5;241m.[39mdim() [38;5;129;01min[39;00m ([38;5;241m4[39m, [38;5;241m5[39m):
  [1;32m   1156[0m     [38;5;28;01mreturn[39;00m t[38;5;241m.[39mto(device, dtype [38;5;28;01mif[39;00m t[38;5;241m.[39mis_floating_point() [38;5;129;01mor[39;00m t[38;5;241m.[39mis_complex() [38;5;28;01melse[39;00m [38;5;28;01mNone[39;00m,
  [1;32m   1157[0m                 non_blocking, memory_format[38;5;241m=[39mconvert_to_format)
  [0;32m-> 1158[0m [38;5;28;01mreturn[39;00m [43mt[49m[38;5;241;43m.[39;49m[43mto[49m[43m([49m[43mdevice[49m[43m,[49m[43m [49m[43mdtype[49m[43m [49m[38;5;28;43;01mif[39;49;00m[43m [49m[43mt[49m[38;5;241;43m.[39;49m[43mis_floating_point[49m[43m([49m[43m)[49m[43m [49m[38;5;129;43;01mor[39;49;00m[43m [49m[43mt[49m[38;5;241;43m.[39;49m[43mis_complex[49m[43m([49m[43m)[49m[43m [49m[38;5;28;43;01melse[39;49;00m[43m [49m[38;5;28;43;01mNone[39;49;00m[43m,[49m[43m [49m[43mnon_blocking[49m[43m)[49m

  [0;31mRuntimeError[0m: CUDA error: device-side assert triggered
  CUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.
  For debugging consider passing CUDA_LAUNCH_BLOCKING=1.
  Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
#+end_example
:END:

#+begin_src shell
  export CUDA_LAUNCH_BLOCKING=1
#+end_src

#+RESULTS:


