#+STARTUP: fold
#+PROPERTY: header-args:ipython :results both :exports both :async yes :session skorch :kernel dual_data

* Notebook Settings

#+begin_src ipython
  %load_ext autoreload
  %autoreload 2
  %reload_ext autoreload
  
  %run /home/leon/dual_task/dual_data/notebooks/setup.py
  %matplotlib inline
  %config InlineBackend.figure_format = 'png'
#+end_src

#+RESULTS:
: The autoreload extension is already loaded. To reload it, use:
:   %reload_ext autoreload
: Python exe
: /home/leon/mambaforge/envs/dual_data/bin/python

* Imports

#+begin_src ipython
  import sys
  sys.path.insert(0, '/home/leon/dual_task/dual_data/')

  import pickle as pkl
  import numpy as np
  import matplotlib.pyplot as plt
  from scipy.stats import circmean
  from src.common.plot_utils import add_vlines
 #+end_src

#+RESULTS:

* Helpers

#+begin_src ipython
  def convert_seconds(seconds):
      h = seconds // 3600
      m = (seconds % 3600) // 60
      s = seconds % 60
      return h, m, s
#+end_src

#+RESULTS:

* Perceptron

#+begin_src ipython
  from skorch import NeuralNetClassifier
  import torch
  import torch.nn as nn
  import torch.optim as optim

  class Perceptron(nn.Module):
      def __init__(self, num_features=361):
          super(Perceptron, self).__init__()
          self.linear = nn.Linear(num_features, 1)

      def forward(self, x):
          return torch.sigmoid(self.linear(x))

#+end_src

#+RESULTS:

#+begin_src ipython
from skorch.callbacks import Callback

class CaptureWeightsCallback(Callback):
    def __init__(self):
        super().__init__()  # Ensure to call the superclass initializer if needed
        self.weights = []

    def on_train_end(self, net, **kwargs):
        # Capture the linear layer's weights after training ends
        self.weights.append(net.module_.linear.weight.data.cpu().numpy())
#+end_src

#+RESULTS:

#+begin_src ipython
  from skorch.callbacks import EarlyStopping

  early_stopping = EarlyStopping(
      monitor='valid_loss',    # Metric to monitor
      patience=5,              # Number of epochs to wait for improvement
      threshold=0.001,       # Minimum change to qualify as an improvement
      threshold_mode='rel',    # 'rel' for relative change, 'abs' for absolute change
      lower_is_better=True     # Set to True if lower metric values are better
  )

  # Instantiate the custom callback
  # capture_weights_cb = CaptureWeightsCallback()
  
  net = NeuralNetClassifier(
      module=Perceptron,
      criterion=nn.BCELoss,
      optimizer=optim.Adam,
      optimizer__lr=0.01,
      max_epochs=100,
      callbacks=[early_stopping],  # Add the EarlyStopping callback here
      verbose=0,
      iterator_train__shuffle=True,  # Ensure the data is shuffled each epoch
      device='cuda' if torch.cuda.is_available() else 'cpu',  # Assuming you might want to use CUDA
  )
#+end_src

#+RESULTS:

* Data
** imports

#+begin_src ipython
  import sys
  sys.path.insert(0, '../')
  
  from src.common.get_data import get_X_y_days, get_X_y_S1_S2
  from src.common.options import set_options
#+end_src

#+RESULTS:

** parameters

#+begin_src ipython
  mice = ['ChRM04','JawsM15', 'JawsM18', 'ACCM03', 'ACCM04']
  tasks = ['DPA', 'DualGo', 'DualNoGo']
  days = ['first', 'last']
  
  kwargs = dict()
  kwargs = {'prescreen': 'fpr', 'pval': 0.05, 'trials': '', 'balance': 'under',
            'method': 'bootstrap', 'bolasso_pval':0.05, 'bolasso_penalty': 'l2',
            'bootstrap': True, 'n_boots': 1000,
            'preprocess': True, 'scaler_BL': 'robust', 'avg_noise':True, 'unit_var_BL':False,
            'clf':'log_loss', 'scaler': None, 'tol':0.001, 'penalty':'l2',
            'out_fold': 'stratified', 'n_out': 5,
            'in_fold': 'stratified', 'n_in': 5,
            'random_state': None, 'n_repeats': 10,
            'n_lambda': 20, 'T_WINDOW': 0.5,
            'features': 'paired',
            'day': 'last'
            }

#+end_src

#+RESULTS:

** Load X, y

#+begin_src ipython
  options = set_options(**kwargs)
  options['reload'] = 0
  options['data_type'] = 'raw'
  options['DCVL'] = 0

  options['mouse'] = 'ACCM03'
  options['task'] = 'DualGo'
  options['day'] = 'last'
  options['features'] = 'choice'
  options['trials'] = ''

  X_days, y_days = get_X_y_days(**options)
  X_data, y_data = get_X_y_S1_S2(X_days, y_days, **options)

  y_data[y_data==-1] = 0
  print(X_data.shape, y_data.shape)
#+end_src

#+RESULTS:
#+begin_example
  loading files from /home/leon/dual_task/dual_data/data/ACCM03
  X_days (960, 361, 84) y_days (960, 6)
  ##########################################
  PREPROCESSING: SCALER robust AVG MEAN False AVG NOISE True UNIT VAR False
  ##########################################
  ##########################################
  DATA: FEATURES choice TASK DualGo TRIALS  DAYS last LASER 0
  ##########################################
  multiple days 0 3 0
  X_S1 (106, 361, 84) X_S2 (54, 361, 84)
  (160, 361, 84) (160,)
#+end_example

* fit
** Standard fit

#+begin_src ipython
  print('X', X_data.shape, 'y', y_data.shape)
#+end_src

#+RESULTS:
: X (160, 361, 84) y (160,)

#+begin_src ipython  
  from sklearn.preprocessing import StandardScaler
  from sklearn.model_selection import train_test_split
  from sklearn.model_selection import cross_val_score, cross_validate
  from sklearn.pipeline import Pipeline
  from time import perf_counter  


  start = perf_counter()

  y = np.float32(y_data[:, np.newaxis])

  scores_list = []
  weights_list = []

  pipe = []
  pipe.append(("scaler", StandardScaler()))
  pipe.append(("clf", net))
  pipe = Pipeline(pipe)
  
  for i in range(X_data.shape[-1]):
      X = np.float32(X_data[..., i])

      scores = cross_val_score(pipe, X, y, cv=5, scoring='f1_weighted')
      scores_list.append(scores)

      # # Now use cross_validate (ensure to set return_estimator=True if you want the fitted estimators)
      # results = cross_validate(pipe, X, y, cv=5, return_estimator=True)      
      # scores_list.append(results['test_score'])

  scores_list = np.array(scores_list)

  end = perf_counter()
  print("Elapsed (with compilation) = %dh %dm %ds" % convert_seconds(end - start))


#+end_src

#+RESULTS:
: Elapsed (with compilation) = 0h 0m 13s
#+RESULTS:

#+begin_src ipython
  time = np.linspace(0, 14, X_data.shape[-1])
  plt.plot(time, scores_list.mean(-1))
  plt.hlines(0.5, 0, 14, 'k', '--')
  plt.ylabel('Score')
  plt.xlabel('Time (s)')
  add_vlines()
  plt.show()
 #+end_src

#+RESULTS:
[[file:./.ob-jupyter/219c6ac2be23ffb96a331e733f7e39cb4dcf6b16.png]]

** multiprocessing

#+begin_src ipython
  y_data[y_data==-1] = 0
  X = np.float32(X_data[..., 0])
  y = np.float32(y_data[:, np.newaxis])
  
  print(X.shape, y.shape)
#+end_src

#+RESULTS:
: (96, 693) (96, 1)

#+begin_src ipython
  from sklearn.model_selection import StratifiedKFold

  kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
  splits = list(kf.split(X, y))  # Assuming X and y are your dataset and labels  
#+end_src

#+RESULTS:

#+begin_src ipython
  def train_and_evaluate(fold_data):
      train_indices, val_indices = fold_data
      X_train, X_val = X[train_indices], X[val_indices]
      y_train, y_val = y[train_indices], y[val_indices]

      net = NeuralNetClassifier(  # Define Skorch NeuralNetClassifier as before
          module=Perceptron,
          criterion=nn.BCELoss,
          optimizer=optim.SGD,
          optimizer__lr=0.01,
          max_epochs=100,
          device='cuda',  # or 'cpu'
      )

      net.partial_fit(X_train, y_train)
      score = net.score(X_val, y_val)

      return score
#+end_src

#+RESULTS:

#+begin_src ipython
  from torch.multiprocessing import Pool, set_start_method

  def run_parallel_cv():
      # Use 'spawn' or 'forkserver' when working with CUDA
      set_start_method('spawn', force=True)

      with Pool(processes=84) as pool:  # Adjust 'processes' based on your system
          scores = pool.map(train_and_evaluate, splits)
          
      print(f"Cross-validated scores: {scores}")
      print(f"Mean CV Score: {np.mean(scores)}")

#+end_src

#+RESULTS:

#+begin_src ipython
  if __name__ == '__main__':
      run_parallel_cv()
#+end_src

#+RESULTS:
: 4beb1cf5-376f-478f-b9fb-6281d7752a28

